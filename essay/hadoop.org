* hadoop
#+OPTIONS: H:5

参考资源
   - Cloudera http://www.cloudera.com/
   - Apache Hadoop http://hadoop.apache.org/
   - Apache Hadoop r1.0.3 文档 http://hadoop.apache.org/common/docs/r1.0.3/
   - Apache Hadoop r1.0.3 中文文档 http://hadoop.apache.org/common/docs/r1.0.3/cn
   - CDH Downloads https://ccp.cloudera.com/display/SUPPORT/Downloads
   - CDH Documentation https://ccp.cloudera.com/display/DOC/Documentation
   - CDH Tutorial https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial

** FAQ
*** Hadoop可以用来做什么
Why Hadoop? http://www.cloudera.com/why-hadoop/

TODO(dirlt):translate it!!!

Simply put, Hadoop can transform the way you store and process data throughout your enterprise. According to analysts, about 80% of the data in the world is unstructured, and until Hadoop, it was essentially unusable in any systematic way. With Hadoop, for the first time you can combine all your data and look at it as one.
   - Make All Your Data Profitable. Hadoop enables you to gain insight from all the data you already have; to ingest the data flowing into your systems 24/7 and leverage it to make optimizations that were impossible before; to make decisions based on hard data, not hunches; to look at complete data, not samples; to look at years of transactions, not days or weeks. In short, Hadoop will change the way you run your organization.
   - Leverage All Types of Data, From All Types of Systems. Hadoop can handle all types of data from disparate systems: structured, unstructured, log files, pictures, audio files, communications records, email– just about anything you can think of. Even when different types of data have been stored in unrelated systems, you can dump it all into your Hadoop cluster before you even know how you might take advantage of it in the future.
   - Scale Beyond Anything You Have Today. The largest social network in the world is built on the same open-source technology as Hadoop, and now exceeds 100 petabytes. It’s unlikely your organization has that much data. As you need more capacity, you just add more commodity servers and Hadoop automatically incorporates the new storage and compute capacity.
     
*** Hadoop包括哪些组件
TODO(dirlt):translate it!!!

Apache Hadoop包括了下面这些组件：
   - [[http://hadoop.apache.org/common/][Hadoop Common]] The common utilities that support the other Hadoop subprojects.
   - [[http://hadoop.apache.org/hdfs/][Hadoop Distributed File System(HDFS)]] A distributed file system that provides high-throughput access to application data.
   - [[http://hadoop.apache.org/mapreduce/][Hadoop MapReduce]] A software framework for distributed processing of large data sets on compute clusters.
和Apache Hadoop相关的组件有：
   - [[http://avro.apache.org/][Avro]] A data serialization system.
   - [[http://cassandra.apache.org/][Cassandra]] A scalable multi-master database with no single points of failure.
   - [[http://incubator.apache.org/chukwa/][Chukwa]] A data collection system for managing large distributed systems.
   - [[http://hbase.apache.org/][HBase]] A scalable, distributed database that supports structured data storage for large tables.
   - [[http://hive.apache.org/][Hive]] A data warehouse infrastructure that provides data summarization and ad hoc querying.
   - [[http://mahout.apache.org/][Mahout]] A Scalable machine learning and data mining library.
   - [[http://pig.apache.org/][Pig]] A high-level data-flow language and execution framework for parallel computation.
   - [[http://zookeeper.apache.org/][ZooKeeper]] A high-performance coordination service for distributed applications.


*** CDH和Apache Hadoop的关系
CDH Hadoop FAQ https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ

TODO(dirlt):translate it!!!

   - What exactly is included in CDH? / Cloudera's Distribution Including Apache Hadoop (CDH) is a certified release of Apache Hadoop. We include some stable patches scheduled to be included in future releases, as well as some patches we have developed for our supported customers, and are in the process of contributing back to Apache.
   - What license is Cloudera's Distribution Including Apache Hadoop released under? / Just like Hadoop, Cloudera's Distribution Including Apache Hadoop is released under the Apache Public License version 2.
   - Is Cloudera forking Hadoop? / Absolutely not. Cloudera is committed to the Hadoop project and the principles of the Apache Software License and Foundation. We continue to work actively with current releases of Hadoop and deliver certified releases to the community as appropriate.
   - Does Cloudera contribute their changes back to Apache? / We do, and will continue to contribute all eligible changes back to Apache. We occasionally release code we know to be stable even if our contribution to Apache is still in progress. Some of our changes are not eligible for contribution, as they capture the Cloudera brand, or link to our tools and documentation, but these do not affect compatibility with core project.

*** CDH产品组件构成
http://www.cloudera.com/content/cloudera/en/products/cdh.html

file:./images/cloudera-enterprise-diagram.png

** 观点
*** Hadoop即将过时了吗？
http://www.kuqin.com/database/20120715/322528.html google提出的三个东西都是解决hadoop的软肋，最终目的还是需要解决大数据上面的实时性问题。

   - 增量索引过滤器（Percolator for incremental indexing）和频繁变化数据集分析。Hadoop是一台大型“机器”，当启动并全速运转时处理数据的性能惊人，你唯一需要操心的就是硬盘的传输速度跟不上。但是每次你准备启动分析数据时，都需要把所有的数据都过一遍，当数据集越来越庞大时，这个问题将导致分析时间无限延长。那么Google是如何解决让搜索结果返回速度越来越接近实时的呢？答案是用增量处理引擎Percolator代替GMR。通过只处理新增的、改动过的或删除的文档和使用二级指数来高效率建目录，返回查询结果。Percolator论文的作者写道：“将索引系统转换成增量系统…将文档处理延迟缩短了100倍。”这意味着索引web新内容的速度比用MapReduce快100倍！类似大型强子对撞机产生的数据将不断变大，Twitter也是如此。这也是为什么HBase中会新增触发流程，而Twitter Storm正在成为实时处理流数据的热门技术。
   - 用于点对点分析的Dremel。Google和Hadoop生态系统都致力于让MapReduce成为可用的点对点分析工具。从Sawzall到Pig和Hive，创建了大量的界面层，但是尽管这让Hadoop看上去更像SQL系统，但是人们忘记了一个基本事实——MapReduce(以及Hadoop)是为组织数据处理任务开发的系统，诞生于工作流内核，而不是点对点分析。今天有大量的BI/分析查询都是点对点模式，属于互动和低延迟的分析。Hadoop的Map和Reduce工作流让很多分析师望而却步，而且工作启动和完成工作流运行的漫长周期对于很多互动性分析来说意味着糟糕的用户体验。于是，Google发明了Dremel（业界也称之为BigQuery产品）专用工具，可以让分析师数秒钟内就扫描成PB（Petabyte）的数据完成点到点查询，而且还能支持可视化。Google在Dremel的论文中声称：“Dremel能够在数秒内完成数万亿行数据的聚合查询，比MapReduce快上100倍！”
   - 分析图数据的Pregel。Google MapReduce的设计初衷是分析世界上最大的数据图谱——互联网。但是在分析人际网络、电信设备、文档和其他一些图数据时就没有那么灵光了，例如MapReduce在计算单源最短路径（SSSP）时效率非常低下，已有的并行图算法库Parallel BGL或者CGMgraph又没有容错。于是Google开发了Pregel，一个可以在分布式通用服务器上处理PB级别图数据的大型同步处理应用。与Hadoop经常在处理图数据时产生指数级数据放大相比，Pregel能够自然高效地处理SSSP或PageRank等图算法，所用时间要短得多，代码也简洁得多。目前唯一能与Pregel媲美的开源选择是Giraph，这是一个早期的Apache孵化项目，调用了HDFS和Zookeeper。Githb上还有一个项目Golden Orb可用。

*** MapReduce和并行数据库，朋友还是敌人？
http://www.cnblogs.com/chinacloud/archive/2010/12/03/1895365.html 第四主题

在 2010年1月的ACM上，有两篇文章非常吸引人注意。一篇文章是Google的Jeffrey Dean、Sanjay Ghemawat发表的标题为《MapReduce:一个灵活的数据库处理工具》，另一篇文章是Michael Stonebraker、Daniel  Abadi、 David J. DeWitt、Sam Madden、Erik Paulson、Andrew Pavlo、Alexander、Rasin等人发表的《MapReduce和并行数据库：是朋友还是敌人？》。这两篇文章让我想起去年初Michael Stonebraker等人就MapReduce发表的一些评论而导致了一次MapReduce和数据库系统的大辩论。那篇文章的标题是《MapReduce：一个巨大的倒退》。这次辩论双方则准备了丰富的实践和实验案例。看上去更加有趣也更加有说服力。以下“正方”代表坚持并行数据库解决方案的Andrew Pavlo、 Michael Stonebraker等，而反方则是Google的MapReduce（下文简称MR）的拥趸Jeffrey Dean、Sanjay Ghemawat等。

--------------------

正方抛出观点。2009 年Andrew Pavlo等人发表了一篇标题为《大规模数据分析的方法对比》（http://database.cs.brown.edu/projects/mapreduce-vs-dbms/ ）的文章，里面对比了数据库和MR两种大规模数据分析方法的对比。通过对比流行的MR软件 Hadoop和一种并行数据库之间的架设、使用和性能等方面的异同，指出MR并不是解决大规模数据分析的好方法，其在性能、易用性等方面有诸多问题：
   - MR没法用索引，总是对数据进行完全扫描；
   - MR输入和输出，总是文件系统中的简单文件；
   - MR需要使用不高效的文本数据格式。

--------------------

反方接招。
   - 对于正方第一个观点，反方如此应对：“错了！MR的输入本身可以是数据库的输出，所以，我们是可以用索引的。另外一个例子是MR从BigTable里面读取数据，如果数据落在一个行范畴里面，当然是可以用索引的。而且，在很多需要处理的数据里头，比如Web Server的日志，经过轮转之后天然就有索引（文件名包含时间戳）。”
   - 对于第二个观点，反方认为：“现存的很多MR系统，本身就是一个异构环境，用户的数据可能存储在关系数据库里头，而其处理结果可能会记录在文件系统里头。而且，这样的环境可能会进化，用户的数据会迁移到新的系统里。而MR可以非常便利地在这些环境上运行。更进一步，用户可以扩展这些存储，比如分布文件系统、数据库查询结果，存储在BigTable里面的数据，结构化的数据（B-tree文件等）。对于这些场合，单个MR处理就可以很容易地捏合它们。”
   - 对于第三个观点，反方认为：“这点的确很精辟。很到位，不过这个因素是取决于具体的实现的，比如在Google的MR实现里，有个 Protocol Buffer层，可以对输入的数据进行格式定义，因此就可以直接适用二进制类型，而不用有额外的格式转换的开销，在我们的测试里，原来要花1731ns的一个格式分析，用Protocol Buffer预定义之后，只要20几ns。所以，如果实现得足够好，我们认为MR系统不会只能处理文本格式的数据，而是可以处理二进制数据，因此效率还可以极大提升。”

除了这些之外，反方还抛出了几块大砖头，等着正方接招：
   - MR与存储系统无关，而且可以不用把数据装载到数据库就直接处理之，在很多场合下，在数据库系统把数据装载到数据库里头并且完成一次分析所花的时间，用MR的方式都能完成50次分析运算了。
   - MR可以表现更复杂的数据变换规则，很多反方的意见都是实现相关的，是针对一些不好的MR的实现做出来的，因此站不住脚。反方的最有力的证据就是，在Google里头跑得很好的一万多各种MR应用，从网页分析到索引建立，从日志分析到网图计算等等。

--------------------

正方的回应。作为正方，Michael Stonebraker 教授等人在同一期杂志上发表了另外一篇文章，很有趣的是刚好排在反方的文章之前。这篇文章以批评与自我批评的方式提出了若干有趣的观点，其中有些刚好是对反方的一个回应：MR系统可以用于（注意：不是胜出）下列场合：
   - ETL类的应用：从多个不同的源读取日志信息；分析以及清理日志数据；执行复杂的变换，比如“会话转换”；决定存储什么样的属性以及把信息装载到DBMS或者其他存储引擎中；
   - 复杂分析应用：这种挖掘类型的应用需要对数据进行多步骤的计算和处理，通常一个程序的输出会是另外一个程序的输入，因此很难用单个SQL语句来表示，这种应用场合下，MR是很好的候选方案；
   - 半结构化数据：因为MR不需要对数据的存储进行格式定义，因此MR比较适合处理半结构化数据，这些数据通常都是一些键值对。这些场合下，MR非常适合做 ETL的事情，如果并行数据库选用了面向列的存储方案，并且查询大多是分析性的查询，那么数据库方案依然是更好些的选择（正方有试验结果支撑）；
   - 快速实施的系统：并行数据库最大的缺点就是架设和调优难度要比MR大得多，虽然一旦架设、调优完毕，并行数据库系统表现出远胜MR的性能和特性，但对大多数急于上手的入门级用户来说，并行数据库系统的学习门槛显然要高得多。最后就是成本，虽然并行数据库在性能和应用编写简易性方面明显胜于MR系统，但现实世界里确实还缺乏完善和健壮的低成本开源解决方案，这点是MR最大的优点。数据库社区显然在这个方面输了一阵。

正方认为，把适合于数据库的工作交给MR去做结果其实并不好。在正方的试验里，证实了MR更加适用于做数据转换和装载的（ETL）工作，在这些场合，MR可以成为并行数据库的良好补充，而不是替代品。为了证明上述论点，正方做了一些有趣的试验，试验对比的双方是并行数据库集群和Hadoop集群，试验的主要内容有：
   - Grep任务：两个系统都对分布在100个节点上的1TB数据进行无法使用排序和索引的Grep处理，按说应该是面向更低层数据接口的Hadoop胜出，结果却出乎人们的意料，是并行数据库快了两倍左右。
   - Web 日志分析：两个系统都对分布在100个节点上的2TB数据进行类似GROUP BY的操作，对每个来源IP的点击和计费记录进行统计运算，这也是一个对所有数据进行扫描的操作，没有办法使用排序和索引。所以，直觉认为直接操作数据文件、更低层的Hadoop应该胜出，结果依然让人大跌眼镜，并行数据库胜出面甚至比Grep任务还要大。
   - 连接（Join）任务的性能：把上面测试的用户访问日志和另外一个包含18M URL的100GB的PageRank表连接起来。这个连接有两个子任务，分别对两个数据集进行复杂的计算。第一个子任务连接在一个特定用户数据范围内找出收入最高的IP地址，找到后再由第二个子任务连接计算这个范围内被访问页面的平均PageRank。数据库对付这种设计复杂连接的分析性查询是非常在行的。最后的结果是并行数据库比Hadoop快了21~36倍。

针对上面的结果，正方做了一些分析，认为这些差距的来源主要来自于具体实现，而非并行数据库模型和MR模型之间的差异。比如，MR可以使用并行数据库为低层的存储，所以所有分析都针对现实中两种模式的具体实现。正方分析了导致差距的几个实现相关的架构原因：
   - 数据解析。Hadoop需要用户代码来对输入的文本数据进行解析，然后再加以计算，而这个解析是每个Map和每个Reduce过程都要进行的，相比之下，并行数据库系统只在装载数据的时候解析一次数据，中间计算的开销大大降低。
   - 数据压缩。并行数据库系统使用数据压缩后，性能显著提升，而MR系统却不能，甚至倒退，在反方的试验中，也没有使用压缩，这方面让人感到奇怪，分析出来的可能原因是商业数据库系统对压缩的调优做得比较好，很多压缩算法，比如gzip，未经调优的话，在现代的CPU上，甚至都不能提供什么优势。
   - 管道化。现代数据库系统基本上都是先生成一个查询规划，然后在执行的时候把计算分发到相应节点上。在该计划里一个操作符必须向下一个操作符发送数据，不管下一个操作符是否在同节点上，因此，合格数据是由第一个操作符“推送”给第二个操作符的。这就构成了良好的从生产者到消费者的流水线作业。中间状态的数据不会写到磁盘上，这种运行时的“背压”会在生产者把消费者整崩溃之前把生产者停下来。这种流水线方式和MR的实现不同，MR是把中间状态写到一个本地的数据结构中，然后由消费者“拖取”。这种本地数据结构通常是相当庞大的，虽然这种做法可以在中间步骤上设置更多检查点，从而可以有更好的容错性，但很显然也引入了新的瓶颈。
   - 调度。在测试的并行数据库一方，查询规划是编译时生成，运行时执行。而MR的调度方案是运行时针对每个存储块，在处理节点上调度一次。这种对每个存储块一次的调度显然开销要大得多。当然，这种调度方式可以让MR适应不同的负载风格和不同性能的节点。
   - 面向列的存储。这个在对比双方的系统里都不存在。但却是并行数据库可以进一步提升的手段。

正方经过试验得出的结论是：MR和并行数据库结合是最好的方案，MR负责数据装载、转换等工作，并行数据库负责查询密集型的任务。正方最后发出的振聋发聩的呼吁是：很多事情并行数据库系统已经做得很好了，我们为什么不站在这个巨人的肩膀上？

*** MapReduce：一个重大的倒退
原文 http://apps.hi.baidu.com/share/detail/6912773

英文 http://www.databasecolumn.com/2008/01/mapreduce-a-major-step-back.html

认为MapReduce相对于数据管理系统是一个巨大退步。

MapReduce可能在某些特定类型的通用计算上是个不错的想法，但是对于数据库社区来说：
   - 从大规模数据应用程序模型来说是一个巨大的倒退。
   - 不是一个最优实现，因为它使用蛮力来代替索引。
   - 一点都不新奇，它只是实现了一个特定的25年前就有的众所周知的技术。
   - 失去了大部分目前数据库管理系统的特性。
   - 不能兼容所有目前数据库管理系统用户已经依赖的工具。

--------------------

MapReduce是一个数据库存取的退步。做为一个数据处理模型，MapReduce呈现出了一个巨大的退步。数据库社区从IBM在1968年第一次发布IMS以来的四十年中学到了以下三个经验：
   - 结构描述是好的。
   - 将结构描述从程序中分离是好的
   - 高阶的访问语言是好的
MapReduce没有吸引上面三个经验中的任何一个，而且还退步到了现在数据库管理系统发明前的60年代。

数据库管理系统社区学习到的关于最重要的结构描述就是：记录的字段和它的数据类型都记录在存储系统中。更重要的是，数据库管理系统的运行时可以保证所有的记录都遵守结构描述。这是避免将垃圾数据添加到数据集中的最好的方法。MapReduce没有这样的方法，也没有避免将垃圾数据添加到数据集中的控制。一个毁坏的数据集可以悄无声息的破坏整个使用这个数据集的MapReduce程序。

将数据描述与程序分离也很关键。如果开发者想在一个数据集上开发一个新的程序，他必须先去了解记录结构。在现代数据库管理系统中，结构描述存储在系统目录中，而且可以被用户用SQL查询来了解它的结构。与此相反的是，如果数据描述不存在，或者隐藏在程序之中，开发者要了解这个数据结构必须通过检查原有的代码。这个工作不仅仅是非常沉闷的，而且开发者必须先找到这个程序的源代码。如果没有相应的结构描述存在，后面的这个沉闷的问题将在所有的MapReduce程序中存在。

在1970年数据库管理系统社区，关系型数据库支持者和数据系统语言协会(Codasyl)支持者进行了一场“剧烈的辩论”。其中一个最大的争议是数据库管理系统的访问程序以何种方式访问：
   -　用统计来获取你想要的数据(关系型的观点)
   - 提供一个算法来进行数据访问(Codasyl的观点)
争论的结果已经是古代史了，但是整个世界都看到了高阶语言的价值以及关系型系统的胜利。以高阶语言的形式编程更加容易编写，易于修改，而且方便一个新来者的理解。Codasyl被批判为“以汇编语言的形式来对数据库管理系统进行访问”。MapReduce程序员有点类似Codasyl程序员。他们用低阶的语言来处理低阶记录。没有人提倡回归汇编语言，类似的，不应该强制任何人用MapReduce来编程。

--------------------

MapReduce是一个粗糙的实现。所有现在数据库管理系统使用hash或者B-tree来索引加快对数据的访问。如果一个用户在查找一个记录集的子记录集（比如雇员中谁的薪水在10000或者谁在鞋生产部门），那么他可以使用索引来有效的缩减查找范围。另外，还提供了一个查询优化器来决定到底是使用索引还是进行一个残忍野蛮的顺序查询。MapReduce没有索引，理所当然的只能使用蛮力来作为处理选项。而不管索引在当前情况下是否是一个最好的访问机制。

一个值得争论的是，MapReduce提出的自动的在计算机集群中提供并行计算的价值。其实这个特性在1980年时代就被数据库管理系统研究社区研究过了，多个原型被提出来，比如Gamma，Bubba和Grace。商业化的利用这些思想在系统则在80年代末期，比如Teradata。概括起来说，在前20年已经出现了高性能，商业化的，面向网格计算机群的SQL引擎（带结构描述和索引）。MapReduce跟这些系统相比并没有那么好。

MapReduce同时存在很多底层的实现问题，特别是数据交换和数据斜交的情况。
   - 一个因素是MapReduce支持者好像没有注意到关于数据斜交的问题。就像在“平行数据库系统：未来的高性能数据库系统”中提到的，数据斜交是构建成功高扩展性并行查询系统的巨大障碍。这个问题重现在map阶段，当拥有相同键的数据拥有大幅度差异的时候。这个差异，反过来导致某些reduce实例花费比其它实例更长甚至常很多的时间来运行。结果就是计算的运行时间由速度最慢的那个reduce实例决定。平行数据库社区已经广泛的研究了这个问题并且拥有了成熟的，MapReduce社区可能愿意采纳的解决方案。
   - 还有第二个严重的性能问题被MapReduce支持者掩盖了。回忆N个map实例中的每个实例都将生成M个输出文件。每个都分发给不同的reduce实例。这些文件都被写入本地硬盘以备map实例使用。如果N是1000，M是500，那么在map阶段将生成500000个本地文件。当reduce阶段开始，500个reduce实例必须读取1000个输入文件，必须使用类似FTP的协议将每个输入文件从各个map实例运行的节点中获取（pull）过来。在100秒内所有reduce实例将同时的运行起来，不可避免的会发生两个或者更多个reduce实例企图并行的从同一个map节点中获取输入文件，包括大量的磁盘搜索，当超过因子20时，将极大的降低磁盘的有效传输率。这就是为什么并行数据库系统不实现分割文件，而使用推(push to sockets)来代替拉(pull)。因为MapReduce通过实现分割文件来获得优秀的容错性，不好说如果MapReduce框架修改成使用推(push)模型是否会成功。

鉴于实验评估，我们严重的怀疑MapReduce在大规模应用中会表现的很好。MapReduce的实现者还需要好好的研究过去25年来并行数据库管理系统的研究文献。

--------------------

MapReduce并不新奇。MapReduce社区看起来感觉他们发现了一个全新的处理大数据集的模型。实际上，MapReduce所使用的技术至少是20年前的。将大数据集划分为小数据集的思想是在Kitsuregawa首次提出的“Application of Hash to Data Base Machine and Its Architecture”的基础上发展出来的一个新的连接算法。在“Multiprocessor Hash-Based Join Algorithms”中，Gerber演示了如何将Kitsuregawa的技术扩展到使用联合分区表，分区执行以及基于hash的分割来连接并行的无共享集群。DeWitt演示了如何采用这些技术来执行有group by子句以及没有group by子句的并行聚合。DeWitt和Gray描述了并行数据库系统以及他们如何处理查询。Shatdal和Naughton探索了并行聚合的替代策略。

Teradata已经出售利用这些技术构建的数据库管理系统20多年了，而这些技术正是MapReduce一伙声称的发明的技术。当然MapReduce提倡者将毫无疑问的声称他们编写的MapReduce函数实现他们的软件与使用并行SQL实现有多么大的不同，我们必须提醒他们，POSTGRES已经在80年代中期就支持了用户自定义函数以及用户自定义聚合。本质上来说，从1995年Illustra引擎开始算，所有现代数据库系统都提供了类似的功能很长一段时间了。

--------------------

MapReduce失去了很多特性。所有下面的特性都被现在的数据库管理系统提供了，而MapReduce没有：
   - 批量导入 将输入数据转化成想要的格式并加载到数据库中
   - 索引 如上文所述
   - 更新 改变数据集中的数据
   - 事务 支持并行更新以及从失败的更新中恢复
   - 完善的约束 防止垃圾数据添加到数据集
   - 完善的引用 类似FK，防止垃圾数据的存在
   - 视图 底层逻辑数据描述可以改变但不需要重写程序
简单的说来，MapReduce只提供了现在数据库管理系统的函数性功能。

--------------------

MapReduce与现有的数据库管理系统工具不兼容。一个现代的SQL数据库管理系统都拥有如下可用的工具：
   - 报表 (比如水晶报表) 将数据友好的展示给人
   - 商业智能工具 (比如Business Objects or Cognos)允许在数据仓库中进行特定查询
   - 数据挖掘工具 (比如Oracle Data Mining)允许用户在大数据集中发现数据规律
   - 复制工具 允许用户在不同的数据库中进行复制传输
   - 数据库设计工具 帮助用户构建数据库
MapReduce不能使用这些工具，同时它也没有自己的工具。直到它能与SQL兼容或者有人编写了这些工具，MapReduce仍然在端到端的任务中显得十分困难。

*** Best Practices for Selecting Apache Hadoop Hardware
http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/

RAID cards, redundant power supplies and other per-component reliability features are not needed. Buy error-correcting RAM and SATA drives with good MTBF numbers. Good RAM allows you to trust the quality of your computations. Hard drives are the largest source of failures, so buy decent ones.（不需要选购RAID，冗余电源或者是一些满足高可靠性组件，但是选择带有ECC的RAM以及good MTBF的SATA硬盘却是非常需要的。ECC RAM可以让你确保计算结果的正确性，而SATA故障是大部分故障的主要原因）

   - On CPU: It helps to understand your workload, but for most systems I recommend sticking with medium clock speeds and no more than 2 sockets. Both your upfront costs and power costs rise quickly on the high-end. For many workloads, the extra performance per node is not cost-effective.（没有特别要求，普通频率，dual-socket？？？）
   - On Power: Power is a major concern when designing Hadoop clusters. It is worth understanding how much power the systems you are buying use and not buying the biggest and fastest nodes on the market.In years past we saw huge savings in pricing and significant power savings by avoiding the fastest CPUs, not buying redundant power supplies, etc.  Nowadays, vendors are building machines for cloud data centers that are designed to reduce cost and power and that exclude a lot of the niceties that bulk up traditional servers.  Spermicro, Dell and HP all have such product lines for cloud providers, so if you are buying in large volume, it is worth looking for stripped-down cloud servers. （根据自己的需要尽量减少能耗开销，撇去一些不需要的部件。而且现在很多厂商也在尽量减少不必要的部件）
   - On RAM: What you need to consider is the amount of RAM needed to keep the processors busy and where the knee in the cost curve resides. Right now 48GB seems like a pretty good number. You can get this much RAM at commodity prices on low-end server motherboards. This is enough to provide the Hadoop framework with lots of RAM (~4 GB) and still have plenty to run many processes. Don’t worry too much about RAM, you’ll find a use for it, often running more processes in parallel. If you don’t, the system will still use it to good effect, caching disk data and improving performance.（RAM方面的话越大越好，对于48GB的RAM来说普通的主板也是支持的。如果RAM用的上的话那么允许多个进程并行执行，如果暂时永不上的话可以做cache来提高速度）
   - On Disk: Look to buy high-capacity SATA drives, usually 7200RPM. Hadoop is storage hungry and seek efficient but it does not require fast, expensive hard drives. Keep in mind that with 12-drive systems you are generally getting 24 or 36 TB/node. Until recently, putting this much storage in a node was not practical because, in large clusters, disk failures are a regular occurrence and replicating 24+TB could swamp the network for long enough to really disrupt work and cause jobs to miss SLAs. The most recent release of Hadoop 0.20.204 is engineered to handle the failure of drives more elegantly, allowing machines to continue serving from their remaining drives. With these changes, we expect to see a lot of 12+ drive systems. In general, add disks for storage and not seeks. If your workload does not require huge amounts of storage, dropping disk count to 6 or 4 per box is a reasonable way to economize.（高容量SATA硬盘，最好是7.2KRPM，并且最好单机上面挂在12个硬盘。对于hadoop之前这种方式并不实际，因为磁盘非常容易损坏并且备份这24TB的数据非常耗时。而hadoop可以很好地解决这个问题。
小集群来说的话，通常单个机器上面挂在4-6个disk即可）
   - On Network: This is the hardest variable to nail down. Hadoop workloads vary a lot. The key is to buy enough network capacity to allow all nodes in your cluster to communicate with each other at reasonable speeds and for reasonable cost. For smaller clusters, I’d recommend at least 1GB all-to-all bandwidth, which is easily achieved by just connecting all of your nodes to a good switch. With larger clusters this is still a good target although based on workload you can probably go lower. In the very large data centers the Yahoo! built, they are seeing 2*10GB per 20 node rack going up to a pair of central switches, with rack nodes connected with two 1GB links. As a rule of thumb, watch the ratio of network-to-computer cost and aim for network cost being somewhere around 20% of your total cost. Network costs should include your complete network, core switches, rack switches, any network cards needed, etc. We’ve been seeing InfiniBand and 10GB Ethernet networks to the node now. If you can build this cost effectively, that’s great. However, keep in mind that Hadoop grew up with commodity Ethernet, so understand your workload requirements before spending too much on the network.（这个主要还是看需求。通常来说网络整体开销占据所有开销的20%，包括核心交换机，机架之间的交换机以及网卡设备等。yahoo大集群的部署方式是rack之间使用2*10GB的核心交换机工作，而20个节点的rack之间内部使用1GB链路）。

*** The dark side of Hadoop - BackType Technology
http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop

谈到了一些在使用hadoop出现的一些问题，而这些问题是hadoop本身的。
   - Critical configuration poorly documented 一些关键的参数和配置并没有很好地说明清楚。
   - Terrible with memory usage 内存使用上面存在问题。hadoop里面有一些非常sloppy的实现，比如chmod以及ln -s等操作，并没有调用fs API而是直接创建一个shell进程来完成。因为fork出一个shell进程需要申请同样大小的内存（虽然实现上是COW），但是这样造成jvm出现oom。解决的办法是开辟一定空间的swap The solution to these memory problems is to allocate a healthy amount of swap space for each machine to protect you from these memory glitches. We couldn't believe how much more stable everything became when we added swap space to our worker machines.
     - Thomas Jungblut's Blog: Dealing with "OutOfMemoryError" in Hadoop http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html 作者给出的解决办法就是修改hadoop的代码，通过调用Java API而不是使用ProcessBuilder来解决。
     - *NOTE(dirlt):出现OOM的话必须区分JVM还是Linux System本身的OOM。JVM出现OOM是抛出异常，而Linux出现OOM是会触发OOM killer* 
   - Zombies hadoop集群出现一些zombie进程，而这些进程会一直持有内存直到大量zombie进程存在最后需要重启。造成这些zombie进程的原因通常是因为jvm oom（增加了swap之后就没有出现这个问题了），但是奇怪的是tasktracker作为这些process的parent，并不负责cleanup这些zombie进程而是依赖这些zombie进程的自己退出，这就是hadoop设计方面的问题。

Making Hadoop easy to deploy, use, and operate should be the #1 priority for the developers of Hadoop.

** 使用问题
*** 搭建单节点集群
搭建单节点集群允许我们在单机做一些模拟或者是测试，还是非常有意义的。如何操作的话可以参考链接 http://z/home/dirlt/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html
这里稍微总结一下：
   - 首先安装ssh和rsync # sudo apt-get install ssh &&  sudo apt-get install rsync 
   - 本机建立好信任关系 # cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
   - 将{hadoop-package}/conf配置文件修改如下：
   - conf/core-site.xml
#+BEGIN_SRC XML
<configuration>
     <property>
         <name>fs.default.name</name>
         <value>hdfs://localhost:9000</value>
     </property>
</configuration>
#+END_SRC
   - conf/hdfs-site.xml
#+BEGIN_SRC XML
<configuration>
     <property>
         <name>dfs.replication</name>
         <value>1</value>
     </property>
</configuration>
#+END_SRC

   - conf/mapred-site.xml
#+BEGIN_SRC XML
<configuration>
     <property>
         <name>mapred.job.tracker</name>
         <value>localhost:9001</value>
     </property>
</configuration>
#+END_SRC
   - 格式化namenode # bin/hadoop namenode -format
   - 启动hadoop集群 # bin/start-all.sh
   - 停止hadoop集群 # bin/stop-all.sh
   - webconsole
     -  NameNode - http://localhost:50070/ 
     -  JobTracker - http://localhost:50030/

*** OutOfMemoryError
   - hadoop的mapreduce作业中经常出现Java heap space解决方案 http://blog.sina.com.cn/s/blog_6345041c01011bjq.html
   - Hadoop troubleshooting http://ww2.cs.fsu.edu/~czhang/errors.html
   - Thomas Jungblut's Blog: Dealing with "OutOfMemoryError" in Hadoop http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html
   - NoSQL | Hadoop http://www.nosql.se/tags/hadoop/

总结起来大致就是以下几种原因吧：
   - Increase the heap size for the TaskTracker, I did this by changing HADOOP_HEAPSIZE to 4096 in /etc/hadoop/conf/hadoop-env.sh to test.  This did not solve it.（增加TaskTracker的heapsize）
   - Increase the heap size for the spawned child.  Add -Xmx1024 in mapred-site.xml for mapred.map.child.java.opts.  This did not solve it. （增加task的heapsize）
   - Make sure that the limit of open files is not reached, I had already done this by adding “mapred – nofile 65536″ in /etc/security/limits.conf.  This did not solve it. （增加文件数目限制）
   - Adding the following to /etc/security/limits.conf and restarting the TaskTracker solved it "mapred – nproc 8192" （增加开辟子进程的数目） 

*** topology rack awareness
有两种方式实现，主要是实现DNS-name/IP到network path映射，network path是如下格式的字符串
   - /switch/rack

第一种可以通过设置topology.node.switch.mapping.impl来设定DNSToSwitchMapping类
#+BEGIN_SRC Java
public interface DNSToSwitchMapping {
  public List<String> resolve(List<String> names);
}
#+END_SRC
实现这个类来完成DNS-name/IP-name到network path的映射.

但是存在另外一种更好的办法就是ScriptBasedMapping，这个是DNSToSwichMapping的一个实现，可以通过配置脚本来做映射。
将属性topology.script.filename设置成为脚本，脚本输入names，然后返回结果是按照空格或者是回车分隔的列表即可。
*NOTE(dirlt):内部使用StringTokenizer来拆分结果*

** Topic
*** Scheduler
   - Fair Scheduler Guide http://archive.cloudera.com/cdh/3/hadoop/fair_scheduler.html
   - Job Scheduling in Hadoop | Apache Hadoop for the Enterprise | Cloudera http://www.cloudera.com/blog/2008/11/job-scheduling-in-hadoop/
   - Understanding Apache Hadoop’s Capacity Scheduler | Hortonworks http://hortonworks.com/blog/understanding-apache-hadoops-capacity-scheduler/
   - Upcoming Functionality in “Fair Scheduler 2.0″ | Apache Hadoop for the Enterprise | Cloudera http://www.cloudera.com/blog/2009/04/upcoming-functionality-in-fair-scheduler-20/

** Hadoop权威指南
*** 初识Hadoop
古代，人们用牛来拉中午，当一头牛拉不动一根圆木的时候，他们不曾想过培育更大更壮的牛。同样，我们也不需要尝试开发超级计算机，而应试着结合使用更多计算机系统。

*** 关于MapReduce
   - 设置HADOOP_CLASSPATH就可以直接使用hadoop CLASSNAME来在本地运行mapreduce程序。
   - hadoop jar $HADOOP_HOME/contrib/streaming/hadoop-streaming-0.20.2-cdh3u3.jar 可以用来启动streaming任务
     - 使用stdin/stdout来作为输入和输出
     - *NOTE（dirlt）：倒是可以探索一下如何使用，但是觉得能力有限*
       - Input/Output Format
       - 外围环境的访问比如访问hdfs以及hbase
       - 程序打包。比如使用很多第三方库的话在其他机器上面没有部署。 
   - hadoop pipes 可以用来启动pipes任务
     - Hadoop的Pipes是Hadoop MapReduce的C++接口代称
     - 使用Unix Domain Socket来作为输入和输出
     - *NOTE（dirlt）：可能使用上面还是没有native mr或者是streaming方式方便*

*** Hadoop分布式文件系统
  - 使用hadoop archive能够将大量小文档打包，存档文件之能够只读访问
    - 使用hadoop archive -archiveName <file>.har -p <parent-path> src dst
    - 存档过程使用mapreduce完成，输出结果为目录
      - part-0 表示存档内容文件，应该是使用一个reduce做聚合。
      - _index,_masterindex 是对存档内容文件的索引文件。
    - har(hadoop archive)文件系统是建立在其他文件系统上面的，比如hdfs或者是local fs.
      - hadoop fs -ls har:///file.har 那么访问的是默认的文件系统上面的file.har
      - 如果想显示地访问hdfs文件系统的话，那么可以hadoop fs -ls har://hdfs-localhost:9000/file.har
      - 如果想显示地访问本地文件系统的话，那么可以使用hadoop fs -ls har://file-localhost/file.har
      - hadoop fs -ls har://schema-<host>/<path> 是通用的访问方式
	
*** Hadoop IO
--------------------
   - 文件系统
     - ChecksumFileSystem 
       - 使用decorator设计模式，底层filesystem称为RawFileSystem
       - 对于每个文件filename都会创建.filename.crc文件存储校验和
       - 计算crc的单位大小通过io.bytes.per.checksum来进行控制
       - 读取文件如果出现错误的话，那么会抛出ChecksumException
       - 考虑到存在多副本的情况，如果读取某个副本出错的话，期间那么会调用reportChecksumFailure方法
	 - *NOTE（dirlt）：这个部分的代码不太好读，非常绕*    
     - RawLocalFileSystem
       - 本地文件系统
     - LocalFileSystem
       - RawLocalFileSystem + ChecksumFileSystem
       - reportChecksumFailure实现为将校验和存在问题的文件移动到bad_files边际文件夹（side directory）
     - DistributedFileSystem
       - 分布式文件系统
     - ChecksumDistributedFileSystem
       - DistributedFileSystem + ChecksumFileSystem

--------------------
   - 压缩解压
     - DEFLATE org.apache.hadoop.io.compress.DefaultCodec 扩展名.defalte
     - Gzip org.apache.hadoop.io.compress.GzipCodec 扩展名.gz 使用DEFLATE算法但是增加了额外的文件头。
     - bzip2 org.apache.hadoop.io.compress.BZip2Codec 扩展名.bz2 自身支持文件切分，内置同步点。
     - LZO com.hadoop.compression.lzo.LzopCodec 扩展名.lzo 和lzop工具兼容，LZO算法增加了额外的文件头。
       - LzopCodec则是纯lzo格式的codec,使用.lzo_deflate作为文件扩展名
       - 因为LZO代码库拥有GPL许可，因此没有办法包含在Apache的发行版本里面。
     - 运行MapReduce时候可能需要针对不同压缩文件解压读取，就需要构造CompressionCodec对象，我们可以通过CompressionCodecFactory来构造这个对象
       - CompressionCodecFactory读取变量io.compression.codecs
       - 然后根据输入文件的扩展名来选择使用何种codec.
       - getDefaultExtension
     - 压缩和解压算法可能同时存在Java实现和原生实现
       - 如果是原生实现的话通常是.so，那么需要设置java.library.path或者是在环境变量里面设置LD_LIBRARY_PATH
       - 如果同时有原生实现和Java实现，我们想只是使用原生实现的话，那么可以设置hadoop.native.lib = false来禁用原生实现。
     - 压缩算法涉及到对应的InputFormat,也就涉及到是否支持切分
       - 对于一些不支持切分的文件，可能存在一些外部工具来建立索引，从而支持切分。
     - 下面这些选项可以针对map结果以及mapreduce结果进行压缩
       - mapred.output.compress = true 将mapreduce结果做压缩
       - mapred.output.compression.codec mapreduce压缩格式
       - mapred.output.compress.type = BLOCK/RECORD 如果输出格式为SequenceFile的话，那么这个参数可以控制是块压缩还是记录压缩
       - *NOTE（dirlt）：我现在强烈感觉MR的中间结果存储格式为SequenceFile* 
       - *NOTE（dirlt）：应该是IFile，但是是否共享了这个配置呢？*
       - mapred.compress.map.output = true 将map结果做压缩
       - mapred.map.output.compression.codec map压缩格式

--------------------
   - 序列化
     - Hadoop的序列化都是基于Writable实现的，WritableComparable则是同时继承Writable,Comparable<T>.
     - 序列化对象需要实现RawComparator，接口为public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)进行二进制比较。
       - WritableComparator简化了这个实现，继承WritableComparator就实现了这个接口
       - 但是这个接口实现起来非常naive，就是将两个byte stream反序列化然后调用对象的compareTo实现
       - 如果想要提高效率的话，可以考虑通过直接比较两个byte stream来做优化。
     - 基于文件的数据结构
       - SequenceFile 主要用来存储KV数据结构，多条记录之间会穿插一些同步标记，因此允许进行切分。
	 - 使用SequenceFileInputFormat和SequenceFileOutputFormat来读取和输出SequenceFile
         - hadoop fs -text 可以用来读取文件
	 - mapred.output.compress.type = BLOCK/RECORD 可以用来控制压缩方式
           - 如果没有使用压缩的话，那么格式为 recordLength(4byte) + keyLength(4byte) + key + value
           - 如果使用记录压缩的话，那么格式为 recordLnegth(4byte) + keyLength(4byte) + key + compressedValue
           - 如果使用块压缩的话，那么格式为 numberRecord(1-5byte) + keyLength(4byte) + compressedKeys + valueLength(4byte) + compressedValues.每个block之间会插入sync标记
	   - 块压缩大小可以使用io.seqfile.compress.blocksize来控制，默认1MB
       - MapFile 也是用来存储KV数据结构，但是可以认为已经按照了Key进行排序 *NOTE（dirlt）：要求添加顺序就按照Key排序*
	 - 存储格式实际上也是SequenceFile，data，index都是。
	 - 底层会建立index，index在搜索的时候会加载到内存里面，这样可以减少data上的随机查询次数。
         - 使用io.map.index.interval可以控制多少个item在index里面创建一个条目
	 - 使用io.map.index.skip = 0/1/2/n 可以控制skip几个index的item，如果为1的话那么表示只是使用1/2的索引。
         - 从SequenceFile创建MapFile非常简单
	   - 首先使用sort将SequenceFile进行排序(可以使用hadoop example的sort）
           - 然后调用hadoop MapFileFixer来建立索引

*** MapReduce应用开发
--------------------
   - Configuration用来读取配置文件，功能还是比较强大的，有变量替换的功能
     - <property><name>...</name><value>...</value></property>
     - 如果使用<final>true</final>标记的话那么这个变量不允许被重置
     - 变量替换可以使用${variable}
     - 通过addResource来添加读取的配置文件

--------------------
   - Hadoop集群有三种工作方式，分别为
     - standalone 使用单个JVM进程来模拟
       - 如果不进行任何配置的话默认使用这个模式 *NOTE（dirlt）：这个模式确实不错*
       - fs.default.name = file 本地文件系统
       - mapred.job.tracker = local
     - pseudo-distributed 本地启动单节点集群
       - fs.default.name = hdfs://localhost
       - mapred.job.tracker = localhost:8021 
     - fully-distributed 完全分布式环境
       - fs.default.name = hdfs://<namenode>
       - mapred.job.tracer = <jobtracker>:8021

--------------------
   - 使用hadoop启动MapReduce任务的常用参数
     1. -D property=value 覆盖默认配置属性
     2. -conf filename 添加配置文件
     3. -fs uri 设置默认文件系统
     4. -jt host:port 设置jobtracker
     5. -files file,file2 这些文件可以在tasktracker工作目录下面访问
     6. -archives archive,archive2 和files类似，但是是存档文件
	- 突然觉得这个差别在files只能是平级结构，而archive可以是层级结构。
     7. -libjars jar1,jar2 和files类似，通常这些JAR文件是MapReduce所需要的。

--------------------
如果希望运行时候动态创建集群的话，可以通过这几个类来创建
   - MiniDFSCluster
   - MiniMRCluster
   - MiniHBaseCluster 
   - MiniZooKeeperClutser
   - *NOTE(dirlt):都称为Mini???Cluster？*
另外还有自带的ClusterMapReduceTestCase以及HBaseTestingUtility来帮助进行mapreduce的testcase. 这些类散步在hadoop,hbase,hadoop-test以及hbase-test里面。 

*NOTE（dirlt）：但是个人觉得可能还是没有本地测试方便，不过倒是可以试试* 

--------------------
job，task and attempt
   - jobID常见格式为 job_200904110811_0002
     - 其中200904110811表示jobtracker从2009.04.11的08:11启动的
     - 0002 表示第三个job,从0000开始计数。超过10000的话就不能够很好地排序
   - taskID常见格式为 task_200904110811_0002_m_000003
     - 前面一串数字和jobID匹配，表示从属于这个job
     - m表示map任务，r表示reduce任务
     - 000003表示这是第4个map任务。顺序是在初始化时候指定的，并不反应具体的执行顺序。
   - attemptID常见格式为 attempt_200904110811_0002_m_000003_0
     - 前面一串数字和taskID匹配，表示从属与这个task
     - attempt出现的原因是因为一个task可能会因为失败重启或者是预测执行而执行多次
     - 如果jobtracker重启而导致作业重启的话，那么做后面id从1000开始避免和原来的attempt冲突。

--------------------
作业调试

   - 相关配置
     - mapred.jobtracker.completeuserjobs.maximum 表示web页面下面展示completed jobs的个数，默认是100，超过的部分放到历史信息页。
     - mapred.jobtracker.restart.recover = true jobtracker重启之后自动恢复作业
     - hadoop.job.history.location 历史作业信息存放位置，超过30天删除，默认在_logs/history
     - hadoop.job.history.user.location 如果不为none那么历史作业信息在这里也会存在一份，不会删除。 
   - 相关命令
     - hadoop fs -getmerge <src> <dst> 能够将hdfs的src下面所有的文件merge合并成为一份文件并且copy到本地
     - hadoop job -history 察看作业历史
     - hadoop job -counter 察看作业计数器
   - 相关日志
     - 系统守护进程日志 写入HADOOP_LOG_DIR里面，可以用来监控namenode以及datanode的运行情况
     - MapReduce作业历史日志 _logs/history
     - MapReduce任务日志 写入HADOOP_LOG_DIR/userlogs里面，可以用来监控每个job的运行情况
   - 分析任务
     - JobConf允许设置profile参数 *NOTE（dirlt）：新的接口里面JobConf->JobContext->Job，Job没有这些接口，但是可以通过Configuration来设置*
       - setProfileEnabled 打开profile功能，默认false，属性 mapred.task.profile
       - setProfileParams 设置profile参数
	 - 属性 mapred.task.profile.params
         - 默认使用hprof -agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s"
	 - 其中%s会替换成为profile输出文件
         - *NOTE（dirlt）：其实这里似乎也可以设置成为jmxremote来通过jvisualvm来调试*
       - setProfileTaskRange(boolean,String)
	 - 参数1表示针对map还是reduce task做profile, true表示map, false表示reduce
	 - 参数2表示针对哪些tasks做优化，"0-2"表示针对0，1，2三个任务，默认也是"0-2"
	 - map task对应属性mapred.task.profile.maps，reduce task对应属性mapred.task.profile.reduces
   - 任务重现
     - 首先将keep.failed.task.files设置为true,这样如果任务失败的话，那么这个任务的输入和输出都会保留下来
       - 如果是map任务的话，那么输入分别会在本地保留
       - 如果是reduce任务的话，那么对应的map任务输出会在本地保留
       - 然后我们使用hadoop IsolationRunner job.xml来重新运行这个任务
       - 可以修改HADOOP_OPTS添加远程调试选项来启动这个任务。
     - 如果希望任务都保留而不仅仅是失败任务保留的话，那么可以设置 keep.task.files.pattern 为正则表达式（与保留的任务ID匹配）

*** MapReduce的工作机制
--------------------
Hadoop运行MapReduce作业的工作原理

file:./images/mapreduce-workflow-architecture.png


其中有几点需要注意的：
   - 计算分片信息是在本地完成的，分片信息和其他resouce(包括jars,files,archives等）一起copy到HDFS上面，然后jobtracker直接读取分片信息。
   - 提交的资源可以设置replication数目，高副本数目可以缓解tasktracker获取resource的压力。参数是mapred.submit.replication.
   - 对于streaming以及pipes的实现，无非就是task并不直接执行任务，而是开辟另外一个子进程来运行streaming或者是pipes的程序。

file:./images/mapreduce-streamming-pipes.jpg

--------------------
进度和状态的更新
   - map任务进度是已经处理输入的比例
   - reduce任务进度分为三个部分
     - shuffle 1/3
     - sort 1/3
     - reduce 1/3
     - 也就是说如果刚运行完成sort的话，那么进度是2/3
   - 状态的更新
     - 触发事件
       - 读取记录
       - 输出记录
       - 修改状态 reporter的setStatus
       - 计数器修改
       - reporter的progress 
     - 子进程有单独线程每隔3秒检查progress位是否设置，如果设置的话那么和tasktracker发起心跳
       - 通过mapred.task.timeout控制
     - tasktracker每隔5秒和jobtracker做心跳
       - 心跳时间通过 mapred.tasktracker.expircy.interval 设置
     - jobClient定期会去jobtracker询问job是否完成
       - jobClient也可以设置属性job.end.notification.url,任务完成jobtracker会调用这个url
       - 可以认为就是推拉方式的结合。
       
--------------------
失败检测和处理
   - 任务失败
     - 子进程抛出异常的话，tasktracker将异常信息记录到日志文件然后标记失败
     - 对于streaming任务的话非0退出表示出现问题，也可以使用stream.non.zero.exit.is.failure = false来规避（ *这样是否就没有办法判断是否正常退出了？* ）
     - 如果长时间没有响应的话，没有和tasktracker有交互，那么也会认为失败。这个时间使用mapred.task.timeout控制，默认10min
     - 如果任务失败的话，jobtracker会尝试进行多次重试
       - map重试次数通过 mapred.map.max.attempts 配置
       - reduce重试次数通过 mapre.reduce.max.attempts 配置
       - *任何任务重试超过4次的话那么会认为整个job失败*
     - 另外需要区分KILLED状态和FAILED状态，对于KILLED状态可能是因为推测执行造成的，不会记录到failed attempts里面
     - 如果我们希望允许少量任务失败的话，那么可以配置
       - mapred.max.map.failures.percent 允许map失败的最大比率
       - mapred.max.reduce.failures.percent 允许reduce失败的最大比率

--------------------
作业的调度
   - fifo scheduler
     - 可以通过mapred.job.priority或者是setJobPriority设置
     - 当队列中有空闲的槽位需要执行任务时，从等待队列中选择优先级最高的作业
   - fair scheduler
   - capacity scheduler

--------------------
shuffle和排序

file:./images/mapreduce-shuffle-sort.jpg

 有下面这些参数控制shuffle和sort的过程 *NOTE（dirlt）：书上倒是有很多参数，但是好多还是不太理解*
    - io.sort.mb map输出缓存空间大小，默认是100MB
    - io.sort.spill.percent 如果map输出超过了缓存空间大小的这个阈值的话，那么就会spill,默认是0.8
      - 每次spill之前先会对这个文件进行排序，如果有combiner的话那么会在上面调用combiner
      - 写磁盘是按照轮询的方式写到mapred.local.dir属性指定的目录下面
    - io.sort.factor 多路归并的数量，默认是10
      - 在map阶段，因为最终会存在多个spill文件，所以需要做多路归并。 *TODO（dirlt）：如果归并数量少的话是否可能会多次merge？*
      - 在reduce阶段的话，因为可能存在多路map输出的结果，所以需要做多路归并。
    - min.num.spill.for.combine 如果指定combiner并且spill次数超过这个值的话就会调用combine,默认为3
    - tracker.http.threads reduce通过HTTP接口来发起数据请求，这个就是HTTP接口相应线程数目，默认为40
    - mapred.reduce.parallel.copies reduce启动多少个线程去请求map输出，默认为5

--------------------
任务的执行
   - 推测执行参数
     - 如果某个任务执行缓慢的话会执行另外一个备份任务
     - mapred.map.tasks.speculative.execution true
     - mapred.reduce.tasks.speculative.execution true
   - JVM重用
     - 一个JVM实例可以用来执行多个task.
     - mapred.job.reuse.jvm.num.tasks/setNumTasksToExecutePerJvm 单个JVM运行任务的最大数目
     - -1表示没有限制
   - 任务执行环境
     - 程序自身可以知道执行环境对于开发还是比较有帮助的
     - 这些属性对于streaming可以通过环境变量获得
     - mapred.job.id string jobID
     - mapred.tip.id string taskID
     - mapred.task.id string attemptID
     - mapred.task.partition int 作业中任务编号
     - mapred.task.is.map boolean 是否为map
     - mapred.work.output.dir / FileOutputFormat.getWorkOutputPath 当前工作目录
      
*** MapReduce的类型与格式
--------------------
MapReduce的类型

老API里面还有MapRunner这个类，这个类主要的作用是可以用来控制Mapper运行的方法，比如可以多线程来控制Mapper的运行。
但是在新API里面已经完全集成到Mapper实现里面来了，用户可以重写两个方法来完全控制mapper的运行
   - map 如何处理kv
   - run 如何从context里面读取kv
#+BEGIN_SRC Java
  protected void map(KEYIN key, VALUEIN value, 
                     Context context) throws IOException, InterruptedException {
    context.write((KEYOUT) key, (VALUEOUT) value);
  }
  public void run(Context context) throws IOException, InterruptedException {
    setup(context);
    while (context.nextKeyValue()) {
      map(context.getCurrentKey(), context.getCurrentValue(), context);
    }
    cleanup(context);
  }
#+END_SRC
*NOTE（dirlt）：觉得这个特性不是特别有用*
   - mapred.input.format.class setInputFormat
   - mapred.mapoutput.key.class setMapOutputKeyClass
   - mapred.mapoutput.value.class setMapOutputValueClass
   - mapred.output.key.class setOutputKeyClass
   - mapred.output.value.class setOutputValueClass
   - mapred.mapper.class setMapperClass
   - mapred.map.runner.class setMapRunnerClass
   - mapred.combiner.class setCombinerClass
   - mapred.partitioner.class setPartitionerClass
   - mapred.output.key.comparator.class setOutputKeyComparatorClass
   - mapred.output.value.groupfn.class setOutputValueGroupingComparator
   - mapred.reducer.class setReducerClass
   - mapred.output.format.class setOutputFormat

--------------------
输入格式

对于InputFormat来说包含两个任务
   - 根据job描述来对输入进行切片（InputSplit）
   - 根据切片信息来读取记录（RecordReader）
#+BEGIN_SRC Java
public abstract class InputFormat<K, V> {
  public abstract 
    List<InputSplit> getSplits(JobContext context
                               ) throws IOException, InterruptedException;
  
   public abstract 
    RecordReader<K,V> createRecordReader(InputSplit split,
                                         TaskAttemptContext context
                                        ) throws IOException, 
                                                 InterruptedException;

}

public abstract class InputSplit {
  public abstract long getLength() throws IOException, InterruptedException;

  public abstract 
    String[] getLocations() throws IOException, InterruptedException;
}

public abstract class RecordReader<KEYIN, VALUEIN> implements Closeable {
  public abstract void initialize(InputSplit split,
                                  TaskAttemptContext context
                                  ) throws IOException, InterruptedException;

  public abstract 
  boolean nextKeyValue() throws IOException, InterruptedException;

  public abstract
  KEYIN getCurrentKey() throws IOException, InterruptedException;
  
  public abstract 
  VALUEIN getCurrentValue() throws IOException, InterruptedException;

  public abstract float getProgress() throws IOException, InterruptedException;
  
  public abstract void close() throws IOException;
}
#+END_SRC

下面是一些常见的InputFormat实现
   - FileInputFormat
     - addInputPath或者是setInputPaths修改输入路径 mapred.input.dir
     - setInputPathFilter可以修改过滤器 mapred.input.path.Filter.class
       - 基本实现会排除隐藏.或者是_开头文件。
       - 自定义的过滤器是建立在默认过滤器的基础上的。
     - 分片大小由下面三个参数控制
       - mapred.min.split.size 1
       - mapred.max.split.size MAX
       - dfs.block.size 64MB
       - 算法是max(minSplitSize,min(maxSplitSize,blockSize))
     - isSplitable可以控制输入文件是否需要分片
   - CombineFileInputFormat 可以处理多个小文件输入，抽象类需要继承实现。
   - TextInputFormat
     - 输入单位是行，key是LongWritable表示行偏移，value是Text表示行内容
   - KeyValueTextInputFormat
     - 输入单位是行，按照key.value.seperator.in.input.line来进行分隔默认是\t
     - key和value的格式都是Text
   - NLineInputFormat
     - 和TextInputFormat非常类似，大师使用多行输入默认为1行
     - 通过mapred.line.input.format.linespermap来控制行数
   - XML
     - InputFormat使用StreamInputFormat,       
     - 设置RecordReader使用stream.recordreader.class来设置
     - RecordReader使用org.apache.hadoop.streaming.StreamXmlRecordReader
     - *NOTE（dirlt）：也有现成的XmlInputFormat的实现*
   - SequenceFileInputFormat
   - SequenceFileAsTextInputFormat
     - 将输入的kv转换成为text对象适合streaming处理方式
   - SequenceFileAsBinaryInputFormat *NOTE（dirlt）：似乎没有什么用！*
   - MultipleInputs
   - DBInputFormat/DBOutputFormat JDBC数据库输入输出
   - TableInputFormat/TableOutputFormat HBase输入输出

--------------------
输出格式
   - TextOutputFormat
     - 使用mpared.textoutputformat.seperator来控制kv的分隔，默认是\t
     - 对应的输入格式为KeyValueTextInputFormat
     - 可以使用NullWritable来忽略输出的k或者是v
   - SequenceFileOutputFormat
   - SequenceFileAsBinaryOutpuFormat *NOTE（dirlt）：似乎没有什么用！*
   - MapFileOutputFormat
   - MultipleOutputFormat 
   - MultipleOutputs
     - 如果不像生成那写part-r-00000这些空文件的话，那么可以将OutputFormat设置成为NullOutputFormat

*** MapReduce的特性
   - 计数器
     - streaming计数器和可以通过写stderr来提交
       - reporter:counter:<group>,<counter>,<amount>
       - reporter:status:<message>
   - 连接
     - map端连接
       - 必须确保多路输入文件的reduce数量相同以及键相同。
       - 使用CompositeInputFormat来运行map端连接。
       - *NOTE（dirlt)；不过我稍微看了一下代码，实现上其实也是针对输入文件对每条记录读取，然后进行join包括inner或者是outer。感觉场景会有限，而且效率不会太高*
   - 分布式缓存
     - 使用-files以及-archives来添加缓存文件
     - 也可以使用DistributedAPI来完成之间事情
       - addCacheFile/addCacheArchive
       - 然后在task里面通过configuration的getLocalCacheFiles以及getLocalCacheArchives来获得这些缓存文件
     - 工作原理
       - 缓存文件首先被放到hdfs上面
       - task需要的话那么会尝试下载，之后会对这个缓存文件进行引用计数，如果为0那么删除
	 - 这也就意味着缓存文件可能会被多次下载
         - 但是运气好的话多个task在一个node上面的话那么就不用重复下载
       - 缓存文件存放在${mapred.local.dir}/taskTracker/archive下面，但是通过软连接指向工作目录
       - 缓存大小通过local.cache.size来配置
   - MapReduce库类
     - ChainMapper/ChainReducer 能够在一个mapper以及reducer里面运行多次mapper以及reducer
       - ChainMapper 允许在Map阶段，多个mapper组成一个chain,然后连续进行调用
       - ChainReducer 允许在Reuduce阶段，reducer完成之后执行一个mapper chain.
       - 最终达到的效果就是 M+ -> R -> M* （1个或者是多个mapper, 一个reducer，然后0个或者是多个mapper)
       - *TODO(dirlt):这样做倒是可以将各个mapper组合起来用作adapter.*

*** 构建Hadoop集群
   - 很多教程说hadoop集群需要配置ssh,但是配置这个前提是你希望使用start-all.sh这个脚本来启动集群
     - 我现在的公司使用apt-get来安装，使用cssh来登陆到所有的节点上面进行配置，因此没有配置这个信任关系
   - Hadoop配置
     - 配置文件
       - hadoop-env.sh 环境变量脚本
       - core-site.xml core配置，包括hdfs以及mapred的IO配置等
       - hdfs-site.xml hadoop进程配置比如namenode以及datanode以及secondary namenode
       - mapred-site.xml mapred进程配置比如jobtracker以及tasktracker
       - masters 运行namenode（secondary namenode)的机器列表，每行一个, *无需分发到各个节点*
	 - *在本地启动primary namenode*
       - slaves 运行datanode以及tasktracker的机器列表，每行一个 *无需分发到各个节点*
	 - *在本地启动jobtracker*
       - hadoop-metrics.properties 对hadoop做监控的配置文件
       - log4j.properties 日志配置文件
       - 这些文件在conf目录下面有，如果想使用不同的文件也可以使用-config来另行指定
       - *NOTE(dirlt):所以从上面这个脚本来看，还是具有一定的局限性的*
     - hadoop-env.sh
       - HADOOP_HEAPSIZE = 1000MB 守护进程大小
       - HADOOP_NAMENODE_OPTS
       - HADOOP_SECONDARYNAMENODE_OPTS
       - HADOOP_IDENT_STRING 用户名称标记，默认为${USER}
       - HADOOP_LOG_DIR hadoop日志文件，默认是HADOOP_INSTALL/logs
     - core-site.xml
       - io.file.buffer.size IO操作缓冲区大小，默认是4KB *这个需要提高*
     - hdfs-site.xml
       - fs.default.name
       - hadoop.tmp.dir hadoop临时目录，默认是在/tmp/hadoop-${user.name}
       - dfs.name.dir namenode数据目录，一系列的目录，namenode内容会同时备份在所有指定的目录中。默认为${hadoop.tmp.dir}/dfs/name
       - dfs.data.dir datanode数据目录，一系列的目录，循环将数据写在各个目录里面。默认是${hadoop.tmp.dir}/dfs/data
       - fs.checkpoint.dir secondarynamenode数据目录，一系列目录，所有目录都会写一份。默认为${hadoop.tmp.dir}/dfs/namesecondary
       - dfs.datanode.ipc.address 0.0.0.0:50020 datanode的RPC接口，主要和namenode交互
       - dfs.datanode.address 0.0.0.0:50010 datanode的data block传输接口，主要和client交互
       - dfs.datanode.http.address 0.0.0.0:50075 datanode的HTTP接口，和user交互
       - dfs.http.address 0.0.0.0:50070 namenode的HTTP接口
       - dfs.secondary.http.address 0.0.0.0:50090 secondard namenode的HTTP接口
       - dfs.datanode.dns.interface default 绑定的NIC，默认是绑定默认的NIC比如eth0	 
       - dfs.hosts / dfs.hosts.exclude 加入的datanode以及排除的datanode
       - dfs.replication = 3 副本数目
       - dfs.block.size = 64MB
       - dfs.datanode.du.reserved 默认datanode会使用目录所在磁盘所有空间，这个值可以保证有多少空间被reserved的
       - fs.trash.interval 单位分钟，如果不为0的话，那么删除文件会移动到回收站，超过这个单位时间的文件才会完全删除。
	 - 回收站位置/home/${user]/.Trash
         - haddop fs -expunge 强制删除
     - mapred-site.xml
       - mapred.job.tracker
       - mapred.local.dir MR中间数据存储，一系列目录，分散写到各个目录下面，默认为${hadoop.tmp.dir}/mapred/local
       - mapred.system.dir MR运行期间存储，比如存放jar或者是缓存文件等。默认${hadoop.tmp.dir}/mapred/system
       - mapred.tasktracker.map.tasks.maximum = 2 单个tasktracker最多多少map任务
       - mapred.tasktracker.reduce.tasks.maximum = 2 
       - mapred.child.java.opts = -Xmx200m 每个子JVM进程200M.
       - mapred.task.tracker.report.address 127.0.0.1:0 tasktracker启动子进程通信的端口，0表示使用任意端口
       - mapred.job.tracker.http.address 0.0.0.0:50030 jobtracker的HTTP接口
       - mapred.task.tracker.http.address 0.0.0.0:50060 tasktrackder的HTTP接口
       - mapred.tasktracker.dns.interface default 绑定的NIC，默认是绑定默认的NIC比如eth0
       - mapred.hosts / mapred.hosts.exclude 加入的tasktracker以及排除的tasktracker.
   - Hadoop Benchmark *NOTE（dirlt）：try it out*
     - 在hadoop安装目录下面有jar可以来做基准测试
     - TestDFSIO测试HDFS的IO性能
     - Sort测试MapReduce性能
     - MRBench多次运行一个小作业来检验小作业能否快速相应
     - NNBench测试namenode硬件的负载
   
*** 管理Hadoop
   - 永久性数据结构
     - namenode的目录结构
       - current表示当前的namenode数据（对于辅助节点上这个数据并不是最新的）
       - previous.checkpoint表示secondarynamenode完成checkpoint的数据（和current可能存在一些编辑差距）
	 - hadoop dfsadmin -saveNamespace 可以强制创建检查点,仅仅在安全模式下面运行
         - 辅助namenode每隔5分钟会检查
	   - 如果超过fs.checkpoint.period = 3600（sec），那么会创建检查点
           - 如果编辑日志大小超过fs.checkpoint.size = 64MB,同样也会创建检查点
	 - 除了将文件copy到namenode之外，在辅助节点上面可以使用选项-importCheckpoint来载入
       - VERSION Java属性文件
	 - namespaceID 每次格式化都会重新生成一个ID，这样可以防止错误的datanode加入
         - cTime namenode存储系统创建时间，对于刚格式化的存储系统为0.对于升级的话会更新到最新的时间戳
	 - storageType NAME_NODE or DATA_NODE
	 - layoutVersion 负整数表示hdfs文件系统布局版本号，对于hadoop升级的话这个版本号可能不会变化
       - edits 编辑日志文件
       - fsimage 镜像文件
       - fstime ???
     - datanode的目录结构
       - blk_<id>以及blk_<id>.meta 表示块数据以及对应的元信息，元数据主要包括校验和等内容
       - 如果datanode文件非常多的话，超过dfs.datanode.numblocks = 64的话，那么会创建一个目录单独存放，最终结果就是形成树存储结构。
       - dfs.data.dir目录是按照round-robin的算法选择的。

   - 安全模式
     - namenode启动的时候会尝试合并edit数据并且新建一个checkpoint，然后进入安全模式，在这个模式内文件系统是只读的
     - 可以通过hadoop dfsadmin -safemode来操作安全模式
     - 当达到下面几个条件的时候会离开安全模式
       - 整个系统的副本数目大于某个阈值的副本数目比率超过一个阈值之后，然后继续等待一段时间就会离开安全模式
       - dfs.replication.min = 1 副本数目阈值
       - dfs.safemode.threshold.pct = 0.999 比率阈值
       - dfs.safemode.extension = 30000(ms) 等待时间

   - 工具
     - dfsadmin
     - fsck
     - scanner
       - DataBlockScanner每隔一段时间会扫描本地的data block检查是否出现校验和问题
       - 时间间隔是dfs.datanode.scan.period.hours = 504默认三周
       - 可以通过页面访问每个datanode的block情况 http://localhost:50075/blockScannerReport
       - 加上listblocks参数可以看每个block情况 http://localhost:50075/blockScannerReport?listblocks *NOTE（dirlt）：可能会很大*
     - balancer
       - 通过start-balancer.sh来启动,集群中只允许存在一个均衡器
       - 均衡的标准是datanode的利用率和集群平均利用率的插值，如果超过某个阈值就会进行block movement
       - -threshold可以执行阈值，默认为10%
       - dfs.balance.bandwidthPerSec = 1024 * 1024 用于balance的带宽上限。

   - 监控
     - 日志
       - jobtracker的stack信息（thread-dump）http://localhost:50030/stacks
     - 度量
       - 度量从属于特性的上下文(context),包括下面几个
	 - dfs
	 - mapred
         - rpc
         - jvm
       - 下面是几种常见的context
	 - FileContext 度量写到文件
         - GangliaContext 度量写到ganglia *(这个似乎比较靠谱）*
	 - CompositeContext 组合context
       - 度量可以从hadoop-metrics.properties进行配置
    
       
** 代码分析


