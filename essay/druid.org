* druid
   - github https://github.com/metamx/druid
  
**  Design · metamx/druid Wiki
https://github.com/metamx/druid/wiki/Design



** Introducing Druid: Real-Time Analytics at a Billion Rows Per Second | Metamarkets
http://metamarkets.com/2011/druid-part-i-real-time-analytics-at-a-billion-rows-per-second/

*** Background
Druid is the distributed, in-memory OLAP data store that resulted.（分布式全内存的OLAP存储系统） 有两个名字需要稍微了解一下：
   - roll up，降维比如使用group
   - drill down. roll up的反义

文中以下面的实际例子来进行分析。假设我们有如下的data table数据集合成为alpha data set,这个部分数据是存放在硬盘上面的。
#+BEGIN_VERSE
timestamp             publisher          advertiser  gender  country  .. dimensions ..   click  price
2011-01-01T01:01:35Z  bieberfever.com    google.com  Male    USA                         0      0.65
2011-01-01T01:03:63Z  bieberfever.com    google.com  Male    USA                         0      0.62
2011-01-01T01:04:51Z  bieberfever.com    google.com  Male    USA                         1      0.45
...
2011-01-01T01:00:00Z  ultratrimfast.com  google.com  Female  UK                          0      0.87
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Female  UK                          0      0.99
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Female  UK                          1      1.53

...
#+END_VERSE

为了能够将数据放入内存，在alpha data set上面做roll up形成beta data set.这个部分可以放入内存。roll up方法如下
#+BEGIN_VERSE
    GROUP BY timestamp, publisher, advertiser, gender, country
      :: impressions = COUNT(1),  clicks = SUM(click),  revenue = SUM(price)
#+END_VERSE
产生的beta data set如下：
#+BEGIN_VERSE
timestamp             publisher          advertiser  gender  country  impressions  clicks  revenue
2011-01-01T01:00:00Z  ultratrimfast.com  google.com  Male    USA      1800         25      15.70
2011-01-01T01:00:00Z  bieberfever.com    google.com  Male    USA      2912         42      29.18
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Male    UK       1953         17      17.31
2011-01-01T02:00:00Z  bieberfever.com    google.com  Male    UK       3194         170     34.01
#+END_VERSE

beta data set主要包含3个部分：
   - Timestamp column: We treat timestamp separately because all of our queries center around the time axis. Timestamps are faceted by varying granularities (hourly, in the example above).（时间列主要用来做时间范围内的查询，roll up时候可以按照不同的粒度切片，上面的例子是按照小时）
   - Dimension columns: Here we have four dimensions of publisher, advertiser, gender, and country. They each represent an axis of the data that we’ve chosen to slice across.（维度列主要用来进行交叉查询）
   - Metric columns: These are impressions, clicks and revenue. These represent values, usually numeric, which are derived from an aggregation operation – such as count, sum, and mean (we also run variance and higher moment calculations). For example, in the first row, the revenue metric of 15.70 is the sum of 1800 event-level prices.（指标列则是一些具体的数值）

对于这些数据集合上面我们可能需要有下面这些操作：
   - “How many impressions from males were on bieberfever.com?” and 
   - “What is the average cost to advertise to women at ultratrimfast.com?”  
   - But we have a hard requirement to meet: we want queries over any arbitrary combination of dimensions at sub-second latencies.
理论上这个集合可能非常大，但是实际上这个大部分的维度交叉item还是非常少的，比如few Kazakhstanis visit beiberfever.com

*** Failed Solution I: Dynamic Roll-Ups with a RDBMS
So about a year ago, we fired up a RDBMS instance (actually, the Greenplum Community Edition, running on an m1.large EC2 box) 开始使用一些关系数据库，但是存在下面这些问题：
   - We stored the data in a star schema, which meant that there was operational overhead maintaining dimension and fact tables. *TOOO（dirlt）：不太明白这个是什么意思*
   - Whenever we needed to do a full table scan, for things like global counts, the queries ran slow. For example, naive benchmarks showed scanning 33 million rows took 3 seconds. （对于一些全表扫描的操作非常地慢 *NOTE（dirlt）：这个主要是因为没有使用并行查询的方式把？* ）
     - We started materializing all dimensional roll-ups of a certain depth, and began routing queries to these pre-aggregated tables. We also implemented a caching layer in front of our queries. （开始通过pre roll up到一定的深度然后在这些table上面进行查询，并且在query之前假设一个cache layer）
     - This approach generally worked and is, I believe, a fairly common strategy in the space. Except, when things weren’t in the cache and a query couldn’t be mapped to a pre-aggregated table, we were back to full scans and slow performance.（上面工作方式在大部分时候工作还是很好的，但是如果没有出现在cache或者是pre-compute的table里面性能就非常差）
     - We tried indexing our way out of it, but given that we are allowing arbitrary combinations of dimensions, we couldn’t really take advantage of composite indexes. （尝试建立二级联合索引，但是因为允许在所有的dimenson上面进行查询，所以还是不行）
     - Additionally, index merge strategies are not always implemented, or only implemented for bitmap indexes, depending on the flavor of RDBMS.（另外一些RDBMS的index merge策略可能没有实现，或者只是实现了bitmap index merge策略 *TODO（dirlt）：不太明白什么意思？* ）
We also benchmarked plain Postgres, MySQL, and InfoBright, but did not observe dramatically better performance.

*** Failed Solution II: Pre-compute the World in NoSQL
   - In short, we took all of our data and pre-computed aggregates for every combination of dimensions. At query time we need only locate the specific pre-computed aggregate and and return it: an O(1) key-value lookup. This made things fast and worked wonderfully when we had a six dimension beta data set.(在NoSQL里面需要预先计算很多维度的组合，但是在查询的时候非常快。如果维度只有6个的时候还是工作非常快速的）
   - But when we added five more dimensions – giving us 11 dimensions total – the time to pre-compute all aggregates became unmanageably large (such that we never waited more than 24 hours required to see it finish).（但是我们测试11个维度的时候，发现计算量太大）
   - Lesson learned: massively scalable counter systems like rainbird are intended for high cardinality data sets with pre-defined hierarchical drill-downs. But they break down when supporting arbitrary drill downs across all dimensions. （NoSQL不太适合高维度的查询，只是适合低纬度并且能够预先计算的场景）
     
*** Introducing Druid: A Distributed, In-Memory OLAP Store
下面是前面两种方式各自的问题：
   - Relational Database Architectures
       - Full table scans were slow, regardless of the storage engine used
       - Maintaining proper dimension tables, indexes and aggregate tables was painful
       - Parallelization of queries was not always supported or non-trivial
   - Massive NOSQL With Pre-Computation
       - Supporting high dimensional OLAP requires pre-computing an exponentially large amount of data

Keeping everything in memory provides fast scans, but it does introduce a new problem: machine memory is limited. The corollary thus was: distribute the data over multiple machines. （内存有限的话通过将数据分布在多个机器上面）

Thus, our requirements were:
   - Ability to load up, store, and query data sets in memory （放在内存里面避免了load up时间）
   - Parallelized architecture that allows us to add more machines in order to relieve memory pressure（分布式查询能够减缓memory压力）
And then we threw in a couple more that seemed like good ideas:
   - Parallelized queries to speed up full scan processing （同时分布式查询可以加快full scan处理速度）
   - No dimensional tables to manage （不维护任何dimensional table）

These are the requirements we used to implement Druid. The system makes a number of simplifying assumptions that fit our use case (namely that all analytics are time-based) and integrates access to real-time and historical data for a configurable amount of time into the past.（做了一些假设来简化设计比如所有的分析都是按照时间来进行划分的，并且支持对实时和非实时数据的统一访问）

** Druid, Part Deux: Three Principles for Fast, Distributed OLAP | Metamarkets
http://metamarkets.com/2011/druid-part-deux-three-principles-for-fast-distributed-olap/

*** Partial Aggregates + In-Memory + Indexes => Fast Queries
   - alpha represents the raw, unaggregated event logs, while beta is its partially aggregated derivative. （将alpha dataset使用部分聚合形成beta dataset)
   - The key to Druid’s speed is maintaining the beta data entirely in memory. Full scans are several orders of magnitude faster in memory than via disk. What we lose in having to compute roll-ups on the fly, we make up for with speed.(将beta data set存放在memory里面）
   - To support drill-downs on specific dimensions (such as results for only ‘bieberfever.com’), we maintain a set of inverted indices.(为了支持在beta dataset上面做drill down，需要维护一个反向索引，这个在另外一片文章里面提到了，主要使用bitmap来表示entry在alpha dataset中的位置，并且对应的表示非常容易进行and/or/not)     
  
*** Distributed Data + Parallelizable Queries => Horizontal Scalability
   - Druid’s performance depends on having memory — lots of it. We achieve the requisite memory scale by dynamically distributing data across a cluster of nodes. As the data set grows, we can horizontally expand by adding more machines.(通过动态地在节点中分布数据来达到比较方便的水平扩展）
   - To facilitate rebalancing, we take chunks of beta data and index them into segments based on time ranges.（为了能够完成rebalance，将beta dataset分片并且进行索引，根据时间范围）
   - For high cardinality dimensions, distributing by time isn’t enough (we generally try to keep segments no larger than 20M rows), so we have introduced partitioning. We store metadata about segments within the query layer and partitioning logic within the segment generation code.（而对于维度比较多的内容，仅仅按照时间分布还是不够的，我们尽量让我一个segment不要超过20M rows所以需要引入partition。 *NOTE（dirlt）：这个partition应该是用户自己定义的* 然后druid将segment的metadata保存在qeury layer上面，而用户在查询的时候需要自己提供partition的code）
   - We persist these segments in a storage system (currently S3) that is accessible from all nodes. If a node goes down, Zookeeper coordinates the remaining live nodes to reconstitute the missing beta set.（segment数据也会在S3文件系统上面进行持久化。这样如果一个server node挂掉的，可以选举另外一个节点从S3文件系统中读取beta dataset。检测node挂掉通过zookeeper协调）
   - Downstream clients of the API are insulated from this rebalancing: Druid’s query API seamlessly handles changes in cluster topology.（下游的client则不需要考虑rebalance的情况）
   - Queries against the Druid cluster are perfectly horizontal. We limited the aggregation operations we support – count, mean, variance and other parametric statistics – that are inherently parallelizable. While less parallelizable operations, such as median, are not supported, this limitation is offset by rich support of histogram and higher-order moment stores. The co-location of processing with in-memory data on each node reduces network load and dramatically improves performance.（限制进行聚合的操作，确保这些操作确实可以并行完成。 如果没有并行完成的话，可以通过  *histogram and higher-order moment stores（高阶矩）* 的支持来补偿 *TODO（dirlt）：WTF is that?* 

*** Real-Time Analytics: Immutable Past, Append-Only Future
   - For real-time analytics, we have an event stream that flows into a set of real-time indexers. These are servers that advertise responsibility for the most recent 60 minutes of data and nothing more. (对于实时分析有专门都的real-time indexer server，处理最近60分钟的数据）
   - They aggregate the real-time feed and periodically push an index segment to our storage system. The segment then gets loaded into memory of a standard server, and is flushed from the real-time indexer.（定期将real-time和历史数据做合并然后刷新real-time的数据）
   - Similarly, for long-range historical data that we want to make available, but not keep hot, we have deep-history servers. These use a memory mapping strategy for addressing segments, rather than loading them all into memory. This provides access to long-range data while maintaining the high-performance that our customers expect for near-term data.（对于那些非常老的历史数据，使用deep-history servers工作方式，使用mmap来访问segments而不用完全载入内存）
