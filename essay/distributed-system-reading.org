* distributed-system-reading
#+OPTIONS: H:4

   - Distributed Systems Reading List http://dancres.org/reading_list.html

** How to beat the CAP theorem
http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html

*** Consistency and Availability
   - The CAP theorem states a database cannot guarantee consistency, availability, and partition-tolerance at the same time. But you can't sacrifice partition-tolerance (see here and here), so you must make a tradeoff between availability and consistency. (只能够在CA之间进行选择）
   - Consistency means that after you do a successful write, future reads will always take that write into account. Availability means that you can always read and write to the system. During a partition, you can only have one of these properties.
   - Systems that choose consistency over availability have to deal with some awkward issues. What do you do when the database isn't available? You can try buffering writes for later, but you risk losing those writes if you lose the machine with the buffer. Also, buffering writes can be a form of inconsistency because a client thinks a write has succeeded but the write isn't in the database yet. Alternatively, you can return errors back to the client when the database is unavailable. But if you've ever used a product that told you to "try again later", you know how aggravating this can be. (选择Consistency，如果出现unavailable的话，那么write如何处理，拒绝client write还是将这个write buffer起来稍后提交，但是即使稍后提交也会出现一致性问题）
   - The other option is choosing availability over consistency. The best consistency guarantee these systems can provide is "eventual consistency". If you use an eventually consistent database, then sometimes you'll read a different result than you just wrote. Sometimes multiple readers reading the same key at the same time will get different results. Updates may not propagate to all replicas of a value, so you end up with some replicas getting some updates and other replicas getting different updates. It is up to you to repair the value once you detect that the values have diverged. This requires tracing back the history using vector clocks and merging the updates together (called "read repair").（如果选择availability的话，那么可能会出现read value inconsistent的情况。如果出现读取value不一致的话那么可能需要tracing back history并且使用vector clock来做merge update，这个过程称为 *read repair* ）
   - I believe that maintaining eventual consistency in the application layer is too heavy of a burden for developers. Read-repair code is extremely susceptible to developer error; if and when you make a mistake, faulty read-repairs will introduce irreversible corruption into the database.（但是read-repair不太好理解同时容易出现bug致使corrupt database）
   - There is another way. You can't avoid the CAP theorem, but you can isolate its complexity and prevent it from sabotaging your ability to reason about your systems. The complexity caused by the CAP theorem is a symptom of fundamental problems in how we approach building data systems. Two problems stand out in particular: the use of mutable state in databases and the use of incremental algorithms to update that state. It is the interaction between these problems and the CAP theorem that causes complexity.（CAP理论造成复杂性最基本的问题在于我们如何构建自己的系统，具体有两个问题：1.我们在database里面使用了mutable state 2.对于这个状态的更新都是增量完成的。正是因为这两个问题参杂在一起导致复杂性）
  
*** What is a data system?
   - There are two crucial properties to note about data. First, data is inherently time based. A piece of data is a fact that you know to be true at some moment of time. The second property of data follows immediately from the first: data is inherently immutable. Because of its connection to a point in time, the truthfulness of a piece of data never changes. One cannot go back in time to change the truthfulness of a piece of data. This means that there are only two main operations you can do with data: read existing data and add more data. CRUD has become CR. （关于数据的理解有两个特性 1.数据本质上是基于时间的，也就是说对于数据的正确性理解是在一定时间范围内的 2.考虑到数据是基于时间的，因为数据本质上是immutable的。考虑到这两点我们可以将data操作从CRUD限定为CR）

*** Beating the CAP theorem
   - What caused complexity before was the interaction between incremental updates and the CAP theorem. Incremental updates and the CAP theorem really don't play well together; mutable values require read-repair in an eventually consistent system. By rejecting incremental updates, embracing immutable data, and computing queries from scratch each time, you avoid that complexity. The CAP theorem has been beaten（最主要的问题就是incemental updates和CAP不太好同时处理。如果data mutable的话那么incremental updates需要使用read-pair来达到最终一致性，而如果我们认为数据是immutable的并且拒绝使用incremental updates的话那么就可以避免使用这种复杂性）

后面有一篇评论是这样的
#+BEGIN_VERSE
>  The other option is choosing availability over consistency. The best consistency guarantee these systems can provide is "eventual consistency"

No, the best consistency these systems can provide is eventually-known consistency: In addition to always servicing reads and writes, you can also ask "when is the most recent point such that you're guaranteed to be consistent with respect to all writes prior to that point?"

This is a far more useful consistency model than eventual consistency, since it allows strong consistency to be constructed on top of it (at the expense of sacrificing availability during a partition).
#+END_VERSE
我觉得这个评论说的很对，nathan marz可能没有理解两者差别
   - eventual-consistency 系统最终可以一致，但是对于read来说不知道在那个点上数据是一致的。好比有A，B两个value需要同时更新 *事务* ，在t1有A(t1) B(t1),而在t2时候可能只是更新了A(t2),而B依然为B(t1). 但是系统没有办法知道在t1这个事务才完成。当然原因也是因为数据是mutable的。
   - eventually-known consistency 同样系统最终可以一致，但是系统可以告诉说在t1上数据集合是一致的，虽然有部分数据修改成为了t2.

file:./images/batch-realtime-architecture.png

** Availability vs. Durability
如果只有一个副本机器A，要是它出故障，则读肯定会失败。要是有多副本机器，A坏了，我还有B和C可以读。写也一样。所以说多副本提高了服务的availability。

如果只有一个副本机器A，要是A的硬盘坏了，那数据就丢失了。要是有多个副本机器，则还可以从其他机器上找回来。所以说多副本提高了数据的durability，写完多副本的数据很难会丢失。

** Google Research Publication: Web Search for a Planet
   - link: http://research.google.com/archive/googlecluster.html
   - date: 2003 *NOTE(dirlt):可以看得出这篇文章相当老了*
   - google搜索架构包括软件和硬件 
     - *NOTE（dirlt):我主要对硬件上的选择比较感兴趣*

*** Google architecture overview
In summary, Google clusters follow three key design principles: 
   - Software reliability. We eschew fault-tol-erant hardware features such as redun-dant power supplies, a redundant array of inexpensive disks (RAID), and high-quality components, instead focusing on tolerating failures in software.（不在硬件层面保证可靠性，相反在软件层面来处理这些问题）
   - Use replication for better request through-put and availability. Because machines are inherently unreliable, we replicate each of our internal services across many machines. Because we already replicate services across multiple machines to obtain sufficient capacity, this type of fault tolerance almost comes for free.（通过replication来获得高吞吐以及可用性）
   - Price/performance beats peak performance. We purchase the CPU generation that currently gives the best performance per unit price, not the CPUs that give the best absolute performance. （关注price/performance而不仅仅是关注performance） 
   - Using commodity PCs reduces the cost of computation. As a result, we can afford to use more computational resources per query, employ more expensive techniques in our ranking algorithm, or search a larger index of documents（使用廉价PC来构建集群）

*** Leveraging commodity parts
--------------------
   - Google’s racks consist of 40 to 80 x86-based servers mounted on either side of a custom made rack (each side of the rack contains twenty 20u or forty 1u servers).（每个rack上面有40-80台x86服务器，rack每一面可以放下20个2u或者是40个1u服务器）
     - Our focus on price/performance favors servers that resemble mid-range desktop PCs in terms of their com-ponents, except for the choice of large disk drives.（主要使用普通的桌面PC但是换成大容量硬盘）
     - Several CPU generations are in active service, ranging from single-processor 533- MHz Intel-Celeron-based servers to dual 1.4-GHz Intel Pentium III servers.（CPU有从533MHz的赛扬到1.4GHz的奔腾）
     - Each server contains one or more integrated drive elec- tronics (IDE) drives, each holding 80 Gbytes.（使用80GB的IDE磁盘驱动器）
     - The servers on each side of a rack interconnect via a 100-Mbps Ethernet switch that has one or two gigabit uplinks to a core gigabit switch that connects all racks together（rack一侧的机器之间通过100Mbps的以太交换机，rack之间交换机使用千兆网卡）
   - Our ultimate selection criterion is cost per query, expressed as the sum of capital expense (with depreciation) and operating costs (host-ing, system administration, and repairs) divid-ed by performance.（费用方面包括资本开销包括折旧，以及运维开销）
     - Realistically, a server will not last beyond two or three years, because of its disparity in performance when compared to newer machines. Machines older than three years are so much slower than current-gener-ation machines that it is difficult to achieve proper load distribution and configuration in clusters containing both types.（但是实际情况是，一台服务器可能在2-3年之后相比更新的机器性能就有非常大的差距，因此对于一台服务器的寿命而言就是2-3年的时间）
     - Given the rel-atively short amortization period, the equip-ment cost figures prominently in the overall cost equation.（考虑到这点因素之后，因此开销主要还是集中在设备上）

--------------------
   - For example, in late 2002 a rack of 88 dual-CPU 2-GHz Intel Xeon servers with 2 Gbytes of RAM and an 80-Gbyte hard disk was offered on RackSaver.com for around $278,000. This figure translates into a monthly capital cost of $7,700 per rack over three years. (2002年一个rack上面88个双核CPU的Xeon服务器，配备2G内存以及80GB的磁盘在racksaver.com上面大概需要278000美元，平均每年7700美元）.
   - The relative importance of equipment cost makes traditional server solutions less appeal-ing for our problem because they increase per-formance but decrease the price/performance. （一些传统的服务器解决方案对于我们来说缺乏吸引力，主要是因为虽然增加了性能，但是降低了performance/price)
     - For example, four-processor motherboards are expensive, and because our application paral-lelizes very well, such a motherboard doesn’t recoup its additional cost with better perfor-mance. （比如对于能够支持4个CPU的主板，因为我们自身程序并行性已经非常好了，所以额外的开销并没有带来更好的性能）
     - Similarly, although SCSI disks are faster and more reliable, they typically cost two or three times as much as an equal-capac-ity IDE drive.（而对于SCSI磁盘来说虽然更快并且并且更加可靠，但是价钱是同样大小的IDE的2-3倍）
   - The cost advantages of using inexpensive, PC-based clusters over high-end multi-processor servers can be quite substantial, at least for a highly parallelizable application like ours.（ *使用PC-based这样的集群而不是使用高端的多处理器server带来的好处是非常明显的，尤其是对于并行程度非常高的应用程序* ）
     - The example $278,000 rack contains 176 2-GHz Xeon CPUs, 176 Gbytes of RAM, and 7 Tbytes of disk space. 
     - In com-parison, a typical x86-based server contains eight 2-GHz Xeon CPUs, 64 Gbytes of RAM, and 8 Tbytes of disk space; it costs about $758,000.（这个是高端服务器的报价，上面是之前提到的集群）
   - Operating thousands of mid-range PCs instead of a few high-end multiprocessor servers incurs significant system administra-tion and repair costs.（对于使用PC集群带来的唯一坏处就是机器非常多而故障率非常高，带来的管理成本和维修成本）
     - However, for a relative-ly homogenous application like Google, where most servers run one of very few appli-cations, these costs are manageable.（但是大部分的应用都是同构的，而且每个server上面只是运行有限的几个程序）
     - Assum-ing tools to install and upgrade software on groups of machines are available, the time and cost to maintain 1,000 servers isn’t much more than the cost of maintaining 100 servers because all machines have identical configu-rations. （同时使用相同的配置以及自动化部署可以在一定程度上解决这个问题）
     - Similarly, the cost of monitoring a cluster using a scalable application-monitor-ing system does not increase greatly with clus-ter size.（对于集群的监控通过可以扩展的监控系统完成）
     - Furthermore, we can keep repair costs reasonably low by batching repairs and ensur-ing that we can easily swap out components with the highest failure rates, such as disks and power supplies.（可以批量地进行部件维修。而且因为软件本身就是对于hardward failure是可容忍的，所以替换一些出问题的组件也非常容易）

*** The power problem
*** Hardware-level application characteristics
   - Examining various architectural characteris-tics of our application helps illustrate which hardware platforms will provide the best price/performance for our query-serving sys-tem.（分析应用程序的一些架构上面的特征，来解释什么硬件可以为查询系统提供更好的性价比）
     - We’ll concentrate on the characteristics of the index server, the component of our infra-structure whose price/performance most heav-ily impacts overall price/performance. （主要是针对index server这个部件来进行分析，因为这个部分对于性价比的影响非常大）
     - The main activity in the index server consists of decoding compressed information in the inverted index and finding matches against a set of documents that could satisfy a query. （index server主要的功能就是decode反向索引信息然后做一些聚合操作）

file:./images/google-index-server-measurements.png

   - The application has a moderately high CPI, considering that the Pentium III is capable of issuing three instructions per cycle. 考虑到P3能够一个cycle执行3条指令，现在每个cycle执行1.1条指令算是相对比较高的CPI了。
   - We expect such behavior, considering that the applica-tion traverses dynamic data structures and that control flow is data dependent, creating a sig-nificant number of difficult-to-predict branches.（对于这个CPI的解释是因为进行遍历了太多动态的数据结构并且有数据以依赖，造成了非常多难以预测的分支）
   - In fact, the same workload running on the newer Pentium 4 processor exhibits nearly twice the CPI and approximately the same branch prediction performance, even though the Pentium 4 can issue more instruc-tions concurrently and has superior branch prediction logic.（事实上，相同的workload在新的P4上面运行产生了2倍的CPI以及相同的分支条转性能，虽然P4能够同时执行更多的指令并且有更好的分支预测）
   - In essence, there isn’t that much exploitable instruction-level parallelism (ILP) in the workload. Our measurements suggest that the level of aggressive out-of-order, speculative execution present in mod-ern processors is already beyond the point of diminishing performance returns for such programs.  *（所以说白了workload的ILP没有那么高，因此测试建议现代处理器里面的乱序执行以及推测执行对我们的应用程序没有太多的用途）*
   - A more profitable way to exploit parallelism for applications such as the index server is to leverage the trivially parallelizable computa-tion.（所以探索并行性更经济的做法是利用这些本身就是可并行的计算）
   - Exploiting such abundant thread-level parallelism at the microarchitecture level appears equally promising. Both simultaneous multithreading (SMT) and chip multiproces-sor (CMP) architectures target thread-level parallelism and should improve the perfor-mance of many of our servers.（另外在微处理器架构层面提升线程级别的并行性还是更加有意义的，SMT或者是CMP似乎都能够提高性能）
     - Some early experiments with a dual-context (SMT) Intel Xeon processor show more than a 30 percent performance improvement over a single-con-text setup. This speedup is at the upper bound of improvements reported by Intel for their SMT implementation.（早期的一些SMT实验发现能够提升30%的性能，但是似乎这个加速是存在上限的）
     - We believe that the potential for CMP sys-tems is even greater. CMP designs, such as Hydra and Piranha seem especially promis-ing.（我们相信CMP是更加经济的做法）
       - In these designs, multiple (four to eight) simpler, in-order, short-pipeline cores replace a complex high-performance core. 设计上使用4-8个非常简单的，顺序执行，短流水线的核
       - The penal-ties of in-order execution should be minor given how little ILP our application yields,（in-order执行带来的损失就是对于稍微降低ILP）
       - and shorter pipelines would reduce or elimi-nate branch mispredict penalties. 短流水线却能够在一定程度上减少分支预测错误惩罚
       - The avail-able thread-level parallelism should allow near-linear speedup with the number of cores, and a shared L2 cache of reasonable size would speed up interprocessor communication.（始终这种线程级别的并行能够基本达到线性加速，而使用合理大小的共享L2可以加快处理器之间的通信）

** You Can’t Sacrifice Partition Tolerance | codahale.com
http://codahale.com/you-cant-sacrifice-partition-tolerance/

   - On Partition Tolerance
     - In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another. When a network is partitioned, all messages sent from nodes in one component of the partition to nodes in another component are lost. (And any pattern of message loss can be modeled as a temporary partition separating the communicating nodes at the exact instant the message is lost.) 
     - For a distributed (i.e., multi-node) system to not require partition-tolerance it would have to run on a network which is guaranteed to never drop messages (or even deliver them late) and whose nodes are guaranteed to never die. You and I do not work with these types of systems because they don’t exist.

   - But Never Both
     - You cannot, however, choose both consistency and availability in a distributed system.
     - As a thought experiment, imagine a distributed system which keeps track of a single piece of data using three nodes—A, B, and C—and which claims to be both consistent and available in the face of network partitions. Misfortune strikes, and that system is partitioned into two components: {A,B} and {C}. In this state, a write request arrives at node C to update the single piece of data. That node only has two options:
       - Accept the write, knowing that neither A nor B will know about this new data until the partition heals.
       - Refuse the write, knowing that the client might not be able to contact A or B until the partition heals.
     - You either choose availability (Door #1) or you choose consistency (Door #2). You cannot choose both.
     - To claim to do so is claiming either that the system operates on a single node (and is therefore not distributed) or that an update applied to a node in one component of a network partition will also be applied to another node in a different partition component magically. 如果同时满足CA的话，就意味着需要牺牲P（或者是在网络断开的情况下面能够magically达成一致，当然这是不可能的）。而不允许parition tolerance的话似乎只有单机系统而非分布式系统。

** The Anatomy Of The Google Architecture
   - link： http://www.slideshare.net/hasanveldstra/the-anatomy-of-the-google-architecture-fina-lv11
   - date：2009-12-09

The Google Philosophy
   - Jedis build their own lightsabres (the MS Eat your own Dog Food)
   - Parallelize Everything
   - Distribute Everything (to atomic level if possible)
   - Compress Everything (CPU cheaper than bandwidth) *优化带宽*
   - Secure Everything (you can never be too paranoid)
   - Cache (almost) Everything
   - Redundantize Everything (in triplicate usually)
   - Latency is VERY evil

*** The Basic Glue
file:./images/the-anatomy-of-google-architecture-basic-glue.png

   1. Exterior Network (Perimeter Architecture) （外部接入层）
   2. Data Centre（数据中心）
   3. Rack Characteristics（机架设计）
   4. Core Server Hardware（硬件设计）
   5. Operating System Implementation（操作系统）
   6. Interior Network Architecture（内部网络架构）

**** Exterior Network
file:./images/google-architecture-exterior-network.png

   - DNS Load Balanced splits traffic (country, .com multiple DNS, other X1) to FW
   - Firewall filters traffic (http/s, smtp,pop etc)
   - Netscalar Load Balancers take Request from FW blocks DOS attacks, ping floods (DOS) – blocks non IPv4/6 and none 80/443 ports and http multiplexes (limited caching capability)
   - User Request forwarded to Squid (Reverse Proxy) probably HUGE cache (Petabytes?)
     - 反向代理，似乎是穿透型的cache
     - 缓存命中率30-60%
     - All Image Thumbnails caches, much Multimedia cached, Expensive common queries cached 缩略图片，多媒体以及开销比较大的搜索
   - If not in Cache forwarded to GWS (Custom C++ Web Server) – now not using Custom apache?     
   - GWS sends the Request to appropriate internal (Cell) servers

**** Data Centre
   - Last estimated were 36 Data Centers, 300+ GFSII Clusters and upwards of 800K machines.（36个数据中心，300+ GFS2集群， *80万机器* ）
   - US (#1) – Europe (#2) – Asia (#3) – South America/Russia (#4)
   - Australia – on Hold
   - Future: Taiwan, Malaysia, Lithuania, and Blythewood, South Carolina.

   - Standard Google Modular DC (Cell) holds 1160 Servers / 250KW Power Consumption in 30 racks (40U).（cell有30个rack，支持40U one side.）
   - A Data Centre would consist of 100s of Modular Cells.（每个数据中心最多100左右个cell)
   - MDCs can also be deployed autonomously at the Perimeter (stand alone). MDC可以独立部署

**** Rack
file:./images/google-architecture-rack.png

*NOTE（dirlt）：在空间以及冷却系统上面减少成本*
   - Mini Server Size
     - Old Servers are Custom 1U
     - New Servers are 2U
     - seem 1/3 width of a normal 2U Server 宽度为普通2U服务器的1/3宽
   -  40U/80U Custom Racks (50% each side) 
     - Huge Heating and Power Issues（冷却系统）
     - Optimized Motherboards（主板优化）
     - Have their own HW builds（定制硬件）
   - Motherboard directly mounted into Rack
     - servers have no casing - just bare boards（没有盖子）
     - assist with heat dispersal issues *NOTE(dirlt):???*

**** Hardware
*NOTE（dirlt）：配置都非常普通*
   - 2U Low-Cost (but not slow) Commodity Servers 
     - 2009 Currently 2-Way, Dual Core/16GB/1-2TB +- Standard 
     - Both Intel/AMD Chipsets – 1 NIC – 2 USB
     - Looks like they RAID1/mirror the disks for better I/O - read performance
     - SATA 7.2K/10K/15K drives? 8 x 2GB DDR3 ECC
   - Standard HW Build (Several HW Build Versions at any one time)
     - Currently at 7Gen Build (1G 2005 was probably Dual Core/SMP)
     - Each Server 12V Battery Backup and can run autonomously without external power (lasts 20-30s?)

| YEAR      | Average Server Specification                                                                                                |
|-----------+-----------------------------------------------------------------------------------------------------------------------------|
| 1999/2000 | PII/PIII 128MB+                                                                                                             |
| 2003/2004 | Celeron 533, PIII 1.4 SMP, 2-4GB DRAM, Dual XEON 2.0/1-4GB/40-160GB IDE - SATA Disks via Silicon Images SATA 3114/SATA 3124 |
| 2006      | Dual Opteron/Working Set DRAM(4GB+)/2x400GB IDE (RAID0?)                                                                    |
| 2009      | 2-Way/Dual Core/16GB/1-2TB SATA                                                                                             |
    
**** Operating System
   - 100% Redhat Linux Based since 1998 inception
     - RHEL (Why not CentOS?)
     - 2.6.X Kernel
     - PAE(Physical Address Extension) 物理地址扩展，32位下面支持64GB内存
     - Custom glibc.. rpc... ipvs...
     - Custom FS (GFS II)
     - Custom Kerberos
     - Custom NFS
     - Custom CUPS
     - Custom gPXE bootloader 
       - *NOTE（dirlt）：open-source network booting software*
     - Custom EVERYTHING.....
   - Kernel/Subsystem Modifications
     - tcmalloc – replaces glibc 2.3 malloc – much faster! works very well with threads...
     - rpc – the rpc layer extensively modified to provide > perf increase < latency (52%/40%) *TODO（dirlt）：？？？*
     - Significantly modified Kernel and Subsystems – all IPv6 enabled
     - Developed and maintained systems to automate installation, updates, and upgrades of Linux systems.
     - Served as technical lead of team responsible for customizing and deploying Linux to internal systems and workstations.
   - Use Python as the primary scripting language
   - Deploy Ubuntu internally (likely for the Desktop) – also Chrome OS base

**** Interior Network
Routing Protocol：
   - Internal network is IPv6 (exterior machines can be reached using IPv6)
   - Heavily Modified Version of OSPF as the IRP
   - Intra-rack network is 100baseT
   - Inter-rack network is 1000baseT
   - Inter-DC network pipes unknown but very fast

Technology:
   - Juniper, Cisco, Foundry, HP, routers and switches

Software:
   - ipvs (ip virtual server)

*** The Major Glue
file:./images/the-anatomy-of-google-architecture-major-glue.png

   - Google File System Architecture – GFS II     
   - Google Database - Bigtable
   - Google Computation - Mapreduce
   - Google Scheduling - GWQ

**** GOOGLE FILE SYSTEM
   - GFS II “Colossus“ Version 2 improves in many ways (is a complete rewrite)
   - Elegant Master Failover (no more 2s delays...) *master 2s内可以恢复*
   - Chunk Size is now 1MB – likely to improve latency for serving data other than Indexing *偏向实时处理,chunksize=1MB*
   - Master can store more Chunk Metadata (therefore more chunks addressable up to 100 million) = also more Chunk Servers *支持亿级别chunk*

**** GOOGLE DATABASE
   - Increased Scalability (across Namespace/Datacenters) 
     - Tablets spread over DC s for a table but expensive (both computationally and financially!) *NOTE（dirlt）：对于tablet跨数据中心的话代价非常大*
   - Multiple Bigtable Clusters replicated throughout DC 数据中心之间的bigtable集群相互同步。
   - Current Status
     - Many Hundreds may be thousands of Bigtable Cells. Late 2009 stated 500 Bigtable clusters（2009年500个多个bigtable cluster)
     - At minimum scaled to many thousands of machine per cell in production 每个集群上面有上千台机器。
     - Cells manage Managing 3-figure TB data (0.X PB) 每个集群管理PB级别数据。

**** GOOGLE MAPREDUCE
   - STATISTICS
     - In September 2009 Google ran 3,467,000 MR Jobs with an average 475 sec completion time averaging 488 machines per MR and utilising 25.5K Machine years 
     - Technique extensively used by Yahoo with Hadoop (similar architecture to Google) and Facebook (since 06 multiple Hadoop clusters, one being 2500CPU/1PB with HBase).
 
**** GOOGLE WORKQUEUE
   - Batch Submission/Scheduler System 批量提交和调度系统
   - Arbitrates (process priorities) Schedules, Allocates Resources, process failover, Reports status, collects results 优先级分配资源，处理failover，汇报状态
     - *NOTE（dirlt）：这个非常类似hadoop后期要做的yarn*
   - Workqueue can manage many tens of thousands of machines *管理上万机器*
   - Launched via API or command line (sawzall example shown)
#+BEGIN_EXAMPLE
saw --program code.szl --workqueue testing
--input_files /gfs/cluster1/2005-02-0[1-7]/submits.* \
--destination /gfs/cluster2/$USER/output@100
#+END_EXAMPLE

*** BUILD YOUR OWN GOOGLE

file:./images/the-open-source-google-stack.png


   - Google PROFITS US $16M A DAY 
   - “Libraries are the predominant way of building programs”
   - Agile Methodologies Used (development iterations, teamwork, collaboration, and process adaptability throughout the life-cycle of the project) 敏捷开发？
   - An infrastructure handles versioning of applications so they can be release without a fear of breaking things = roll out with minimal QA *NOTE（dirlt）：有专门的程序来处理程序版本之间兼容关系，持续集成？！*

** Case Study GFS: Evolution on Fast-forward
@2009

--------------------
#+BEGIN_VERSE
MCKUSICK Now, under the current schema for GFS, you have one master per cell, right?

QUINLAN That’s correct.

MCKUSICK And historically you’ve had one cell per data center, right?

QUINLAN That was initially the goal, but it didn’t work out like that to a large extent—partly because of the limitations of the single-master design and partly because isolation proved to be difficult. As a consequence, people generally ended up with more than one cell per data center. We also ended up doing what we call a “multi-cell” approach, which basically made it possible to put multiple GFS masters on top of a pool of chunkservers. That way, the chunkservers could be configured to have, say, eight GFS masters assigned to them, and that would give you at least one pool of underlying storage—with multiple master heads on it, if you will. Then the application was responsible for partitioning data across those different cells.

#+END_VERSE

GFS初始设计是在一个data center/cell部署一个master. 但是事实证明这种方式不太好，一方面是因为master本身限制造成压力，另外一方面是在单master上面完成隔离比较困难。因此后来采用了mult-cell的方法，在一个data center/cell部署多个master,但是这些master贡献一个chunkserver pool. 用户程序通过自己partition决定数据元信息存放在哪个master上面。

--------------------
#+BEGIN_VERSE
MCKUSICK What longer-term strategy have you come up with for dealing with the file-count issue? Certainly, it doesn’t seem that a distributed master is really going to help with that—not if the master still has to keep all the metadata in memory, that is.

QUINLAN The distributed master certainly allows you to grow file counts, in line with the number of machines you’re willing to throw at it. That certainly helps.

One of the appeals of the distributed multimaster model is that if you scale everything up by two orders of magnitude, then getting down to a 1-MB average file size is going to be a lot different from having a 64-MB average file size. If you end up going below 1 MB, then you’re also going to run into other issues that you really need to be careful about. For example, if you end up having to read 10,000 10-KB files, you’re going to be doing a lot more seeking than if you’re just reading 100 1-MB files.

My gut feeling is that if you design for an average 1-MB file size, then that should provide for a much larger class of things than does a design that assumes a 64-MB average file size. Ideally, you would like to imagine a system that goes all the way down to much smaller file sizes, but 1 MB seems a reasonable compromise in our environment.

MCKUSICK What have you been doing to design GFS to work with 1-MB files?

QUINLAN We haven’t been doing anything with the existing GFS design. Our distributed master system that will provide for 1-MB files is essentially a whole new design. That way, we can aim for something on the order of 100 million files per master. You can also have hundreds of masters. 

MCKUSICK So, essentially no single master would have all this data on it?

QUINLAN That’s the idea.

#+END_VERSE

*NOTE（dirlt）：没有太看懂* 解决文件数量限制问题可以通过分布式master来解决。 *NOTE（dirlt）：但是个人觉得降低chunkszie似乎没有什么意义？*

--------------------
With the recent emergence within Google of BigTable, a distributed storage system for managing structured data, one potential remedy for the file-count problem—albeit perhaps not the very best one—is now available.

#+BEGIN_VERSE
MCKUSICK I guess the question I’m really trying to pose here is: Did BigTable just get stuck into a lot of these applications as an attempt to deal with the small-file problem, basically by taking a whole bunch of small things and then aggregating them together?

QUINLAN That has certainly been one use case for BigTable, but it was actually intended for a much more general sort of problem. If you’re using BigTable in that way—that is, as a way of fighting the file-count problem where you might have otherwise used a file system to handle that—then you would not end up employing all of BigTable’s functionality by any means. BigTable isn’t really ideal for that purpose in that it requires resources for its own operations that are nontrivial. Also, it has garbage-collection policy that’s not super-aggressive, so that might not be the most efficient way to use your space. I’d say that the people who have been using BigTable purely to deal with the file- count problem probably haven’t been terribly happy, but there’s no question that it is one way for people to handle that problem.

#+END_VERSE

The other major challenge for GFS, of course, has revolved around finding ways to build latency- sensitive applications on top of a file system designed around an entirely different set of priorities.

Our user base has definitely migrated from being a MapReduce-based world to more of an interactive world that relies on things such as BigTable. Gmail is an obvious example of that. Videos aren’t quite as bad where GFS is concerned because you get to stream data, meaning you can buffer. Still, trying to build an interactive database on top of a file system that was designed from the start to support more batch-oriented operations has certainly proved to be a pain point.

*BigTable的出现解决了GFS出现的两个问题，一个侧面地解决了大量小文件存储问题虽然不是非常优雅但也可用，另外一方面是来处理延迟敏感的user-face application*

--------------------
#+BEGIN_VERSE
MCKUSICK Was this done by design?

QUINLAN At the time, it must have seemed like a good idea, but in retrospect I think the consensus is that it proved to be more painful than it was worth. It just doesn’t meet the expectations people have of a file system, so they end up getting surprised. Then they had to figure out work-arounds. MCKUSICK In retrospect, how would you handle this differently?

QUINLAN I think it makes more sense to have a single writer per file.

MCKUSICK All right, but what happens when you have multiple people wanting to append to a log? 

QUINLAN You serialize the writes through a single process that can ensure the replicas are consistent.
#+END_VERSE

*GFS里面对于一个文件允许多个writer同时操作，因为mutation order以及支持random write造成的一致性问题一直是论文中最难理解的部分。google要是从头设计的话，也会使用HDFS方式支持append并且一个文件只允许一个appender*


