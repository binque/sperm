* mapred
** YARN
*** Introducing Apache Hadoop YARN 
http://hortonworks.com/blog/introducing-apache-hadoop-yarn/ 

看起来YARN的主要目的是将Hadoop不仅仅用于map-reduce的计算方式，还包括MPI，graph-processing，simple services等，
而MR仅仅是作为其中一种计算方式。底层依然是使用HDFS。发布方式的话还是将HDFS，YARN，MR，以及Common一起统一发布。

*** Apache Hadoop YARN – Background and an Overview 
http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/

对于MR来说，最关键的一点就是lack of data motion。通过将任务放在数据所在的机器上面，而不是将数据移动到任务所在的机器上面，可以节省带宽提高计算效率。现在来说MR分为下面三个部分：
   - The end-user *MapReduce API* for programming the desired MapReduce application. 
   - The *MapReduce framework*, which is the runtime implementation of various phases such as the map phase, the sort/shuffle/merge aggregation and the reduce phase. （framework做的事情是runtime的工作，比如怎么划分数据，怎么进行reducer上面的拉数据等）
   - The *MapReduce system*, which is the backend infrastructure required to run the user’s MapReduce application, manage cluster resources, schedule thousands of concurrent jobs etc. （system做的事情是确保runtime可以work的工作，集群管理如何调度）

file:./images/MRArch.png

For a while, we have understood that the Apache Hadoop MapReduce framework needed an overhaul. In particular, with regards to the JobTracker, we needed to address several aspects regarding scalability, cluster utilization, ability for customers to control upgrades to the stack i.e. customer agility and equally importantly, supporting workloads other than MapReduce itself. 考虑对于MR framework需要做下面这些改进，尤其是对于JobTracker来说：
   - 扩展性。我的理解是master有更好的处理能力，应该来支持更多的节点加入集群。2009年产品部署上能够达到5k个节点。
   - 集群利用。现在hadoop是将所有的nodes看作是distince map-reduce slots的，并且两者是不可替换的。可能mapper使用非常多而reducer非常少（或者相反），这样的情况会限制集群利用效率。
   - 灵活地控制software stack。我的理解是对于软件的升级，可能不能够完全替换，因此需要支持集群中有多个版本的MR运行。主要还是兼容性问题。
   - 服务不同的workload而非MR。比如MPI，graph-processing，realtime-processing，并且减少HDFS到自己存储系统之间数据的迁移（现在MR输入一定要在HDFS上面）

--------------------

YARN主要做的工作就是在资源利用的改进上面，将资源利用已经workflow分离：
   - 资源利用通过引入的ResouceManager（RM）以及NodeManager（NM）来管理。
     - NM主要做单机上面的资源收集汇报给RM
     - RM能够用来了解整个集群的资源使用情况，通过收集NM以及AM汇报信息。
     - RM提供pluggable Scheduler来计算资源分配。
   - workflow方面将MR和其他类型workflow分离，抽象成为ApplicationManager（AM）以及Container（既有ResourceAllocation概念，也有ApplicationNode概念）
     
file:./images/YARNArch.png

*** Apache Hadoop YARN – Concepts and Applications
http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/

将AM和RM分离的好处在于：一方面减轻RM的压力这样可以让RM管理更多的集群，另外一方面可以让AM支持更多类型的计算而不仅仅是MR

AM对RM提供Resource Request。对于Resource Model定义包括下面几个方面：
   - Resource-name (hostname, rackname – we are in the process of generalizing this further to support more complex network topologies with YARN-18).（我需要哪些机器，可以制定host，rack，或者是*/any）
   - Memory (in MB)（需要使用的内存大小）
   - CPU (cores, for now)（CPU的个数）
   - In future, expect us to add more resource-types such as disk/network I/O, GPUs etc.（各种IO参数）
每一个Resource Model如果满足之后在一个机器上面形成一个Container。Resource Request包括下面几个部分：
   - <resource-name, priority, resource-requirement, number-of-containers>
   - resource-name is either hostname, rackname or * to indicate no preference. In future, we expect to support even more complex topologies for virtual machines on a host, more complex networks etc.
   - priority is intra-application priority for this request (to stress, this isn’t across multiple applications).
   - resource-requirement is required capabilities such as memory, cpu etc. (at the time of writing YARN only supports memory and cpu).
   - number-of-containers is just a multiple of such containers.（我需要多少个这样的container？）

ApplicationMaster需要通知Container来执行任务，因为现在的任务不限于MR，需要提供下面这些信息：
   - Command line to launch the process within the container. 命令行
   - Environment variables. 环境变量
   - Local resources necessary on the machine prior to launch, such as jars, shared-objects, auxiliary data files etc. 一些本地资源
   - Security-related tokens. 安全token

整个YARN执行任务的步骤包括下面这几步： Application execution consists of the following steps:
   - Application submission. 提交任务
   - Bootstrapping the ApplicationMaster instance for the application. 启动AM
   - Application execution managed by the ApplicationMaster instance. AM在不同的Container启动task

Let’s walk through an application execution sequence (steps are illustrated in the diagram):
   - A client program submits the application, including the necessary specifications to launch the application-specific ApplicationMaster itself. （用户首先提交AM）
   - The ResourceManager assumes the responsibility to negotiate a specified container in which to start the ApplicationMaster and then launches the ApplicationMaster.（RM为AM分配所需要的Container，并且启动AM）
   - The ApplicationMaster, on boot-up, registers with the ResourceManager – the registration allows the client program to query the ResourceManager for details, which allow it to  directly communicate with its own ApplicationMaster.（AM向RM进行注册）
   - During normal operation the ApplicationMaster negotiates appropriate resource containers via the resource-request protocol.（AM通过Resouce Request和RM进行资源协调，获得所需要的Container）
   - On successful container allocations, the ApplicationMaster launches the container by providing the container launch specification to the NodeManager. The launch specification, typically, includes the necessary information to allow the container to communicate with the ApplicationMaster itself.（AM通知Container所处的NM启动task）
   - The application code executing within the container then provides necessary information (progress, status etc.) to its ApplicationMaster via an application-specific protocol.（Container会定时和AM进行通信，通知进度等）
   - During the application execution, the client that submitted the program communicates directly with the ApplicationMaster to get status, progress updates etc. via an application-specific protocol.（client直接和AM进行通信了解整个任务进度）
   - Once the application is complete, and all necessary work has been finished, the ApplicationMaster deregisters with the ResourceManager and shuts down, allowing its own container to be repurposed.（任务完成之后AM通知RM注销并且释放所持有的Container）

file:./images/yarnflow.png

*** Apache Mesos (Twitter Open Source Open House)
https://speakerdeck.com/u/benh/p/apache-mesos-twitter-open-source-open-house

和YARN类似的资源调度层。

** Usage
*** 验证多路输入正确性
多路输入包括从多路hbase以及多路hdfs输入，下面是一个验证程序，可以在上面修改确认是否OK。大致思路如下：
   - 编写hbase以及hdfs对应的mapper
   - 构造两个htable以及hdfs file
   - mapper效果就是将内容直接转发出去
   - reducer将输出结果写入到hdfs文件
   - 设置1个reducer这样可以容易地验证结果。
*NOTE（dirlt）：现在似乎只能够实现多个hdfs，1个htable作为输入*

#+BEGIN_SRC Java
package com.umeng.dp.helper;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.hbase.mapreduce.TableMapper;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TestHBaseInputAndHDFSInputMR {
    public static final String kInTableName1 = "test.temporary.in1";
    public static final String kInTableName2 = "test.temporary.in2";
    public static final String kInFileName1 = "/tmp/test.temporary.in1";
    public static final String kInFileName2 = "/tmp/test.temporary.in2";
    public static final String kOutFileName = "/tmp/test.temporary.out";
    private final static byte[] kByteColumnFamily = Bytes.toBytes("CF");
    private final static byte[] kByteColumn = Bytes.toBytes("CL");

    public static class FMapper extends Mapper<LongWritable, Text, Text, Text> {
        @Override
        protected void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {
            context.write(new Text("0"),
                    new Text("file mapper value=" + value.toString()));
        }
    }

    public static class TMapper extends TableMapper<Text, Text> {
        @Override
        protected void map(ImmutableBytesWritable key, Result result,
                Context context) throws IOException, InterruptedException {
            context.write(
                    new Text("0"),
                    new Text("table mapper key = "
                            + Bytes.toString(key.get())
                            + ", value="
                            + Bytes.toString(result.getValue(kByteColumnFamily,
                                    kByteColumn))));
        }
    }

    public static class TMapper2 extends TMapper {
    }

    public static class FTReducer extends
            Reducer<Text, Text, NullWritable, Text> {
        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context)
                throws IOException, InterruptedException {
            for (Text v : values) {
                context.write(null, v);
            }
        }
    }

    public static void createTable(String name, Configuration conf)
            throws IOException {
        HBaseAdmin admin = new HBaseAdmin(conf);
        if (admin.isTableAvailable(name)) {
            admin.disableTable(name);
            admin.deleteTable(name);
        }
        HTableDescriptor dp = new HTableDescriptor(name);
        dp.addFamily(new HColumnDescriptor(kByteColumnFamily));
        admin.createTable(dp);

        HTable table = new HTable(name);
        Put put = new Put(Bytes.toBytes(name + ".rowkey"));
        put.add(kByteColumnFamily, kByteColumn, Bytes.toBytes(name + ".tvalue"));
        table.put(put);
        table.close();
    }

    public static void createFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        FSDataOutputStream fos = fs.create(p);
        fos.writeBytes(name + ".fvalue\n");
        fos.close();
        fs.close();
    }

    public static void deleteFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        fs.close();
    }

    public static Job configureJob(Configuration conf, String[] args)
            throws IOException {
         // fill htable some data.
         createTable(kInTableName1, conf);
         createTable(kInTableName2, conf);
         // write hdfs some data.
         createFile(kInFileName1, conf);
         createFile(kInFileName2, conf);
        // delete out
        deleteFile(kOutFileName, conf);

        String jobName = "TestHBaseInputAndHDFSInputMR#"
                + new SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
        // setup environment.
        Job job = new Job(conf);
        job.setJobName(jobName);
        job.setJarByClass(TestHBaseInputAndHDFSInputMR.class);

        // mapper option.
        Scan scan = new Scan();
        scan.setCaching(500); // 1 is the default in Scan, which will be bad for
                              // MapReduce jobs
                              // TableMapReduceUtil.initTableMapperJob(kInTableName1,
                              // scan,
        TableMapReduceUtil.initTableMapperJob(kInTableName1, scan,
                TMapper.class, Text.class, Text.class, job);
        // simplest way.
        MultipleInputs.addInputPath(job, new Path(kInTableName1),
                TableInputFormat.class, TMapper.class);
        
        MultipleInputs.addInputPath(job, new Path(kInFileName1),
                TextInputFormat.class, FMapper.class);
        MultipleInputs.addInputPath(job, new Path(kInFileName2),
                TextInputFormat.class, FMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        // reducer option.
        job.setReducerClass(Reducer.class);
        job.setOutputKeyClass(NullWritable.class);
        job.setOutputValueClass(Text.class);
        job.setNumReduceTasks(1); // just one reducer.
        // output option.
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job, new Path(kOutFileName));
        return job;
    }

    public static void main(String[] args) {
        try {
            Configuration conf = HBaseConfiguration.create();
            args = new GenericOptionsParser(conf, args).getRemainingArgs();
            Job job = configureJob(conf, args);
            job.submit();
            System.exit(job.waitForCompletion(true) ? 0 : 1);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
#+END_SRC

*** 验证多路输出正确性
多路输出包括输出到hdfs和htable，下面是一个验证程序。大致似乎是这样的：
   - 构造一个输入文件
   - 从这个文件读取内容并且在mapper里面写到file和table里面
   - 没有设置任何reducer
*NOTE（dirlt）：似乎只是支持1个htable，和1个hdfs的目录。但是hdfs每个输出文件会有一个前缀用来进行区分*

#+BEGIN_EXAMPLE
[dirlt@umeng-ubuntu-pc] > hadoop fs -ls /tmp/test/temporary.out/
12/10/08 16:59:00 INFO security.UserGroupInformation: JAAS Configuration already set up for Hadoop, not re-installing.
Found 4 items
-rw-r--r--   1 dirlt supergroup          0 2012-10-08 16:58 /tmp/test/temporary.out/_SUCCESS
drwxr-xr-x   - dirlt supergroup          0 2012-10-08 16:57 /tmp/test/temporary.out/_logs
-rw-r--r--   1 dirlt supergroup         35 2012-10-08 16:57 /tmp/test/temporary.out/f-m-00000
-rw-r--r--   1 dirlt supergroup          0 2012-10-08 16:57 /tmp/test/temporary.out/part-m-00000
#+END_EXAMPLE
可以看到在mapper输出里面，以f来作为前缀而没有使用part来作为前缀作为输出（虽然存在part这个文件，但是我们没有使用context进行输出）

#+BEGIN_SRC Java
package com.umeng.dp.mapreduce;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TestHBaseOutputAndHDFSOutputMR {
    public static final String kInputFileName = "/tmp/test/temporary.in";
    public static final String kOutputTableName = "test.temporary.out";
    public static final String kOutputFileName = "/tmp/test/temporary.out";
    private final static byte[] kByteColumnFamily = Bytes.toBytes("CF");
    private final static byte[] kByteColumn = Bytes.toBytes("CL");
    
    public static void createTable(String name, Configuration conf)
            throws IOException {
        HBaseAdmin admin = new HBaseAdmin(conf);
        if (admin.isTableAvailable(name)) {
            admin.disableTable(name);
            admin.deleteTable(name);
        }
        HTableDescriptor dp = new HTableDescriptor(name);
        dp.addFamily(new HColumnDescriptor(kByteColumnFamily));
        admin.createTable(dp);
        admin.close();
    }
    
    public static void createFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        FSDataOutputStream fos = fs.create(p);
        fos.writeBytes(name + ".fvalue\n");
        fos.close();
        fs.close();
    }
    
    public static void deleteFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        fs.close();
    }
    
    public static class XMap extends Mapper<LongWritable, Text, NullWritable, NullWritable > {
        private MultipleOutputs<NullWritable, NullWritable> mos = null;
        @Override
        public void setup(Context ctx) throws IOException, InterruptedException {
            super.setup(ctx);
            mos = new MultipleOutputs<NullWritable, NullWritable>(ctx);
        }
        
        @Override
        public void map(LongWritable k, Text v, Context ctx) throws IOException, InterruptedException {
            mos.write("f", new Text("fkey"), v);
            mos.write("f2", new Text("fkey"), v);
            Put put = new Put(Bytes.toBytes("tkey"));
            put.add(kByteColumnFamily, kByteColumn, v.getBytes());
            mos.write("t", null, put);
        }
        
        @Override
        public void cleanup(Context ctx) throws IOException, InterruptedException {
            super.cleanup(ctx);
            mos.close();           
        }
    }
    
    public static Job configureJob(Configuration conf, String[] args)
            throws IOException {
        createTable(kOutputTableName,conf);
        deleteFile(kOutputFileName, conf);
        createFile(kInputFileName, conf);
        
        String jobName = "TestHBaseOutputAndHDFSOutputMR#"
                + new SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
        // setup environment.
        Job job = new Job(conf);
        job.setJobName(jobName);
        job.setJarByClass(TestHBaseOutputAndHDFSOutputMR.class);        
        job.setMapperClass(TestHBaseOutputAndHDFSOutputMR.XMap.class);
        FileInputFormat.setInputPaths(job, new Path(kInputFileName));
        
        job.setOutputKeyClass(NullWritable.class);
        job.setOutputValueClass(NullWritable.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        
        MultipleOutputs.addNamedOutput(job, "f", TextOutputFormat.class, Text.class, Text.class);
        MultipleOutputs.addNamedOutput(job, "f2", TextOutputFormat.class, Text.class, Text.class);
        MultipleOutputs.addNamedOutput(job, "t", TableOutputFormat.class, NullWritable.class, Put.class);
        FileOutputFormat.setOutputPath(job, new Path(kOutputFileName));
        job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, kOutputTableName);
        
        job.setNumReduceTasks(0);
        return job;
    }

    public static void main(String[] args) {
        try {
            Configuration conf = HBaseConfiguration.create();
            args = new GenericOptionsParser(conf, args).getRemainingArgs();
            Job job = configureJob(conf, args);
            job.submit();
            System.exit(job.waitForCompletion(true) ? 0 : 1);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

#+END_SRC
