* mapred
** YARN
*** Introducing Apache Hadoop YARN | Hortonworks
http://hortonworks.com/blog/introducing-apache-hadoop-yarn/ 

看起来YARN的主要目的是将Hadoop不仅仅用于map-reduce的计算方式，还包括MPI，graph-processing，simple services等，
而MR仅仅是作为其中一种计算方式。底层依然是使用HDFS。发布方式的话还是将HDFS，YARN，MR，以及Common一起统一发布。

*** Apache Hadoop YARN – Background and an Overview | Hortonworks
http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/

对于MR来说，最关键的一点就是lack of data motion。通过将任务放在数据所在的机器上面，而不是将数据移动到任务所在的机器上面，可以节省带宽提高计算效率。现在来说MR分为下面三个部分：
   - The end-user *MapReduce API* for programming the desired MapReduce application. 
   - The *MapReduce framework*, which is the runtime implementation of various phases such as the map phase, the sort/shuffle/merge aggregation and the reduce phase. （framework做的事情是runtime的工作，比如怎么划分数据，怎么进行reducer上面的拉数据等）
   - The *MapReduce system*, which is the backend infrastructure required to run the user’s MapReduce application, manage cluster resources, schedule thousands of concurrent jobs etc. （system做的事情是确保runtime可以work的工作，集群管理如何调度）

file:./images/MRArch.png

For a while, we have understood that the Apache Hadoop MapReduce framework needed an overhaul. In particular, with regards to the JobTracker, we needed to address several aspects regarding scalability, cluster utilization, ability for customers to control upgrades to the stack i.e. customer agility and equally importantly, supporting workloads other than MapReduce itself. 考虑对于MR framework需要做下面这些改进，尤其是对于JobTracker来说：
   - 扩展性。我的理解是master有更好的处理能力，应该来支持更多的节点加入集群。2009年产品部署上能够达到5k个节点。
   - 集群利用。现在hadoop是将所有的nodes看作是distince map-reduce slots的，并且两者是不可替换的。可能mapper使用非常多而reducer非常少（或者相反），这样的情况会限制集群利用效率。
   - 灵活地控制software stack。我的理解是对于软件的升级，可能不能够完全替换，因此需要支持集群中有多个版本的MR运行。主要还是兼容性问题。
   - 服务不同的workload而非MR。比如MPI，graph-processing，realtime-processing，并且减少HDFS到自己存储系统之间数据的迁移（现在MR输入一定要在HDFS上面）

--------------------

YARN主要做的工作就是在资源利用的改进上面，将资源利用已经workflow分离：
   - 资源利用通过引入的ResouceManager（RM）以及NodeManager（NM）来管理。
     - NM主要做单机上面的资源收集汇报给RM
     - RM能够用来了解整个集群的资源使用情况，通过收集NM以及AM汇报信息。
     - RM提供pluggable Scheduler来计算资源分配。
   - workflow方面将MR和其他类型workflow分离，抽象成为ApplicationManager（AM）以及Container（既有ResourceAllocation概念，也有ApplicationNode概念）
     
file:./images/YARNArch.png

*** Apache Hadoop YARN – Concepts and Applications | Hortonworks
http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/

将AM和RM分离的好处在于：一方面减轻RM的压力这样可以让RM管理更多的集群，另外一方面可以让AM支持更多类型的计算而不仅仅是MR

AM对RM提供Resource Request。对于Resource Model定义包括下面几个方面：
   - Resource-name (hostname, rackname – we are in the process of generalizing this further to support more complex network topologies with YARN-18).（我需要哪些机器，可以制定host，rack，或者是*/any）
   - Memory (in MB)（需要使用的内存大小）
   - CPU (cores, for now)（CPU的个数）
   - In future, expect us to add more resource-types such as disk/network I/O, GPUs etc.（各种IO参数）
每一个Resource Model如果满足之后在一个机器上面形成一个Container。Resource Request包括下面几个部分：
   - <resource-name, priority, resource-requirement, number-of-containers>
   - resource-name is either hostname, rackname or * to indicate no preference. In future, we expect to support even more complex topologies for virtual machines on a host, more complex networks etc.
   - priority is intra-application priority for this request (to stress, this isn’t across multiple applications).
   - resource-requirement is required capabilities such as memory, cpu etc. (at the time of writing YARN only supports memory and cpu).
   - number-of-containers is just a multiple of such containers.（我需要多少个这样的container？）

ApplicationMaster需要通知Container来执行任务，因为现在的任务不限于MR，需要提供下面这些信息：
   - Command line to launch the process within the container. 命令行
   - Environment variables. 环境变量
   - Local resources necessary on the machine prior to launch, such as jars, shared-objects, auxiliary data files etc. 一些本地资源
   - Security-related tokens. 安全token

整个YARN执行任务的步骤包括下面这几步： Application execution consists of the following steps:
   - Application submission. 提交任务
   - Bootstrapping the ApplicationMaster instance for the application. 启动AM
   - Application execution managed by the ApplicationMaster instance. AM在不同的Container启动task

Let’s walk through an application execution sequence (steps are illustrated in the diagram):
   - A client program submits the application, including the necessary specifications to launch the application-specific ApplicationMaster itself. （用户首先提交AM）
   - The ResourceManager assumes the responsibility to negotiate a specified container in which to start the ApplicationMaster and then launches the ApplicationMaster.（RM为AM分配所需要的Container，并且启动AM）
   - The ApplicationMaster, on boot-up, registers with the ResourceManager – the registration allows the client program to query the ResourceManager for details, which allow it to  directly communicate with its own ApplicationMaster.（AM向RM进行注册）
   - During normal operation the ApplicationMaster negotiates appropriate resource containers via the resource-request protocol.（AM通过Resouce Request和RM进行资源协调，获得所需要的Container）
   - On successful container allocations, the ApplicationMaster launches the container by providing the container launch specification to the NodeManager. The launch specification, typically, includes the necessary information to allow the container to communicate with the ApplicationMaster itself.（AM通知Container所处的NM启动task）
   - The application code executing within the container then provides necessary information (progress, status etc.) to its ApplicationMaster via an application-specific protocol.（Container会定时和AM进行通信，通知进度等）
   - During the application execution, the client that submitted the program communicates directly with the ApplicationMaster to get status, progress updates etc. via an application-specific protocol.（client直接和AM进行通信了解整个任务进度）
   - Once the application is complete, and all necessary work has been finished, the ApplicationMaster deregisters with the ResourceManager and shuts down, allowing its own container to be repurposed.（任务完成之后AM通知RM注销并且释放所持有的Container）

file:./images/yarnflow.png

*** Apache Mesos (Twitter Open Source Open House)
https://speakerdeck.com/u/benh/p/apache-mesos-twitter-open-source-open-house

*** Apache Hadoop YARN – NodeManager | Hortonworks
http://hortonworks.com/blog/apache-hadoop-yarn-nodemanager/

file:./images/yarn-nodemanager-arch.png

   - NodeStatusUpdater 做一些资源状态汇报，并且接收RM请求停止已经运行的container
   - ContainerManager *核心部分*
     - RPC server 接收AM的命令运行或停止container，和ContainerTokenSecretManager协作完成请求认证。所有操作会记录在audit-log
     - ResourceLocalizationService 准备一些applicaiton所需要的资源
     - ContainersLauncher 维护container线程池，接收RM/AM的请求来运行和停止container
     - AuxServices 提供额外服务。当application在这个node上面第一个container运行或者是application结束的时候会收到通知。
     - ContainersMonitor 监控container运行状况，如果资源使用超限的话会kill container
     - LogHandler 收集application本地产生的日志进行聚合并且上传到hdfs
   - ContainerExecutor 执行container
   - NodeHealthCheckerService 对于node做一些健康检查，将一些资源数据给NodeStatusUpdater
   - Security
     - ApplicationACLsManagerNM
     - ContainerTokenSecretManager
   - WebServer 当前运行的application以及对应的container，资源利用状况以及聚合的log

*** Apache Hadoop YARN – ResourceManager | Hortonworks
http://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/

file:./images/yarn-resourcemanager-arch.png

   - Components interfacing RM to the clients:
     - ClientService 用户接口用来提交删除application以及获得当前集群的状况等数据
     - AdminService 管理接口可以用来调整queue的优先级或者是增加node等
   - Components connecting RM to the nodes:
     - ResourceTrackerService 用来和NodeManager做RPC
     - NMLivelinessMonitor 检测NM是否存活
     - NodesListManager 维护当前所有的NM节点
   - Components interacting with the per-application AMs 
     - ApplicationMasterService 用来和AM交互部分接口，AM的资源请求通过这个接口提交，然后转向YarnScheduler处理
     - AMLivelinessMonitor 检测AM是否存活
   - The core of the ResourceManager *核心部分*
     - ApplicationsManager 维护当所有提交的Application
     - ApplicationACLsManager
     - ApplicationMasterLauncher 负责AM的启动
     - YarnScheduler *NOTE（dirlt）：似乎这个调度行为是在一开始就决定的*
       - The Scheduler is responsible for allocating resources to the various running applications subject to constraints of capacities, queues etc. It performs its scheduling function based on the resource requirements of the applications such as memory, CPU, disk, network etc. Currently, only memory is supported and support for CPU is close to completion.
     - ContainerAllocationExpirer application可能占用container但是却不使用。可以用来检测哪些container没有使用。
   - TokenSecretManagers
     - ApplicationTokenSecretManager
     - ContainerTokenSecretManager
     - RMDelegationTokenSecretManager
   - DelegationTokenRenewer

** Source Analysis
*** task如何向tasktracker进行定时汇报
   - task不管是mapper还是reducer，和mr框架相关的内容都包含在了Context里面。
   - Context初始化里面需要传入一个Reporter类，这个类主要用来和tasktracker汇报信息。Reporter本身是一个抽象类，一个具体实现类有TaskReporter
   - TaskReporter本身实现了一个run方法，代码里面可以看到在和tasktracker通信。如果任务每个完成的话，那么会不断检查sendProgress这个标志位，这个标志位也被progress方法设置.
   - 在MapTask以及ReduceTask里面的run方法，首先会创建reporter对象并且启动（startCommunicationThread），然后执行具体的map或者是reduce过程。（runNewMapper/runNewReducer） ，最后回到了Context.run
   - 在Context.run里面本质工作是在不断地读取kv然后交给appcode来进行处理，在每次调用nextKeyValue里面，实际上调用了report.progress方法。
简单地来说，有单独的汇报线程，然后在mapper以及reducer里面每次读取一个kv的话都会调用progress，之后汇报线程就可以向tasktracker汇报状态。因此如果自己某个任务耗时过长的话，可以调用context.progress().


** Usage
*** 多路输入
多路输入包括从多路hbase以及多路hdfs输入：
   - 编写hbase以及hdfs对应的mapper
   - 构造两个htable以及hdfs file
   - mapper效果就是将内容直接转发出去
   - reducer将输出结果写入到hdfs文件
   - 设置1个reducer这样可以容易地验证结果。
*NOTE（dirlt）：现在似乎只能够实现多个hdfs，1个htable作为输入*

#+BEGIN_SRC Java
package com.umeng.dp.helper;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.hbase.mapreduce.TableMapper;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TestHBaseInputAndHDFSInputMR {
    public static final String kInTableName1 = "test.temporary.in1";
    public static final String kInTableName2 = "test.temporary.in2";
    public static final String kInFileName1 = "/tmp/test.temporary.in1";
    public static final String kInFileName2 = "/tmp/test.temporary.in2";
    public static final String kOutFileName = "/tmp/test.temporary.out";
    private final static byte[] kByteColumnFamily = Bytes.toBytes("CF");
    private final static byte[] kByteColumn = Bytes.toBytes("CL");

    public static class FMapper extends Mapper<LongWritable, Text, Text, Text> {
        @Override
        protected void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {
            context.write(new Text("0"),
                    new Text("file mapper value=" + value.toString()));
        }
    }

    public static class TMapper extends TableMapper<Text, Text> {
        @Override
        protected void map(ImmutableBytesWritable key, Result result,
                Context context) throws IOException, InterruptedException {
            context.write(
                    new Text("0"),
                    new Text("table mapper key = "
                            + Bytes.toString(key.get())
                            + ", value="
                            + Bytes.toString(result.getValue(kByteColumnFamily,
                                    kByteColumn))));
        }
    }

    public static class TMapper2 extends TMapper {
    }

    public static class FTReducer extends
            Reducer<Text, Text, NullWritable, Text> {
        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context)
                throws IOException, InterruptedException {
            for (Text v : values) {
                context.write(null, v);
            }
        }
    }

    public static void createTable(String name, Configuration conf)
            throws IOException {
        HBaseAdmin admin = new HBaseAdmin(conf);
        if (admin.isTableAvailable(name)) {
            admin.disableTable(name);
            admin.deleteTable(name);
        }
        HTableDescriptor dp = new HTableDescriptor(name);
        dp.addFamily(new HColumnDescriptor(kByteColumnFamily));
        admin.createTable(dp);

        HTable table = new HTable(name);
        Put put = new Put(Bytes.toBytes(name + ".rowkey"));
        put.add(kByteColumnFamily, kByteColumn, Bytes.toBytes(name + ".tvalue"));
        table.put(put);
        table.close();
    }

    public static void createFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        FSDataOutputStream fos = fs.create(p);
        fos.writeBytes(name + ".fvalue\n");
        fos.close();
        fs.close();
    }

    public static void deleteFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        fs.close();
    }

    public static Job configureJob(Configuration conf, String[] args)
            throws IOException {
         // fill htable some data.
         createTable(kInTableName1, conf);
         createTable(kInTableName2, conf);
         // write hdfs some data.
         createFile(kInFileName1, conf);
         createFile(kInFileName2, conf);
        // delete out
        deleteFile(kOutFileName, conf);

        String jobName = "TestHBaseInputAndHDFSInputMR#"
                + new SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
        // setup environment.
        Job job = new Job(conf);
        job.setJobName(jobName);
        job.setJarByClass(TestHBaseInputAndHDFSInputMR.class);

        // mapper option.
        Scan scan = new Scan();
        scan.setCaching(500); // 1 is the default in Scan, which will be bad for
                              // MapReduce jobs
                              // TableMapReduceUtil.initTableMapperJob(kInTableName1,
                              // scan,
        TableMapReduceUtil.initTableMapperJob(kInTableName1, scan,
                TMapper.class, Text.class, Text.class, job);
        // simplest way.
        MultipleInputs.addInputPath(job, new Path(kInTableName1),
                TableInputFormat.class, TMapper.class);
        
        MultipleInputs.addInputPath(job, new Path(kInFileName1),
                TextInputFormat.class, FMapper.class);
        MultipleInputs.addInputPath(job, new Path(kInFileName2),
                TextInputFormat.class, FMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        // reducer option.
        job.setReducerClass(Reducer.class);
        job.setOutputKeyClass(NullWritable.class);
        job.setOutputValueClass(Text.class);
        job.setNumReduceTasks(1); // just one reducer.
        // output option.
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job, new Path(kOutFileName));
        return job;
    }

    public static void main(String[] args) {
        try {
            Configuration conf = HBaseConfiguration.create();
            args = new GenericOptionsParser(conf, args).getRemainingArgs();
            Job job = configureJob(conf, args);
            job.submit();
            System.exit(job.waitForCompletion(true) ? 0 : 1);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
#+END_SRC

*** 多路输出
多路输出包括输出到hdfs和htable
   - 构造一个输入文件
   - 从这个文件读取内容并且在mapper里面写到file和table里面
   - 没有设置任何reducer

**** MultipleOutputs
   - 支持多个htable，
   - 如果使用write(String namedOutput, K key, V value)会写到FileOutputFormat设置的目录下面，文件附上前缀namedOutput-，编号从0开始
   - 如果使用write(String namedOutput, K key, V value, String baseOutputPath) 会写到baseOutputPath目录下面，文件名没有前缀，编号还是从0开始
   - 因为最后输出是调用MultipleOutputs.write而非Context.write，因此和mrunit配合不太好

#+BEGIN_EXAMPLE
[dirlt@umeng-ubuntu-pc] > hadoop fs -ls /tmp/test/temporary.out/
12/10/08 16:59:00 INFO security.UserGroupInformation: JAAS Configuration already set up for Hadoop, not re-installing.
Found 4 items
-rw-r--r--   1 dirlt supergroup          0 2012-10-08 16:58 /tmp/test/temporary.out/_SUCCESS
drwxr-xr-x   - dirlt supergroup          0 2012-10-08 16:57 /tmp/test/temporary.out/_logs
-rw-r--r--   1 dirlt supergroup         35 2012-10-08 16:57 /tmp/test/temporary.out/f-m-00000
-rw-r--r--   1 dirlt supergroup          0 2012-10-08 16:57 /tmp/test/temporary.out/part-m-00000
#+END_EXAMPLE
可以看到在mapper输出里面，以f来作为前缀而没有使用part来作为前缀作为输出（虽然存在part这个文件，但是我们没有使用context进行输出）

#+BEGIN_SRC Java
package com.umeng.dp.offline;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat;
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TestHBaseOutputAndHDFSOutputMR {
    public static final String kInputFileName = "/tmp/test/temporary.in";
    public static final String kOutputTableName = "test.temporary.out";
    public static final String kOutputTableName2 = "test.temporary.out2";
    public static final String kOutputFileName = "/tmp/test/temporary.out";
    private final static byte[] kByteColumnFamily = Bytes.toBytes("CF");
    private final static byte[] kByteColumn = Bytes.toBytes("CL");
    
    public static void createTable(String name, Configuration conf)
            throws IOException {
        HBaseAdmin admin = new HBaseAdmin(conf);
        if (admin.isTableAvailable(name)) {
            admin.disableTable(name);
            admin.deleteTable(name);
        }
        HTableDescriptor dp = new HTableDescriptor(name);
        dp.addFamily(new HColumnDescriptor(kByteColumnFamily));
        admin.createTable(dp);
        admin.close();
    }
    
    public static void createFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        FSDataOutputStream fos = fs.create(p);
        fos.writeBytes(name + ".fvalue\n");
        fos.close();
        fs.close();
    }
    
    public static void deleteFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        fs.close();
    }
    
    public static class XMap extends Mapper<LongWritable, Text, NullWritable, NullWritable > {
        private MultipleOutputs<NullWritable, NullWritable> mos = null;
        @Override
        public void setup(Context ctx) throws IOException, InterruptedException {
            super.setup(ctx);
            mos = new MultipleOutputs<NullWritable, NullWritable>(ctx);
        }
        
        @Override
        public void map(LongWritable k, Text v, Context ctx) throws IOException, InterruptedException {
            mos.write("f", new Text("fkey"), v);
            mos.write("f2", new Text("fkey"), v);
            Put put = new Put(Bytes.toBytes("tkey"));
            put.add(kByteColumnFamily, kByteColumn, v.getBytes());
            mos.write("t", new ImmutableBytesWritable(Bytes.toBytes(kOutputTableName)), put);
            mos.write("t", new ImmutableBytesWritable(Bytes.toBytes(kOutputTableName2)), put);
        }
        
        @Override
        public void cleanup(Context ctx) throws IOException, InterruptedException {
            super.cleanup(ctx);
            mos.close();           
        }
    }
    
    public static Job configureJob(Configuration conf, String[] args)
            throws IOException {
        createTable(kOutputTableName,conf);
        createTable(kOutputTableName2,conf);
        deleteFile(kOutputFileName, conf);
        createFile(kInputFileName, conf);
        
        String jobName = "TestHBaseOutputAndHDFSOutputMR#"
                + new SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
        // setup environment.
        Job job = new Job(conf);
        job.setJobName(jobName);
        job.setJarByClass(TestHBaseOutputAndHDFSOutputMR.class);        
        job.setMapperClass(TestHBaseOutputAndHDFSOutputMR.XMap.class);
        FileInputFormat.setInputPaths(job, new Path(kInputFileName));
        
        MultipleOutputs.addNamedOutput(job, "f", TextOutputFormat.class, Text.class, Text.class);
        MultipleOutputs.addNamedOutput(job, "f2", TextOutputFormat.class, Text.class, Text.class);
        MultipleOutputs.addNamedOutput(job, "t", MultiTableOutputFormat.class, ImmutableBytesWritable.class, Put.class);
        MultipleOutputs.addNamedOutput(job, "t2", MultiTableOutputFormat.class, ImmutableBytesWritable.class, Put.class);
        FileOutputFormat.setOutputPath(job, new Path(kOutputFileName));        
        
        job.setNumReduceTasks(0);
        return job;
    }

    public static void main(String[] args) {
        try {
            Configuration conf = HBaseConfiguration.create();
            args = new GenericOptionsParser(conf, args).getRemainingArgs();
            Job job = configureJob(conf, args);
            job.submit();
            System.exit(job.waitForCompletion(true) ? 0 : 1);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

#+END_SRC

但是上面这种方式实际上并不太好进行测试。一种可以测试的方法就是MultipleOutputs进行mock，修改其write的方法将结果缓存起来。

**** MultipleTableOutputFormat
但是如果我们只是输出到多个表的话，还是可以使用mrunit来进行测试的，下面是如果只有htable输出的情况，这样更加方面做ut
#+BEGIN_SRC Java
package com.umeng.dp.offline;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import junit.framework.TestCase;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TestHBaseOutputAndHDFSOutputMR extends TestCase {
    public static final String kInputFileName = "/tmp/test/temporary.in";
    public static final String kOutputTableName = "test.temporary.out";
    public static final String kOutputTableName2 = "test.temporary.out2";
    private final static byte[] kByteColumnFamily = Bytes.toBytes("CF");
    private final static byte[] kByteColumn = Bytes.toBytes("CL");
    
    public static void createTable(String name, Configuration conf)
            throws IOException {
        HBaseAdmin admin = new HBaseAdmin(conf);
        if (admin.isTableAvailable(name)) {
            admin.disableTable(name);
            admin.deleteTable(name);
        }
        HTableDescriptor dp = new HTableDescriptor(name);
        dp.addFamily(new HColumnDescriptor(kByteColumnFamily));
        admin.createTable(dp);
        admin.close();
    }
    
    public static void createFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        FSDataOutputStream fos = fs.create(p);
        fos.writeBytes(name + ".fvalue\n");
        fos.close();
        fs.close();
    }
    
    public static void deleteFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        fs.close();
    }
    
    public static class XMap extends Mapper<LongWritable, Text, ImmutableBytesWritable, Put > {
        @Override
        public void setup(Context ctx) throws IOException, InterruptedException {
            super.setup(ctx);
        }
        
        @Override
        public void map(LongWritable k, Text v, Context ctx) throws IOException, InterruptedException {            
            Put put = new Put(Bytes.toBytes("tkey"));
            put.add(kByteColumnFamily, kByteColumn, v.getBytes());
            ctx.write(new ImmutableBytesWritable(Bytes.toBytes(kOutputTableName)), put);
            ctx.write(new ImmutableBytesWritable(Bytes.toBytes(kOutputTableName2)), put);
        }
        
        @Override
        public void cleanup(Context ctx) throws IOException, InterruptedException {
            super.cleanup(ctx);   
        }
    }
    
    public static Job configureJob(Configuration conf, String[] args)
            throws IOException {
        createTable(kOutputTableName,conf);
        createTable(kOutputTableName2,conf);
        createFile(kInputFileName, conf);
        
        String jobName = "TestHBaseOutputAndHDFSOutputMR#"
                + new SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
        // setup environment.
        Job job = new Job(conf);
        job.setJobName(jobName);
        job.setJarByClass(TestHBaseOutputAndHDFSOutputMR.class);        
        job.setMapperClass(TestHBaseOutputAndHDFSOutputMR.XMap.class);
        FileInputFormat.setInputPaths(job, new Path(kInputFileName));
        job.setOutputFormatClass(MultiTableOutputFormat.class);               
        job.setNumReduceTasks(0);
        return job;
    }

    public static void main(String[] args) {
        try {
            Configuration conf = HBaseConfiguration.create();
            args = new GenericOptionsParser(conf, args).getRemainingArgs();
            Job job = configureJob(conf, args);
            job.submit();
            System.exit(job.waitForCompletion(true) ? 0 : 1);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

#+END_SRC

*** 获取集群运行状况
可以通过JobClient这个对象来获得集群和上面的作业运行状况，更多信息可以详细阅读一下JobClient的API部分
#+BEGIN_SRC Java
package com.dirlt.mapreduce;

import java.io.IOException;

import org.apache.hadoop.mapred.ClusterStatus;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.JobStatus;

public class Cluster {
    public static void main(String[] args) throws IOException {
        JobConf jconf = new JobConf();
        JobClient jclient = new JobClient(jconf);
        JobStatus[] js = jclient.getAllJobs();
        for (JobStatus j : js) {
            System.out.println("JOB=jobid:" + j.getJobID().toString() + ",setup:"
                    + j.setupProgress() + ",cleanup:" + j.cleanupProgress()
                    + ",map:" + j.mapProgress() + ",reduce:"
                    + j.reduceProgress() + ",status:"
                    + JobStatus.getJobRunState(j.getRunState()));
        }
        ClusterStatus cs = jclient.getClusterStatus(true);
        System.out.println("CSR=map-cap:" + cs.getMaxMapTasks() + ",map-used:"
                + cs.getMapTasks() + ",reduce-cap:" + cs.getMaxReduceTasks()
                + ",reduce-used:" + cs.getReduceTasks());
    }
}

#+END_SRC

