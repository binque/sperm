<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>gfs</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="gfs"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2012-08-02 13:24:27 CST"/>
<meta name="author" content="dirtysalt"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>

</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">gfs</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 gfs</a>
<ul>
<li><a href="#sec-1-1">1.1 Abstract</a></li>
<li><a href="#sec-1-2">1.2 Introduction</a></li>
<li><a href="#sec-1-3">1.3 Design Overview</a>
<ul>
<li><a href="#sec-1-3-1">1.3.1 Assumptions</a></li>
<li><a href="#sec-1-3-2">1.3.2 Interface</a></li>
<li><a href="#sec-1-3-3">1.3.3 Architecture</a></li>
<li><a href="#sec-1-3-4">1.3.4 Single Master</a></li>
<li><a href="#sec-1-3-5">1.3.5 Chunk Size</a></li>
<li><a href="#sec-1-3-6">1.3.6 Metadata</a>
<ul>
<li><a href="#sec-1-3-6-1">1.3.6.1 In-Memory Data Strucutres</a></li>
<li><a href="#sec-1-3-6-2">1.3.6.2 Chunk Locations</a></li>
<li><a href="#sec-1-3-6-3">1.3.6.3 Operation Log</a></li>
</ul>
</li>
<li><a href="#sec-1-3-7">1.3.7 Consistency Model</a></li>
<li><a href="#sec-1-3-8">1.3.8 Implications for Applications</a></li>
</ul>
</li>
<li><a href="#sec-1-4">1.4 System Interactions</a>
<ul>
<li><a href="#sec-1-4-1">1.4.1 Leases and Mutation Order</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> gfs</h2>
<div class="outline-text-2" id="text-1">


<ul>
<li>link: <a href="http://research.google.com/archive/gfs.html">http://research.google.com/archive/gfs.html</a>
</li>
<li>title: The Google File System
</li>
<li>author: Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung(Google Inc)
</li>
<li>date: 2003
</li>
</ul>


<p>
TODO(dirlt):translate them.!!!
</p>

</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Abstract</h3>
<div class="outline-text-3" id="text-1-1">

<ul>
<li>a scalable distributed file system for large distributed data-intensive applications.
</li>
<li>fault tolerance while running on inexpensive commodity hardware.(搭建在廉价PC上)
</li>
<li>high aggregate performance to a large number of clients.
</li>
<li>The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients.(当时最大集群提供100TB级别的数据存储访问，分布在上千台机器上面的上千个磁盘，能够被百个clients并发访问)
</li>
</ul>


</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Introduction</h3>
<div class="outline-text-3" id="text-1-2">

<ul>
<li>GFS shares many of the same goals as previous distributed file systems such as performance, scalability, reliability, and availability.
</li>
<li>However, its design has been driven by key observations of our application work-loads and technological environment, both current and an-ticipated, that reflect a marked departure from some earlier file system design assumptions.
<ul>
<li>First, component failures are the norm rather than the exception.  constant monitoring, error detection, fault tolerance, and automatic recovery must be integral to the system.
</li>
<li>Second, files are huge by traditional standards. Multi-GB files are common. Each file typically contains many applica-tion objects such as web documents. When we are regularly working with fast growing data sets of many TBs comprising billions of objects, it is unwieldy to manage billions of ap-proximately KB-sized files even when the file system could support it.
</li>
<li>Third, most files are mutated by appending new data rather than overwriting existing data. Random writes within a file are practically non-existent. Once written, the files are only read, and often only sequentially.Given this access pattern on huge files, appending becomes the fo-cus of performance optimization and atomicity guarantees, while caching data blocks in the client loses its appeal.
</li>
<li>Fourth, co-designing the applications and the file system API benefits the overall system by increasing our flexibility.
<ul>
<li>relaxed GFS's consistency model to vastly simplify the file system without imposing an onerous burden on the applications.
</li>
<li>introduced an atomic append operation  so that multiple clients can append concurrently to a file without extra synchronization between them.
</li>
</ul>

</li>
</ul>

</li>
<li>Multiple GFS clusters are currently deployed for different purposes. The largest ones have over 1000 storage nodes, over 300 TB of diskstorage, and are heavily accessed by hundreds of clients on distinct machines on a continuous
</li>
</ul>

<p>basis.
</p>
</div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Design Overview</h3>
<div class="outline-text-3" id="text-1-3">


</div>

<div id="outline-container-1-3-1" class="outline-4">
<h4 id="sec-1-3-1"><span class="section-number-4">1.3.1</span> Assumptions</h4>
<div class="outline-text-4" id="text-1-3-1">

<ul>
<li>The system is built from many inexpensive commodity components that often fail. It must constantly monitor itself and detect, tolerate, and recover promptly from component failures on a routine basis.
</li>
<li>The system stores a modest number of large files. We expect a few million files, each typically 100 MB or larger in size. Multi-GB files are the common case and should be managed efficiently. Small files must be supported, but we need not optimize for them
</li>
<li>The workloads primarily consist of two kinds of reads: large streaming reads and small random reads.In large streaming reads, individual operations typically read hundreds of KBs, more commonly 1 MB or more. Successive operations from the same client often read through a contiguous region of a file. A small ran-dom read typically reads a few KBs at some arbitrary offset. Performance-conscious applications often batch and sort their small reads to advance steadily through the file rather than go backand forth.
</li>
<li>The workloads also have many large, sequential writes that append data to files. Typical operation sizes are similar to those for reads. Once written, files are sel-dom modified again. Small writes at arbitrary posi-tions in a file are supported but do not have to be efficient.
</li>
<li>The system must efficiently implement well-defined se-mantics for multiple clients that concurrently append to the same file. Our files are often used as producer-consumer queues or for many-way merging. Hundreds of producers, running one per machine, will concur-rently append to a file. Atomicity with minimal syn-chronization overhead is essential. The file may be read later, or a consumer may be reading through the file simultaneously.
</li>
<li>High sustained bandwidth is more important than low latency. Most of our target applications place a pre-mium on processing data in bulkat a high rate, while few have stringent response time requirements for an individual read or write.
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-2" class="outline-4">
<h4 id="sec-1-3-2"><span class="section-number-4">1.3.2</span> Interface</h4>
<div class="outline-text-4" id="text-1-3-2">

<ul>
<li>GFS provides a familiar file system interface, though it does not implement a standard API such as POSIX.
</li>
<li>Files are organized hierarchically in directories and identified by path-names.
</li>
<li>We support the usual operations:
<ul>
<li>create
</li>
<li>delete
</li>
<li>open
</li>
<li>close
</li>
<li>read
</li>
<li>write(random)
</li>
</ul>

</li>
<li>snapshot
</li>
<li>record append
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-3" class="outline-4">
<h4 id="sec-1-3-3"><span class="section-number-4">1.3.3</span> Architecture</h4>
<div class="outline-text-4" id="text-1-3-3">

<ul>
<li>A GFS cluster consists of a single masterand multiple chunkservers and is accessed by multiple clients.
</li>
<li>Files are divided into fixed-size chunks. Each chunk is identified by an immutable and globally unique 64 bit chunk handle assigned by the master at the time of chunk creation.
</li>
<li>Chunkservers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range.   
</li>
<li>For reliability, each chunk is replicated on multi-ple chunkservers. By default, we store three replicas, though users can designate different replication levels for different regions of the file namespace.
</li>
<li>The master maintains all file system metadata. This in-cludes the namespace, access control information, the map-ping from files to chunks, and the current locations of chunks. It also controls system-wide activities such as chunk lease management, garbage collection of orphaned chunks, and chunk migration between chunkservers. The master peri-odically communicates with each chunkserver in HeartBeat messages to give it instructions and collect its state.
</li>
<li>Clients interact with the master for metadata opera-tions, but all data-bearing communication goes directly to the chunkservers. We do not provide the POSIX API and therefore need not hookinto the Linux vnode layer.
</li>
<li>Neither the client nor the chunkserver caches file data. Client caches offer little benefit because most applications stream through huge files or have working sets too large to be cached. Chunkservers need not cache file data because chunks are stored as local files and so Linux’s buffer cache already keeps frequently accesseddata in memory.
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-4" class="outline-4">
<h4 id="sec-1-3-4"><span class="section-number-4">1.3.4</span> Single Master</h4>
<div class="outline-text-4" id="text-1-3-4">

<ul>
<li>Having a single master vastly simplifies our design and enables the master to make sophisticated chunk placement and replication decisions using global knowledge. However, we must minimize its involvement in reads and writes so that it does not become a bottleneck.
</li>
<li>Clients never read and write file data through the master. Instead, a client asks the master which chunkservers it should contact. It caches this information for a limited time and interacts with the chunkservers directly for many subsequent operations.
</li>
<li>Further reads of the same chunkrequire no more client-master interaction until the cached information expires or the file is reopened. In fact, the client typically asks for multiple chunks in the same request and the master can also include the informa-tion for chunks immediately following those requested. This extra information sidesteps several future client-master in-teractions at practically no extra cost.
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-5" class="outline-4">
<h4 id="sec-1-3-5"><span class="section-number-4">1.3.5</span> Chunk Size</h4>
<div class="outline-text-4" id="text-1-3-5">

<ul>
<li>Chunk size is one of the key design parameters. We have chosen 64 MB, which is much larger than typical file sys-tem blocksizes.
</li>
<li>Each chunk replica is stored as a plain Linux file on a chunkserver and is extended only as needed. Lazy space allocation avoids wasting space due to internal fragmentation, perhaps the greatest objection against such a large chunk size.(对于这么大的chunksize来说，可能文件内部碎片是最大的障碍)
</li>
<li>A large chunk size offers several important advantages.
<ul>
<li>First, it reduces clients' need to interact with the master because reads and writes on the same chunk require only one initial request to the master for chunk location informa-tion.
</li>
<li>Second, since on a large chunk, a client is more likely to perform many operations on a given chunk, it can reduce network overhead by keeping a persis-tent TCP connection to the chunkserver over an extended period of time. TODO(dirlt):这个和节省网络开销有什么关系?
</li>
<li>Third, it reduces the size of the metadata stored on the master. This allows us to keep the metadata in memory,
</li>
</ul>

</li>
<li>On the other hand, a large chunk size, even with lazy space allocation, has its disadvantages.
<ul>
<li>A small file consists of a small number of chunks, perhaps just one. The chunkservers storing those chunks may become hot spots if many clients are accessing the same file. In practice, hot spots have not been a major issue because our applications mostly read large multi-chunkfiles sequentially.
</li>
<li>We fixed this problem by storing such executables with a higher replication factor and by making the batch-queue system stagger application start times. A potential long-term solution is to allow clients to read data from other
</li>
</ul>

</li>
</ul>

<p>clients in such situations.(针对上面这个热点问题，问题提到可以通过提高replication因子来散布在更多的chunkserver上，并且通过让程序启动时间交错来缓解这个问题。但是长远的解决办法应该是允许P2P的方式从其他client上读取)
</p>
</div>

</div>

<div id="outline-container-1-3-6" class="outline-4">
<h4 id="sec-1-3-6"><span class="section-number-4">1.3.6</span> Metadata</h4>
<div class="outline-text-4" id="text-1-3-6">

<ul>
<li>The master stores three major types of metadata:
<ul>
<li>the file and chunk namespaces,
</li>
<li>the mapping from files to chunks,
</li>
<li>and the locations of each chunk's replicas
</li>
</ul>

</li>
<li>All metadata is kept in the masters memory.
</li>
<li>The first two types (names-paces and file-to-chunk mapping) are also kept persistent by logging mutations to an operation log stored on the mas-ter's local diskand replicated on remote machines.
</li>
<li>The master does not store chunk location informa-tion persistently. Instead, it asks each chunkserver about its chunks at master startup and whenever a chunkserver joins the cluster.
</li>
</ul>



</div>

<div id="outline-container-1-3-6-1" class="outline-5">
<h5 id="sec-1-3-6-1"><span class="section-number-5">1.3.6.1</span> In-Memory Data Strucutres</h5>
<div class="outline-text-5" id="text-1-3-6-1">

<ul>
<li>Since metadata is stored in memory, master operations are fast. Furthermore, it is easy and efficient for the master to periodically scan through its entire state in the background. This periodic scanning is used to implement chunk garbage collection, re-replication in the presence of chunkserver fail-ures, and chunk migration to balance load and diskspace usage across chunkservers.
</li>
<li>One potential concern for this memory-only approach is that the number of chunks and hence the capacity of the whole system is limited by how much memory the master has. This is not a serious limitation in practice. The mas-ter maintains less than 64 bytes of metadata for each 64 MB chunk. the file namespace data typically requires less then 64 bytes per file because it stores file names compactly us-ing prefix compression.(对于master在内存维护数据结构的话，需要考虑内存占用问题。但是在实际中并不是一个太大的约束。对于64MB chunk而言会保存64字节的meta数据，并且对于一个文件来说使用前缀压缩可以将文件名压缩到64字节以下)
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-6-2" class="outline-5">
<h5 id="sec-1-3-6-2"><span class="section-number-5">1.3.6.2</span> Chunk Locations</h5>
<div class="outline-text-5" id="text-1-3-6-2">

<ul>
<li>The master does not keep a persistent record of which chunkservers have a replica of a given chunk. It simply polls chunkservers for that information at startup. The master can keep itself up-to-date thereafter because it controls all chunk placement and monitors chunkserver status with reg-ular HeartBeat messages. This eliminated the problem of keeping the master and chunkservers in sync as chunkservers join and leave the cluster, change names, fail, restart, and so on. In a cluster with hundreds of servers, these events happen all too often. (对于chunkserver加入集群,或者是chunkserver改变名字，宕机重启等事情的话，保持master和chunkserver同步是一件非常麻烦的事情，尤其是这些事情经常发生)
</li>
<li>Another way to understand this design decision is to real-ize that a chunkserver has the final word over what chunks it does or does not have on its own disks. There is no point in trying to maintain a consistent view of this information on the master because errors on a chunkserver may cause chunks to vanish spontaneously (e.g., a disk may go bad and be disabled) or an operator may rename a chunkserver.(对于chunkserver而言才是最终决定是否包含chunk的。对于master包含这种一致性view的话没有任何用户，因为对于chunkserver而言的很可能会因为故障导致某些chunk就丢失，或者是op就直接修改chunkserver名字) NOTE(dirlt):其实一致性view还是需要通过chunkserver和master之间交互来决定。对于master来说完全可以作为作为一个cache角色存在，只是保存chunk replacement的一个cache.通过这个cache来减少问题几率。然后通过periodically来更新cache内容。
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-6-3" class="outline-5">
<h5 id="sec-1-3-6-3"><span class="section-number-5">1.3.6.3</span> Operation Log</h5>
<div class="outline-text-5" id="text-1-3-6-3">

<ul>
<li>The operation log contains a historical record of critical metadata changes. It is central to GFS. Not only is it the only persistent record of metadata, but it also serves as a logical time line that defines the order of concurrent op-erations. Files and chunks, as well as their versions (see Section 4.5), are all uniquely and eternally identified by the logical times at which they were created.(log记录了对于meta信息关键的修改，一方面可以用来持久化metadata,另外一方面也为并发操作进行排序。file以及chunk分配的version都是按照他们创建的逻辑顺序分配的。TODO(dirlt):file也需要version吗???
</li>
<li>Since the operation log is critical, we must store it reli-ably and not make changes visible to clients until metadata changes are made persistent. Otherwise, we effectively lose the whole file system or recent client operations even if the chunks themselves survive. Therefore, we replicate it on multiple remote machines and respond to a client opera-tion only after flushing the corresponding log record to disk both locally and remotely. The master batches several log records together before flushing thereby reducing the impact of flushing and replication on overall system throughput.
</li>
<li>The master recovers its file system state by replaying the operation log. To minimize startup time, we must keep the log small. The master checkpoints its state whenever the log grows beyond a certain size so that it can recover by loading the latest checkpoint from local disk and replaying only the limited number of log records after that.
</li>
<li>The checkpoint is in a compact B-tree like form that can be directly mapped into memory and used for namespace lookup without ex-tra parsing. This further speeds up recovery and improves availability.
</li>
<li>Because building a checkpoint can take a while, the mas-ter's internal state is structured in such a way that a newcheckpoint can be created without delaying incoming muta-tions. The master switches to a new log file and creates the new checkpoint in a separate thread. The new checkpoint includes all mutations before the switch. It can be created in a minute or so for a cluster with a few million files. When completed, it is written to diskboth locally and remotely.
</li>
<li>Recovery needs only the latest complete checkpoint and subsequent log files. Older checkpoints and log files can be freely deleted, though we keep a few around to guard against catastrophes. A failure during checkpointing does not affect correctness because the recovery code detects and skips incomplete checkpoints.
</li>
</ul>


</div>
</div>

</div>

<div id="outline-container-1-3-7" class="outline-4">
<h4 id="sec-1-3-7"><span class="section-number-4">1.3.7</span> Consistency Model</h4>
<div class="outline-text-4" id="text-1-3-7">

<p>GFS的一致性模型理解可能容易出现分歧，我的理解大致是这样的：
</p><ul>
<li>一致性模型包含两种，为consistent和defined.
</li>
<li>所谓consistent就是说所有的replicas内容都是一致的。
</li>
<li>所谓defined，隐含地就包括consistent,另外一方面意思就是所有的写内容都必须完整保存下来。
</li>
</ul>

<p>我们以两种写为例，write和append. 必须清楚GFS可能会会分块写的，
</p>
<p>
首先考虑write.假设write A和write B操作。两个操作均写两个相同块x,y.其中write A发起顺序是(Ay,Ax),而write B发起顺序是(Bx,By). 
同时发起，
</p><ul>
<li>Ay和Bx发起，同时完成
</li>
<li>Ax和By发起，同时完成。
</li>
</ul>

<p>其最终结果就是(Ax,By).不过这个结果并不是write A和write B中的任意一个。这种情况所有的写内容没有完整保存下来，因为是undefined的。
但是索性的是每个replicas上都是(Ax,By)结果，所以是consistent的。
</p>
<p>
而对于append来说，append A和append B操作，同时发起的话，最终结果不管顺序如何，肯定Ax,Ay以及Bx,By写的内容都会完整保留下来。
但是对于Ay,Ax可能并不连续，但是没有问题，我们可以在应用层上来区分。GFS也会保证所有的replicas结果相同consistent.这种情况是defined的。
</p>
<p>
所以总结GFS一致性模型就是 
</p><table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left">op</th><th scope="col" class="left">Write</th><th scope="col" class="left">Append</th></tr>
</thead>
<tbody>
<tr><td class="left">Serial Success</td><td class="left">defined</td><td class="left">defined interspersed with inconsistent</td></tr>
<tr><td class="left">Concurrent Success</td><td class="left">consistent</td><td class="left">defined interspersed with inconsistent</td></tr>
<tr><td class="left">Failure</td><td class="left">Inconsistent</td><td class="left">Inconsistent</td></tr>
</tbody>
</table>

对于Append中出现inconsistent情况(其实也应该归于failure部分)是因为append部分replics失败。但是对于append部分replicas失败没有关系，
我们继续从primary chunk的offset开始提交(其他replicas也从这个offset开始提交).因为首先写的是primary.所以如果其他replicas没有写成功的话，
那么下一次使用primary last offset写就会出现空洞(可以被GFS识别)造成inconsistent. 对于append来说GFS保证至少原子提交一次。(at least once atomically)


<hr/>

<ul>
<li>File namespace mutations (e.g., file creation) are atomic. They are handled exclusively by the master: namespace locking guarantees atomicity and correctness (Section 4.1); the master's operation log defines a global total order of these operations
</li>
<li>The state of a file region after a data mutation depends on the type of mutation, whether it succeeds or fails, and whether there are concurrent mutations.下面是对一致性模型的解释:     
<ul>
<li>A file region is consistent if all clients will always see the same data, regardless of which replicas they read from.
</li>
<li>A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety.
</li>
<li>When a mutation succeeds without interference from concurrent writers, the affected region is defined (and by implication consistent): all clients will always see what the mutation has written.
</li>
<li>Concurrent successful mutations leave the region undefined but consistent: all clients see the same data, but it may not reflect what any one mutation has written. Typically, it consists of mingled fragments from multiple mutations.
</li>
<li>A failed mutation makes the region in-consistent (hence also undefined): different clients may see different data at different times.
</li>
</ul>

</li>
<li>Data mutations may be writes or record appends. A write causes data to be written at an application-specified file offset. A record append causes data (the "record") to be appended atomically at least once even in the presence of
</li>
</ul>

<p>concurrent mutations, but at an offset of GFS's choosing (Section 3.3). (In contrast, a "regular" append is merely a write at an offset that the client believes to be the current end of file.)(对于append操作的话会返回插入的offset)
</p><ul>
<li>The offset is returned to the client and marks the beginning of a defined region that contains the record. In addition, GFS may insert padding or record duplicates in between. They occupy regions considered to be inconsistent and are typically dwarfed by the amount of user data.(对于连续写的话会在其中插入padding或者是存在一些record duplicated，因此造成部分region的不一致.关于存在record duplicated的话原因之前说过了，而对于存在padding会在后面提到，这个是因为record append行为决定的)
</li>
<li>After a sequence of successful mutations, the mutated file region is guaranteed to be defined and contain the data writ-ten by the last mutation. GFS achieves this by (a) applying mutations to a chunkin the same order on all its replicas (Section 3.1), and (b) using chunkversion numbers to detect any replica that has become stale because it has missed mu-tations while its chunkserver was down (Section 4.5). Stale replicas will never be involved in a mutation or given to clients asking the master for chunk locations. They are garbage collected at the earliest opportunity.(对于一致性的话,GFS是通过所有replicas按照某个顺序进行提交，而对于一些没有更上mutation的replica[比如是因为down掉一段时间]会变成stale状态。对于变成stale状态的replica可以通过检查chunkvesrsion来判断。一旦replica变成stale状态的话，那么就不能够再参与chunk的存储，所有上面的chunk都会被及早GC.)
</li>
<li>GFS identifies failed chunkservers by regular handshakes between master and all chunkservers and detects data corruption by checksumming (Section 5.2). Once a problem surfaces, the data is restored from valid replicas as soon as possible (Section 4.3). A chunk is lost irreversibly only if all its replicas are lost before GFS can react, typically within minutes. Even in this case, it be-comes unavailable, not corrupted: applications receive clear errors rather than corrupt data.(GFS检测chunkserver状态是通过握手，或者是chunkserver向master汇报自己检测checksum情况来发现的。一旦发现数据损坏那么可以在分钟级别内重新进行备份。)
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3-8" class="outline-4">
<h4 id="sec-1-3-8"><span class="section-number-4">1.3.8</span> Implications for Applications</h4>
<div class="outline-text-4" id="text-1-3-8">

<ul>
<li>GFS applications can accommodate the relaxed consis-tency model with a few simple techniques already needed for other purposes:(应用程序如何更好使用GFS):
<ul>
<li>relying on appends rather than overwrites
</li>
<li>checkpointing, and
</li>
<li>writing self-validating, self-identifying records.
</li>
</ul>

</li>
</ul>


</div>
</div>

</div>

<div id="outline-container-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> System Interactions</h3>
<div class="outline-text-3" id="text-1-4">


</div>

<div id="outline-container-1-4-1" class="outline-4">
<h4 id="sec-1-4-1"><span class="section-number-4">1.4.1</span> Leases and Mutation Order</h4>
<div class="outline-text-4" id="text-1-4-1">

<p>这节主要讲GFS是如何来确定mutation order的，必须存在一个primary角色来做mutation order定义，这样才能够保证serial write达到defined状态。
</p>
<ul>
<li>The master grants a chunklease to one of the repli-cas, which we call the primary . The primary picks a serial order for all mutations to the chunk. All replicas follow this order when applying mutations. Thus, the global mutation order is defined first by the lease grant order chosen by the master, and within a lease by the serial numbers assigned by the primary.(对于每个chunk replicas会挑选出一个primary,并且分配一个lease.在这段lease时间内，所有这个chunk上的的mutation都会由这个primary来进行定序。)
</li>
<li>The lease mechanism is designed to minimize manage-ment overhead at the master. A lease has an initial timeout of 60 seconds.However, as long as the chunkis being mu-tated, the primary can request and typically receive exten-sions from the master indefinitely. These extension requests and grants are piggybacked on the HeartBeat messages reg-ularly exchanged between the master and all chunkserves.The master may sometimes try to revoke a lease before it expires (e.g., when the master wants to disable mutations on a file that is being renamed). Even if the master loses communication with a primary, it can safely grant a new lease to another replica after the old lease expires. (对于primary理论上可以无限地延长自己的lease.对于lease的扩展都是通过hearbeat的piggyback回去的。但是有时候master可能有时候希望可以撤回这个权限，因为可能文件需要被rename.撤回权限可以很简单地通知primary,或者如果没有通知上的话，直接等待超时即可。lease timeout通常设置在60s.所以heartbeat的频率肯定不能够低于60s一次。)
</li>
<li>交互过程大致就是
<ul>
<li>client首先询问master要到所有的chunk location.
</li>
<li>TODO(dirlt):
</li>
</ul>

</li>
</ul>



</div>
</div>
</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2012-08-02 13:24:27 CST</p>
<p class="creator">Org version 7.8.02 with Emacs version 23</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
</body>
</html>
