<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>mapred</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="mapred"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2012-11-14 11:36:55 CST"/>
<meta name="author" content="dirtysalt"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>

</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">mapred</h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 mapred</a>
<ul>
<li><a href="#sec-1-1">1.1 YARN</a>
<ul>
<li><a href="#sec-1-1-1">1.1.1 Introducing Apache Hadoop YARN</a></li>
<li><a href="#sec-1-1-2">1.1.2 Apache Hadoop YARN – Background and an Overview</a></li>
<li><a href="#sec-1-1-3">1.1.3 Apache Hadoop YARN – Concepts and Applications</a></li>
<li><a href="#sec-1-1-4">1.1.4 Apache Mesos (Twitter Open Source Open House)</a></li>
</ul>
</li>
<li><a href="#sec-1-2">1.2 Usage</a>
<ul>
<li><a href="#sec-1-2-1">1.2.1 验证多路输入正确性</a></li>
<li><a href="#sec-1-2-2">1.2.2 验证多路输出正确性</a></li>
<li><a href="#sec-1-2-3">1.2.3 获取集群运行状况</a></li>
</ul>
</li>
<li><a href="#sec-1-3">1.3 Source Analysis</a>
<ul>
<li><a href="#sec-1-3-1">1.3.1 task如何向tasktracker进行定时汇报</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> mapred</h2>
<div class="outline-text-2" id="text-1">


</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> YARN</h3>
<div class="outline-text-3" id="text-1-1">


</div>

<div id="outline-container-1-1-1" class="outline-4">
<h4 id="sec-1-1-1"><span class="section-number-4">1.1.1</span> Introducing Apache Hadoop YARN</h4>
<div class="outline-text-4" id="text-1-1-1">

<p><a href="http://hortonworks.com/blog/introducing-apache-hadoop-yarn/">http://hortonworks.com/blog/introducing-apache-hadoop-yarn/</a> 
</p>
<p>
看起来YARN的主要目的是将Hadoop不仅仅用于map-reduce的计算方式，还包括MPI，graph-processing，simple services等，
而MR仅仅是作为其中一种计算方式。底层依然是使用HDFS。发布方式的话还是将HDFS，YARN，MR，以及Common一起统一发布。
</p>
</div>

</div>

<div id="outline-container-1-1-2" class="outline-4">
<h4 id="sec-1-1-2"><span class="section-number-4">1.1.2</span> Apache Hadoop YARN – Background and an Overview</h4>
<div class="outline-text-4" id="text-1-1-2">

<p><a href="http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/">http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/</a>
</p>
<p>
对于MR来说，最关键的一点就是lack of data motion。通过将任务放在数据所在的机器上面，而不是将数据移动到任务所在的机器上面，可以节省带宽提高计算效率。现在来说MR分为下面三个部分：
</p><ul>
<li>The end-user <b>MapReduce API</b> for programming the desired MapReduce application. 
</li>
<li>The <b>MapReduce framework</b>, which is the runtime implementation of various phases such as the map phase, the sort/shuffle/merge aggregation and the reduce phase. （framework做的事情是runtime的工作，比如怎么划分数据，怎么进行reducer上面的拉数据等）
</li>
<li>The <b>MapReduce system</b>, which is the backend infrastructure required to run the user’s MapReduce application, manage cluster resources, schedule thousands of concurrent jobs etc. （system做的事情是确保runtime可以work的工作，集群管理如何调度）
</li>
</ul>


<p>
<img src="./images/MRArch.png"  alt="./images/MRArch.png" />
</p>
<p>
For a while, we have understood that the Apache Hadoop MapReduce framework needed an overhaul. In particular, with regards to the JobTracker, we needed to address several aspects regarding scalability, cluster utilization, ability for customers to control upgrades to the stack i.e. customer agility and equally importantly, supporting workloads other than MapReduce itself. 考虑对于MR framework需要做下面这些改进，尤其是对于JobTracker来说：
</p><ul>
<li>扩展性。我的理解是master有更好的处理能力，应该来支持更多的节点加入集群。2009年产品部署上能够达到5k个节点。
</li>
<li>集群利用。现在hadoop是将所有的nodes看作是distince map-reduce slots的，并且两者是不可替换的。可能mapper使用非常多而reducer非常少（或者相反），这样的情况会限制集群利用效率。
</li>
<li>灵活地控制software stack。我的理解是对于软件的升级，可能不能够完全替换，因此需要支持集群中有多个版本的MR运行。主要还是兼容性问题。
</li>
<li>服务不同的workload而非MR。比如MPI，graph-processing，realtime-processing，并且减少HDFS到自己存储系统之间数据的迁移（现在MR输入一定要在HDFS上面）
</li>
</ul>



<hr/>

<p>
YARN主要做的工作就是在资源利用的改进上面，将资源利用已经workflow分离：
</p><ul>
<li>资源利用通过引入的ResouceManager（RM）以及NodeManager（NM）来管理。
<ul>
<li>NM主要做单机上面的资源收集汇报给RM
</li>
<li>RM能够用来了解整个集群的资源使用情况，通过收集NM以及AM汇报信息。
</li>
<li>RM提供pluggable Scheduler来计算资源分配。
</li>
</ul>

</li>
<li>workflow方面将MR和其他类型workflow分离，抽象成为ApplicationManager（AM）以及Container（既有ResourceAllocation概念，也有ApplicationNode概念）
</li>
</ul>


<p>     
<img src="./images/YARNArch.png"  alt="./images/YARNArch.png" />
</p>
</div>

</div>

<div id="outline-container-1-1-3" class="outline-4">
<h4 id="sec-1-1-3"><span class="section-number-4">1.1.3</span> Apache Hadoop YARN – Concepts and Applications</h4>
<div class="outline-text-4" id="text-1-1-3">

<p><a href="http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/">http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/</a>
</p>
<p>
将AM和RM分离的好处在于：一方面减轻RM的压力这样可以让RM管理更多的集群，另外一方面可以让AM支持更多类型的计算而不仅仅是MR
</p>
<p>
AM对RM提供Resource Request。对于Resource Model定义包括下面几个方面：
</p><ul>
<li>Resource-name (hostname, rackname – we are in the process of generalizing this further to support more complex network topologies with YARN-18).（我需要哪些机器，可以制定host，rack，或者是*/any）
</li>
<li>Memory (in MB)（需要使用的内存大小）
</li>
<li>CPU (cores, for now)（CPU的个数）
</li>
<li>In future, expect us to add more resource-types such as disk/network I/O, GPUs etc.（各种IO参数）
</li>
</ul>

<p>每一个Resource Model如果满足之后在一个机器上面形成一个Container。Resource Request包括下面几个部分：
</p><ul>
<li>&lt;resource-name, priority, resource-requirement, number-of-containers&gt;
</li>
<li>resource-name is either hostname, rackname or * to indicate no preference. In future, we expect to support even more complex topologies for virtual machines on a host, more complex networks etc.
</li>
<li>priority is intra-application priority for this request (to stress, this isn’t across multiple applications).
</li>
<li>resource-requirement is required capabilities such as memory, cpu etc. (at the time of writing YARN only supports memory and cpu).
</li>
<li>number-of-containers is just a multiple of such containers.（我需要多少个这样的container？）
</li>
</ul>


<p>
ApplicationMaster需要通知Container来执行任务，因为现在的任务不限于MR，需要提供下面这些信息：
</p><ul>
<li>Command line to launch the process within the container. 命令行
</li>
<li>Environment variables. 环境变量
</li>
<li>Local resources necessary on the machine prior to launch, such as jars, shared-objects, auxiliary data files etc. 一些本地资源
</li>
<li>Security-related tokens. 安全token
</li>
</ul>


<p>
整个YARN执行任务的步骤包括下面这几步： Application execution consists of the following steps:
</p><ul>
<li>Application submission. 提交任务
</li>
<li>Bootstrapping the ApplicationMaster instance for the application. 启动AM
</li>
<li>Application execution managed by the ApplicationMaster instance. AM在不同的Container启动task
</li>
</ul>


<p>
Let’s walk through an application execution sequence (steps are illustrated in the diagram):
</p><ul>
<li>A client program submits the application, including the necessary specifications to launch the application-specific ApplicationMaster itself. （用户首先提交AM）
</li>
<li>The ResourceManager assumes the responsibility to negotiate a specified container in which to start the ApplicationMaster and then launches the ApplicationMaster.（RM为AM分配所需要的Container，并且启动AM）
</li>
<li>The ApplicationMaster, on boot-up, registers with the ResourceManager – the registration allows the client program to query the ResourceManager for details, which allow it to  directly communicate with its own ApplicationMaster.（AM向RM进行注册）
</li>
<li>During normal operation the ApplicationMaster negotiates appropriate resource containers via the resource-request protocol.（AM通过Resouce Request和RM进行资源协调，获得所需要的Container）
</li>
<li>On successful container allocations, the ApplicationMaster launches the container by providing the container launch specification to the NodeManager. The launch specification, typically, includes the necessary information to allow the container to communicate with the ApplicationMaster itself.（AM通知Container所处的NM启动task）
</li>
<li>The application code executing within the container then provides necessary information (progress, status etc.) to its ApplicationMaster via an application-specific protocol.（Container会定时和AM进行通信，通知进度等）
</li>
<li>During the application execution, the client that submitted the program communicates directly with the ApplicationMaster to get status, progress updates etc. via an application-specific protocol.（client直接和AM进行通信了解整个任务进度）
</li>
<li>Once the application is complete, and all necessary work has been finished, the ApplicationMaster deregisters with the ResourceManager and shuts down, allowing its own container to be repurposed.（任务完成之后AM通知RM注销并且释放所持有的Container）
</li>
</ul>


<p>
<img src="./images/yarnflow.png"  alt="./images/yarnflow.png" />
</p>
</div>

</div>

<div id="outline-container-1-1-4" class="outline-4">
<h4 id="sec-1-1-4"><span class="section-number-4">1.1.4</span> Apache Mesos (Twitter Open Source Open House)</h4>
<div class="outline-text-4" id="text-1-1-4">

<p><a href="https://speakerdeck.com/u/benh/p/apache-mesos-twitter-open-source-open-house">https://speakerdeck.com/u/benh/p/apache-mesos-twitter-open-source-open-house</a>
</p>
<p>
和YARN类似的资源调度层。
</p>
</div>
</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Usage</h3>
<div class="outline-text-3" id="text-1-2">


</div>

<div id="outline-container-1-2-1" class="outline-4">
<h4 id="sec-1-2-1"><span class="section-number-4">1.2.1</span> 验证多路输入正确性</h4>
<div class="outline-text-4" id="text-1-2-1">

<p>多路输入包括从多路hbase以及多路hdfs输入，下面是一个验证程序，可以在上面修改确认是否OK。大致思路如下：
</p><ul>
<li>编写hbase以及hdfs对应的mapper
</li>
<li>构造两个htable以及hdfs file
</li>
<li>mapper效果就是将内容直接转发出去
</li>
<li>reducer将输出结果写入到hdfs文件
</li>
<li>设置1个reducer这样可以容易地验证结果。
</li>
</ul>

<p><b>NOTE（dirlt）：现在似乎只能够实现多个hdfs，1个htable作为输入</b>
</p>



<pre class="src src-Java">package com.umeng.dp.helper;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.hbase.mapreduce.TableMapper;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TestHBaseInputAndHDFSInputMR {
    public static final String kInTableName1 = <span style="color: #00ffff; font-style: italic;">"test.temporary.in1"</span>;
    public static final String kInTableName2 = <span style="color: #00ffff; font-style: italic;">"test.temporary.in2"</span>;
    public static final String kInFileName1 = <span style="color: #00ffff; font-style: italic;">"/tmp/test.temporary.in1"</span>;
    public static final String kInFileName2 = <span style="color: #00ffff; font-style: italic;">"/tmp/test.temporary.in2"</span>;
    public static final String kOutFileName = <span style="color: #00ffff; font-style: italic;">"/tmp/test.temporary.out"</span>;
    private final static byte[] kByteColumnFamily = Bytes.toBytes(<span style="color: #00ffff; font-style: italic;">"CF"</span>);
    private final static byte[] kByteColumn = Bytes.toBytes(<span style="color: #00ffff; font-style: italic;">"CL"</span>);

    public static class FMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {
        @Override
        protected void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {
            context.write(new Text(<span style="color: #00ffff; font-style: italic;">"0"</span>),
                    new Text(<span style="color: #00ffff; font-style: italic;">"file mapper value="</span> + value.toString()));
        }
    }

    public static class TMapper extends TableMapper&lt;Text, Text&gt; {
        @Override
        protected void map(ImmutableBytesWritable key, Result result,
                Context context) throws IOException, InterruptedException {
            context.write(
                    new Text(<span style="color: #00ffff; font-style: italic;">"0"</span>),
                    new Text(<span style="color: #00ffff; font-style: italic;">"table mapper key = "</span>
                            + Bytes.toString(key.get())
                            + <span style="color: #00ffff; font-style: italic;">", value="</span>
                            + Bytes.toString(result.getValue(kByteColumnFamily,
                                    kByteColumn))));
        }
    }

    public static class TMapper2 extends TMapper {
    }

    public static class FTReducer extends
            Reducer&lt;Text, Text, NullWritable, Text&gt; {
        @Override
        protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
                throws IOException, InterruptedException {
            for (Text v : values) {
                context.write(null, v);
            }
        }
    }

    public static void createTable(String name, Configuration conf)
            throws IOException {
        HBaseAdmin admin = new HBaseAdmin(conf);
        if (admin.isTableAvailable(name)) {
            admin.disableTable(name);
            admin.deleteTable(name);
        }
        HTableDescriptor dp = new HTableDescriptor(name);
        dp.addFamily(new HColumnDescriptor(kByteColumnFamily));
        admin.createTable(dp);

        HTable table = new HTable(name);
        Put put = new Put(Bytes.toBytes(name + <span style="color: #00ffff; font-style: italic;">".rowkey"</span>));
        put.add(kByteColumnFamily, kByteColumn, Bytes.toBytes(name + <span style="color: #00ffff; font-style: italic;">".tvalue"</span>));
        table.put(put);
        table.close();
    }

    public static void createFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        FSDataOutputStream fos = fs.create(p);
        fos.writeBytes(name + <span style="color: #00ffff; font-style: italic;">".fvalue\n"</span>);
        fos.close();
        fs.close();
    }

    public static void deleteFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        fs.close();
    }

    public static Job configureJob(Configuration conf, String[] args)
            throws IOException {
         // fill htable some data.
         createTable(kInTableName1, conf);
         createTable(kInTableName2, conf);
         // write hdfs some data.
         createFile(kInFileName1, conf);
         createFile(kInFileName2, conf);
        // delete out
        deleteFile(kOutFileName, conf);

        String jobName = <span style="color: #00ffff; font-style: italic;">"TestHBaseInputAndHDFSInputMR#"</span>
                + new SimpleDateFormat(<span style="color: #00ffff; font-style: italic;">"yyyyMMddHHmmss"</span>).format(new Date());
        // setup environment.
        Job job = new Job(conf);
        job.setJobName(jobName);
        job.setJarByClass(TestHBaseInputAndHDFSInputMR.class);

        // mapper option.
        Scan scan = new Scan();
        scan.setCaching(500); // 1 is the default in Scan, which will be bad for
                              // MapReduce jobs
                              // TableMapReduceUtil.initTableMapperJob(kInTableName1,
                              // scan,
        TableMapReduceUtil.initTableMapperJob(kInTableName1, scan,
                TMapper.class, Text.class, Text.class, job);
        // simplest way.
        MultipleInputs.addInputPath(job, new Path(kInTableName1),
                TableInputFormat.class, TMapper.class);

        MultipleInputs.addInputPath(job, new Path(kInFileName1),
                TextInputFormat.class, FMapper.class);
        MultipleInputs.addInputPath(job, new Path(kInFileName2),
                TextInputFormat.class, FMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        // reducer option.
        job.setReducerClass(Reducer.class);
        job.setOutputKeyClass(NullWritable.class);
        job.setOutputValueClass(Text.class);
        job.setNumReduceTasks(1); // just one reducer.
        // output option.
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job, new Path(kOutFileName));
        return job;
    }

    public static void main(String[] args) {
        try {
            Configuration conf = HBaseConfiguration.create();
            args = new GenericOptionsParser(conf, args).getRemainingArgs();
            Job job = configureJob(conf, args);
            job.submit();
            System.exit(job.waitForCompletion(true) ? 0 : 1);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
</pre>


</div>

</div>

<div id="outline-container-1-2-2" class="outline-4">
<h4 id="sec-1-2-2"><span class="section-number-4">1.2.2</span> 验证多路输出正确性</h4>
<div class="outline-text-4" id="text-1-2-2">

<p>多路输出包括输出到hdfs和htable，下面是一个验证程序。大致似乎是这样的：
</p><ul>
<li>构造一个输入文件
</li>
<li>从这个文件读取内容并且在mapper里面写到file和table里面
</li>
<li>没有设置任何reducer
</li>
</ul>


<p>
<b>NOTE（dirlt）：似乎只是支持1个htable，和1个hdfs的目录。但是hdfs每个输出文件会有一个前缀用来进行区分</b>
</p>
<p>
<b>NOTE（dirlt）：支持多个htable，并且write(String namedOutput, K key, V value, String baseOutputPath) 这个函数支持写到不同的目录下面。当使用这个函数调用的话，那么不会像之前的方式每个文件使用不同的前缀</b>
</p>
<p>
<b>NOTE（dirlt）：所以对于多路输出方面的话，完全可以做到多个htable以及多个hdfs目录的输出</b>
</p>



<pre class="example">[dirlt@umeng-ubuntu-pc] &gt; hadoop fs -ls /tmp/test/temporary.out/
12/10/08 16:59:00 INFO security.UserGroupInformation: JAAS Configuration already set up for Hadoop, not re-installing.
Found 4 items
-rw-r--r--   1 dirlt supergroup          0 2012-10-08 16:58 /tmp/test/temporary.out/_SUCCESS
drwxr-xr-x   - dirlt supergroup          0 2012-10-08 16:57 /tmp/test/temporary.out/_logs
-rw-r--r--   1 dirlt supergroup         35 2012-10-08 16:57 /tmp/test/temporary.out/f-m-00000
-rw-r--r--   1 dirlt supergroup          0 2012-10-08 16:57 /tmp/test/temporary.out/part-m-00000
</pre>

<p>
可以看到在mapper输出里面，以f来作为前缀而没有使用part来作为前缀作为输出（虽然存在part这个文件，但是我们没有使用context进行输出）
</p>



<pre class="src src-Java">package com.umeng.dp.offline;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat;
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TestHBaseOutputAndHDFSOutputMR {
    public static final String kInputFileName = <span style="color: #00ffff; font-style: italic;">"/tmp/test/temporary.in"</span>;
    public static final String kOutputTableName = <span style="color: #00ffff; font-style: italic;">"test.temporary.out"</span>;
    public static final String kOutputTableName2 = <span style="color: #00ffff; font-style: italic;">"test.temporary.out2"</span>;
    public static final String kOutputFileName = <span style="color: #00ffff; font-style: italic;">"/tmp/test/temporary.out"</span>;
    private final static byte[] kByteColumnFamily = Bytes.toBytes(<span style="color: #00ffff; font-style: italic;">"CF"</span>);
    private final static byte[] kByteColumn = Bytes.toBytes(<span style="color: #00ffff; font-style: italic;">"CL"</span>);

    public static void createTable(String name, Configuration conf)
            throws IOException {
        HBaseAdmin admin = new HBaseAdmin(conf);
        if (admin.isTableAvailable(name)) {
            admin.disableTable(name);
            admin.deleteTable(name);
        }
        HTableDescriptor dp = new HTableDescriptor(name);
        dp.addFamily(new HColumnDescriptor(kByteColumnFamily));
        admin.createTable(dp);
        admin.close();
    }

    public static void createFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        FSDataOutputStream fos = fs.create(p);
        fos.writeBytes(name + <span style="color: #00ffff; font-style: italic;">".fvalue\n"</span>);
        fos.close();
        fs.close();
    }

    public static void deleteFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        fs.close();
    }

    public static class XMap extends Mapper&lt;LongWritable, Text, NullWritable, NullWritable &gt; {
        private MultipleOutputs&lt;NullWritable, NullWritable&gt; mos = null;
        @Override
        public void setup(Context ctx) throws IOException, InterruptedException {
            super.setup(ctx);
            mos = new MultipleOutputs&lt;NullWritable, NullWritable&gt;(ctx);
        }

        @Override
        public void map(LongWritable k, Text v, Context ctx) throws IOException, InterruptedException {
            mos.write(<span style="color: #00ffff; font-style: italic;">"f"</span>, new Text(<span style="color: #00ffff; font-style: italic;">"fkey"</span>), v);
            mos.write(<span style="color: #00ffff; font-style: italic;">"f2"</span>, new Text(<span style="color: #00ffff; font-style: italic;">"fkey"</span>), v);
            Put put = new Put(Bytes.toBytes(<span style="color: #00ffff; font-style: italic;">"tkey"</span>));
            put.add(kByteColumnFamily, kByteColumn, v.getBytes());
            mos.write(<span style="color: #00ffff; font-style: italic;">"t"</span>, new ImmutableBytesWritable(Bytes.toBytes(kOutputTableName)), put);
            mos.write(<span style="color: #00ffff; font-style: italic;">"t"</span>, new ImmutableBytesWritable(Bytes.toBytes(kOutputTableName2)), put);
        }

        @Override
        public void cleanup(Context ctx) throws IOException, InterruptedException {
            super.cleanup(ctx);
            mos.close();           
        }
    }

    public static Job configureJob(Configuration conf, String[] args)
            throws IOException {
        createTable(kOutputTableName,conf);
        createTable(kOutputTableName2,conf);
        deleteFile(kOutputFileName, conf);
        createFile(kInputFileName, conf);

        String jobName = <span style="color: #00ffff; font-style: italic;">"TestHBaseOutputAndHDFSOutputMR#"</span>
                + new SimpleDateFormat(<span style="color: #00ffff; font-style: italic;">"yyyyMMddHHmmss"</span>).format(new Date());
        // setup environment.
        Job job = new Job(conf);
        job.setJobName(jobName);
        job.setJarByClass(TestHBaseOutputAndHDFSOutputMR.class);        
        job.setMapperClass(TestHBaseOutputAndHDFSOutputMR.XMap.class);
        FileInputFormat.setInputPaths(job, new Path(kInputFileName));

        MultipleOutputs.addNamedOutput(job, <span style="color: #00ffff; font-style: italic;">"f"</span>, TextOutputFormat.class, Text.class, Text.class);
        MultipleOutputs.addNamedOutput(job, <span style="color: #00ffff; font-style: italic;">"f2"</span>, TextOutputFormat.class, Text.class, Text.class);
        MultipleOutputs.addNamedOutput(job, <span style="color: #00ffff; font-style: italic;">"t"</span>, MultiTableOutputFormat.class, ImmutableBytesWritable.class, Put.class);
        MultipleOutputs.addNamedOutput(job, <span style="color: #00ffff; font-style: italic;">"t2"</span>, MultiTableOutputFormat.class, ImmutableBytesWritable.class, Put.class);
        FileOutputFormat.setOutputPath(job, new Path(kOutputFileName));        

        job.setNumReduceTasks(0);
        return job;
    }

    public static void main(String[] args) {
        try {
            Configuration conf = HBaseConfiguration.create();
            args = new GenericOptionsParser(conf, args).getRemainingArgs();
            Job job = configureJob(conf, args);
            job.submit();
            System.exit(job.waitForCompletion(true) ? 0 : 1);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

</pre>


<p>
但是上面这种方式实际上并不太好进行测试。一种可以测试的方法就是MultipleOutputs进行mock，修改其write的方法将结果缓存起来。
</p>

<hr/>

<p>
但是如果我们只是输出到多个表的话，还是可以使用mrunit来进行测试的，下面是如果只有htable输出的情况，这样更加方面做ut
</p>


<pre class="src src-Java">package com.umeng.dp.offline;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;

import junit.framework.TestCase;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TestHBaseOutputAndHDFSOutputMR extends TestCase {
    public static final String kInputFileName = <span style="color: #00ffff; font-style: italic;">"/tmp/test/temporary.in"</span>;
    public static final String kOutputTableName = <span style="color: #00ffff; font-style: italic;">"test.temporary.out"</span>;
    public static final String kOutputTableName2 = <span style="color: #00ffff; font-style: italic;">"test.temporary.out2"</span>;
    private final static byte[] kByteColumnFamily = Bytes.toBytes(<span style="color: #00ffff; font-style: italic;">"CF"</span>);
    private final static byte[] kByteColumn = Bytes.toBytes(<span style="color: #00ffff; font-style: italic;">"CL"</span>);

    public static void createTable(String name, Configuration conf)
            throws IOException {
        HBaseAdmin admin = new HBaseAdmin(conf);
        if (admin.isTableAvailable(name)) {
            admin.disableTable(name);
            admin.deleteTable(name);
        }
        HTableDescriptor dp = new HTableDescriptor(name);
        dp.addFamily(new HColumnDescriptor(kByteColumnFamily));
        admin.createTable(dp);
        admin.close();
    }

    public static void createFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        FSDataOutputStream fos = fs.create(p);
        fos.writeBytes(name + <span style="color: #00ffff; font-style: italic;">".fvalue\n"</span>);
        fos.close();
        fs.close();
    }

    public static void deleteFile(String name, Configuration conf)
            throws IOException {
        FileSystem fs = FileSystem.get(conf);
        Path p = new Path(name);
        if (fs.exists(p)) {
            fs.delete(p, true);
        }
        fs.close();
    }

    public static class XMap extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put &gt; {
        @Override
        public void setup(Context ctx) throws IOException, InterruptedException {
            super.setup(ctx);
        }

        @Override
        public void map(LongWritable k, Text v, Context ctx) throws IOException, InterruptedException {            
            Put put = new Put(Bytes.toBytes(<span style="color: #00ffff; font-style: italic;">"tkey"</span>));
            put.add(kByteColumnFamily, kByteColumn, v.getBytes());
            ctx.write(new ImmutableBytesWritable(Bytes.toBytes(kOutputTableName)), put);
            ctx.write(new ImmutableBytesWritable(Bytes.toBytes(kOutputTableName2)), put);
        }

        @Override
        public void cleanup(Context ctx) throws IOException, InterruptedException {
            super.cleanup(ctx);   
        }
    }

    public static Job configureJob(Configuration conf, String[] args)
            throws IOException {
        createTable(kOutputTableName,conf);
        createTable(kOutputTableName2,conf);
        createFile(kInputFileName, conf);

        String jobName = <span style="color: #00ffff; font-style: italic;">"TestHBaseOutputAndHDFSOutputMR#"</span>
                + new SimpleDateFormat(<span style="color: #00ffff; font-style: italic;">"yyyyMMddHHmmss"</span>).format(new Date());
        // setup environment.
        Job job = new Job(conf);
        job.setJobName(jobName);
        job.setJarByClass(TestHBaseOutputAndHDFSOutputMR.class);        
        job.setMapperClass(TestHBaseOutputAndHDFSOutputMR.XMap.class);
        FileInputFormat.setInputPaths(job, new Path(kInputFileName));
        job.setOutputFormatClass(MultiTableOutputFormat.class);               
        job.setNumReduceTasks(0);
        return job;
    }

    public static void main(String[] args) {
        try {
            Configuration conf = HBaseConfiguration.create();
            args = new GenericOptionsParser(conf, args).getRemainingArgs();
            Job job = configureJob(conf, args);
            job.submit();
            System.exit(job.waitForCompletion(true) ? 0 : 1);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

</pre>


</div>

</div>

<div id="outline-container-1-2-3" class="outline-4">
<h4 id="sec-1-2-3"><span class="section-number-4">1.2.3</span> 获取集群运行状况</h4>
<div class="outline-text-4" id="text-1-2-3">

<p>可以通过JobClient这个对象来获得集群和上面的作业运行状况，更多信息可以详细阅读一下JobClient的API部分
</p>


<pre class="src src-Java">package com.dirlt.mapreduce;

import java.io.IOException;

import org.apache.hadoop.mapred.ClusterStatus;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.JobStatus;

public class Cluster {
    public static void main(String[] args) throws IOException {
        JobConf jconf = new JobConf();
        JobClient jclient = new JobClient(jconf);
        JobStatus[] js = jclient.getAllJobs();
        for (JobStatus j : js) {
            System.out.println(<span style="color: #00ffff; font-style: italic;">"JOB=jobid:"</span> + j.getJobID().toString() + <span style="color: #00ffff; font-style: italic;">",setup:"</span>
                    + j.setupProgress() + <span style="color: #00ffff; font-style: italic;">",cleanup:"</span> + j.cleanupProgress()
                    + <span style="color: #00ffff; font-style: italic;">",map:"</span> + j.mapProgress() + <span style="color: #00ffff; font-style: italic;">",reduce:"</span>
                    + j.reduceProgress() + <span style="color: #00ffff; font-style: italic;">",status:"</span>
                    + JobStatus.getJobRunState(j.getRunState()));
        }
        ClusterStatus cs = jclient.getClusterStatus(true);
        System.out.println(<span style="color: #00ffff; font-style: italic;">"CSR=map-cap:"</span> + cs.getMaxMapTasks() + <span style="color: #00ffff; font-style: italic;">",map-used:"</span>
                + cs.getMapTasks() + <span style="color: #00ffff; font-style: italic;">",reduce-cap:"</span> + cs.getMaxReduceTasks()
                + <span style="color: #00ffff; font-style: italic;">",reduce-used:"</span> + cs.getReduceTasks());
    }
}

</pre>


</div>
</div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Source Analysis</h3>
<div class="outline-text-3" id="text-1-3">


</div>

<div id="outline-container-1-3-1" class="outline-4">
<h4 id="sec-1-3-1"><span class="section-number-4">1.3.1</span> task如何向tasktracker进行定时汇报</h4>
<div class="outline-text-4" id="text-1-3-1">

<ul>
<li>task不管是mapper还是reducer，和mr框架相关的内容都包含在了Context里面。
</li>
<li>Context初始化里面需要传入一个Reporter类，这个类主要用来和tasktracker汇报信息。Reporter本身是一个抽象类，一个具体实现类有TaskReporter
</li>
<li>TaskReporter本身实现了一个run方法，代码里面可以看到在和tasktracker通信。如果任务每个完成的话，那么会不断检查sendProgress这个标志位，这个标志位也被progress方法设置.
</li>
<li>在MapTask以及ReduceTask里面的run方法，首先会创建reporter对象并且启动（startCommunicationThread），然后执行具体的map或者是reduce过程。（runNewMapper/runNewReducer） ，最后回到了Context.run
</li>
<li>在Context.run里面本质工作是在不断地读取kv然后交给appcode来进行处理，在每次调用nextKeyValue里面，实际上调用了report.progress方法。
</li>
</ul>

<p>简单地来说，有单独的汇报线程，然后在mapper以及reducer里面每次读取一个kv的话都会调用progress，之后汇报线程就可以向tasktracker汇报状态。因此如果自己某个任务耗时过长的话，可以调用context.progress().
</p>
</div>
</div>
</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2012-11-14 11:36:55 CST</p>
<p class="creator">Org version 7.8.02 with Emacs version 23</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
</body>
</html>
<html><body>
<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F54a700ad7035f6e485eaf2300641e7e9' type='text/javascript'%3E%3C/script%3E"));
</script></body></html>